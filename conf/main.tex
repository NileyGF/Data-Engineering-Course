\documentclass[12pt]{book}

% Packages
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
\usepackage{amsmath}  % For math equations
\usepackage{hyperref} % For hyperlinks
\usepackage{titlesec} % For customizing section titles
\usepackage{graphicx} % Required for inserting images

\title{Ingeniería de Datos. Conferencias}
\author{Lic. Niley González}
\date{2024 - 2025}

% Customize Chapter and Section Titles
\titleformat{\chapter}[display]
  {\normalfont\LARGE\bfseries}{\thechapter}{20pt}{}
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}

\begin{document}

\maketitle

% Chapter 1: Conference 1
\chapter{Conferencia 1}
\normalfont\LARGE \textbf{Introducción a la Ingeniería de Datos. Metodologías para la Ciencia de Datos}
\normalfont\small\\

¿Cómo definirían ustedes la ciencia de datos?\\

\textit{Pausa para que los estudiantes compartan sus ideas.}\\

Diferentes autores han intentado definir el campo de la ciencia de datos; algunas de las prespectivas son:

\begin{itemize}
    \item campo multidisciplinario que combina áreas como la informática, las matemáticas y la estadística. Su objetivo principal es utilizar métodos y técnicas científicas para extraer conocimiento y valor de grandes volúmenes de datos, estructurados o no estructurados. (2019)
    \item campo interdisciplinario que integra disciplinas como las ciencias de la computación, el aprendizaje automático, las matemáticas y las estadísticas. (2021)
    \item tema multidisciplinario cuyo propósito es descubrir conocimiento para apoyar la toma de decisiones en diversos contextos empresariales.(2020)
\end{itemize}
En general, hay un consenso en que la ciencia de datos es un campo multidisciplinario o interdisciplinario que se nutre de áreas como la informática, la estadística y las ciencias de la computación. Su enfoque principal es el estudio de los datos, con el propósito de extraer conocimiento y valor a partir de ellos.\\

Ahora, ¿qué entienden ustedes por una metodología?\\

\textit{Pausa para que los estudiantes compartan sus ideas.}\\

Una metodología puede entenderse como una \textbf{estrategia, guía o conjunto de pautas} que nos ayudan a desarrollar un proceso o actividad de manera estructurada. A diferencia de las herramientas o tecnologías específicas, las metodologías no están ligadas a un software o hardware en particular. En cambio, proporcionan un \textbf{marco de trabajo} que nos indica cómo proceder de manera sistemática para alcanzar nuestros objetivos.

¿Por qué es importante esto?\\
En el contexto de la ciencia de datos, las metodologías ayudan a abordar problemas complejos de manera organizada, intentando que cada paso del proceso esté bien definido y alineado con los objetivos del proyecto. 

% https://medium.datadriveninvestor.com/data-science-project-management-methodologies-f6913c6b29eb

\section{Metodologías en Ciencia de Datos}
\label{sec:metodologias}

A continuación se presentan varias metodologías de la Ciencia de Datos, analizando críticamente su estructura, enfoque y aplicabilidad en diferentes contextos. El objetivo es comprender cómo cada metodología se adapta -o no- a distintos escenarios organizacionales, técnicos y de complejidad. A través de este análisis, descubriremos por qué ninguna metodología puede aplicarse universalmente a todas las circunstancias, y cómo su implementación rígida y sin adaptación puede llevar a resultados subóptimos.

\subsection{KDD (Knowledge Discovery in Databases)}
Definido como un proceso interactivo e iterativo de 5 fases para descubrir conocimiento por sus creadores en 1996:
\begin{itemize}
    \item \textbf{Selección}: Los datos cambian de acuerdo con los objetivos del proceso. Se establece un grupo de datos con el que proceder.
    \item \textbf{Procesamiento/Limpieza}: En la fase de data cleaning se examina la calidad de los datos y se manejan situaciones como     datos con parámetros faltantes/nulos, datos duplicados, etc.
    \item \textbf{Transformación/Reducción}: Convertir datos pre-procesados en utilizables, identificando características importantes según los objetivos del proceso.
    \item \textbf{Minería de Datos}: Búsqueda de patrones de interés mediante técnicas como clasificación, regresión, clustering, correlaciones, etc.
    \item \textbf{Interpretación/Evaluación}: Fase final, de consolidación del conocimiento encontrado en los datos. Se preparan los resultados para documentación y toma de decisiones. Los datos se han transformado en visualizaciones para facilitar la evaluación del resultado depurado.
\end{itemize}

\subsection{CRISP-DM (Cross-Industry Standard Process for Data Mining)}
Publicada en 1999 con el objetivo de estandarizar los procesos de minería de datos en diversos sectores, se ha consolidado como la metodología más utilizada en proyectos de minería de datos, análisis y ciencia de datos:
\begin{itemize}
    \item \textbf{Comprensión Empresarial}: se centra en entender los objetivos del proyecto, para luego ser evaluados y descubrir si     los datos son aptos para cumplir con los objetivos y producir un plan de proyecto.
    \item \textbf{Comprensión de Datos}: Recopilación, descripción y exploración de los datos iniciales.
    \item \textbf{Preparación de Datos}: 5 tareas: selección de datos, limpieza de datos, construcción de datos, integración de datos y ajuste del formato.
    \item \textbf{Modelado}: Construcción y evaluación de varios modelos con diferentes técnicas algorítmicas. Se determina que algoritmos probar (por ejemplo, regresión, red neuronal); se realiza un     diseño de experimentos; construir el modelo y evaluar el modelo.
    \item \textbf{Evaluación}: Se evalúa y revisa la creación de modelos respecto a los objetivos comerciales. Para ello se evaluan los resultados, se revisan los procesos y se determinan los próximos pasos.
    \item \textbf{Implementación}: Despliegue, seguimiento, mantenimiento y revisión final de los resultados obtenidos.
\end{itemize}
CRISP-DM presenta un proceso iterativo estructurado, definido y documentado. Es una metodología empleada como referencia por otras metodologías.

\subsection{SEMMA (Sample, Explore, Modify, Model, Assess)}
Propuesta para manejo de grandes volúmenes de datos:
\begin{itemize}
    \item \textbf{Muestreo}: Selección de una muestra representativa de datos del problema que está investigando. La forma correcta de obtener una muestra es la selección aleatoria.
    \item \textbf{Exploración}: Exploración de información útil, con la finalidad de sintetizar el problema y mejorar la eficiencia del modelo.
    \item \textbf{Modificación}: Manipulación de los datos con base en la investigación realizada para que los datos ingresados al modelo estén definidos y en un formato adecuado.
    \item \textbf{Modelado}: Modelado de datos, con el propósito de establecer una relación entre las variables explicativas y el objeto de estudio.
    \item \textbf{Evaluación}: Validación comparativa de los resultados, a través del análisis de los modelos, comparado con otros modelos estadísticos o una nueva muestra poblacional. 
\end{itemize}

\subsection{RAMSYS (Rapid collaborative data Mining System)}
Desarrollada por Steve Moyle en 2002. Es una metodología que apoya proyectos de minería de datos, por ello amplía el método CRISP-DM.\\
Define su metodología en tres roles:
\begin{itemize}
    \item \textbf{Modeladores}: encargados de probar la viabilidad de las hipótesis y generar nuevos conocimientos.
    \item \textbf{Data Master}: responsable de mantener la versión actual de la base de datos, las transformaciones y la información sobre los datos, como metadatos e información sobre la calidad de los datos.
    \item \textbf{Comité de Dirección}: responsable de establecer los desafíos del proyecto, definir criterios, recibir y seleccionar las presentaciones.
    %\item Enfatiza colaboración e intercambio de conocimiento
    %\item Permite experimentación con diversas técnicas de resolución de problemas
\end{itemize}

% \subsection{CATALYST (P3TQ)}
% Conocida como P3TQ por sus siglas en inglés product, place, price, time and quantity, que existen en la cadena de valor organizacional. Está formada por dos partes o sub-metodologías. La primera denominada modelado de negocio o MII, y la segunda llamada minería de datos o MIII. \\
% \begin{itemize}
%     \item \textbf{Metodología para el Modelado del Negocio (MII)}:
    
%     Brinda una guía de pasos para reconocer un problema y las necesidades reales de la empresa. Con ella se busca modelar el problema/oportunidad que aborda el proyecto.\\
%     El proceso incluye: exploración de datos en búsqueda de patrones de interés, identificación de problemas/oportunidades, prospección de capacidades de minería de datos y el desarrollo de estrategias organizacionales.
%     \item \textbf{Metodología para la Minería de Datos (MIII)}: se divide en los siguientes pasos:\\
%     - Preparación y exploración de datos para encontrar relaciones útiles.\\
%     - Selección de herramientas y modelos para analizar el problema.\\
%     - Refinamiento iterativo del modelo. Verificar los resultados, matrices, gráficos u observaciones según sea el método exploratorio o predictivo.\\
%     - Implementación del modelo  incluyendo una revisión de la retroalimentación con los usuarios.
% \end{itemize}

\subsection{TDSP (Team Data Science Process)}
Metodología de Microsoft de 2017. Es de cierta forma una combinación de Scrum y CRISP-DM. El ciclo de vida de TDSP se compone de cinco etapas principales:
\begin{itemize}
    \item \textbf{Comprensión empresarial}: Se definen los objetivos y se identifican las fuentes de datos.
    \item \textbf{Adquisición y comprensión de datos}: Se incorporan los datos y se determina si se puede responder a la pregunta 
    planteada (combina efectivamente la Comprensión de los Datos y la Limpieza de los Datos de CRISP-DM).
    \item \textbf{Modelado}: Ingeniería de características (feature engineering) y entrenamiento de modelos (model training). Combina Modelado y Evaluación de CRISP-DM).
    \item \textbf{Implementación}: Implementar en un entorno de producción.
    \item \textbf{Aceptación del cliente}: Validación por parte del cliente de si el sistema satisface las necesidades del negocio (una fase no cubierta explícitamente por CRISP-DM).
\end{itemize}
TDSP aborda la debilidad de CRISP-DM en cuanto a la falta de definición del equipo, definiendo seis roles:
\begin{itemize}
    \item Arquitecto de soluciones
    \item Project Manager
    \item Ingeniero de datos
    \item Científico de datos
    \item Desarrollador de aplicaciones
    \item Líder de proyecto (Project lead)
\end{itemize}

\subsection{Conclusiones del tema}

A lo largo de esta conferencia, hemos explorado diversas metodologías utilizadas en la Ciencia de Datos. Aunque cada una tiene sus particularidades, todas comparten elementos comunes:

\begin{itemize}
    \item \textbf{Comprensión del Negocio}: Definir el problema empresarial y se identifican los objetivos del análisis. El equipo de ciencia de datos debe trabajar en estrecha colaboración con los clientes para entender el problema y definir los objetivos.
    \item \textbf{Comprensión de los Datos}: Identificar y recopilar los datos requerido para el análisis. Exploración de los datos para entender su estructura, calidad y completitud.
    \item \textbf{Preparación de los Datos}: Limpiar, transformar y preparar los datos para garantizar que estén en el formato y calidad adecuados para el análisis.
    \item \textbf{Modelado de Datos}: Seleccionar técnicas de modelado adecuadas para analizar los datos e implementar modelos predictivos. Esta etapa también involucra la selección de algoritmos, ajuste de parámetros y validación del modelo.
    \item \textbf{Evaluación}: Evaluar el rendimiento del modelo y su capacidad para resolver el problema empresarial. Utilizando métricas de evaluación adecuadas y realizando mejoras al modelo si es necesario.
    \item \textbf{Despliegue}: Desplegar el modelo en un entorno de producción, integrándolo en los procesos de la negocio y asegurando su correcto funcionamiento.
    \item \textbf{Monitoreo y Mantenimiento}: Supervisar el rendimiento del modelo en producción y realizar ajustes para mantener su efectividad.
\end{itemize}

En resumen, una metodología de Ciencia de Datos es un enfoque estructurado que combina comprensión del negocio, manejo de datos, modelado y evaluación para transformar datos en soluciones efectivas. Sin embargo, es crucial recordar que:

\begin{itemize}
    \item \textbf{No existe una metodología universal}: Cada proyecto tiene características únicas que pueden requerir adaptaciones o combinaciones de enfoques.
    \item \textbf{La flexibilidad es clave}: Las metodologías no deben aplicarse de manera rígida, sino como guías que permitan ajustarse a las necesidades específicas del problema.
    \item \textbf{La colaboración es esencial}: La comunicación entre científicos de datos, ingenieros y stakeholders es fundamental para alinear objetivos y garantizar resultados útiles.
    \item \textbf{El ciclo nunca termina}: La Ciencia de Datos es un proceso iterativo, donde el monitoreo y la mejora continua son parte integral del éxito.
\end{itemize}

\section{Ingeniería de Datos}

Para concluir vamos a definir en que consiste el trabajo de un ingeniero de datos. 

En el libro \textit{Fundamentals of Data Engineering} de \textit{Joe Reis and Matt Housley} se define el término como:

La Ingeniería de Datos consiste en el desarrollo, interpretación y mantenimiento de sistemas y procesos que toman datos crudos y producen información consistente de alta calidad que soporta downstream casos de uso como analíticas y machine learning.

Un ingeniero de datos maneja el ciclo de vida de la ingeniería de datos que abarca el proceso que comienza en obtener los datos de las fuentes y termina sirviéndolos, después de procesados para los casos de uso.

\textbf{En este curso estaremos viendo la ingeniería de datos como proceso integral que abarca la recolección, almacenamiento, procesamiento, y disponibilidad de datos.}\\

\subsection{Breve historia de la evolución de la ingeniería de datos}
\textbf{Los comienzos: 1980 a 2000} de \textemp{Data Warehouse} a la Web.\\
El nacimiento de la ingeniería de datos tiene sus raíces en data warehousing\footnote{Centralized repository that aggregates data from various sources to support data analysis, data mining and artificial intelligence}.\\
Que data desde la década de 1970 y toma forma en los 80s cuando se crea el término data warehouse. Luego surge el Structured Query Language (SQL).
Mientras crecían los nacientes sistemas de datos, los negocios necesitaban dedicar herramientas a reportar y a business intelligence (BI). 
Surgiendo roles como BI ingeniero\footnote{A business intelligence engineer designs, implements, and maintains systems used to collect and analyze business intelligence data.}, desarrolladores ETL\footnote{Responsible for designing, building, managing, and maintaining ETL (Extract, Transform, Load) processes.} e ingenieros de data warehouse.
Precursores de lo que se considera actualmente los ingenieros de datos. También se popularizó el internet a mediados de los 90s, surgiendo una ola de compañías centradas en la web.

\textbf{Los inicios de los 2000}: El nacimiento de la ingeniería de datos contemporánea.\\
Las grandes compañías sobrevivientes como Yahoo, Google y Amazon crecerían hasta convertirse en gigantes tecnológicos. 
La necesidad de sistemas escalables, con alta disponibilidad, confiables y cost-effective llevó a innovaciones en sistemas distribuidos y almacenamiento, marcando el inicio de la era del 'big data'. La combinación del surgimiento de nuevos algoritmos y metodologías como: 'Google File System', 'MapReduce', Apache Hadoop, Amazon Elastic Compute Cloud y su apertura como servicio al público a través de Amazon Web Services (AWS) creó una nueva era en el manejo y tratamiento de datos.
Nacía la era de ingeniero de big data.\\
Mientras AWS se volvió muy rentable otras compañías lanzaros sus propios ecosistemas en la nube: Google Cloud, Microsoft Azure, DigitalOcean. La nube es discutiblemente una de las innovaciones más significativas del siglo 21; iniciando una revolución en la forma en que el software y las aplicaciones de datos se desarrollan y despliegan

\textbf{Finales de los 2000 y la década de 2010}: The Big Data Engineering Era\\
Surgen herramientas open source que democratizan el acceso a tecnologías de big data; que ya no estarían limitadas solo a las grandes compañías. \\
También comienza la transformación de procesamiento en batch a streaming a partir de eventos. \\
Ocurre una explosión de herramientas de manejo de datos. Y los ingenieros de bog data debían ser proficientes en desarrollo de software y configuración de infraestructuras de bajo nivel. \\
Big data engineers se centraban en manejar sistemas de datos de gran escala, pero la complejidad y el costo de mantener  estas nuevas herramientas impulsaron a optar por simplificaciones. \\
El término "big data" perdió su brillo mientras se volvían más accesibles las herramientas para procesarlos; los ingenieros de big data engineers pasan a ser simplemente ingenieros de datos.

\textbf{2020s}: Ingeniería por el ciclo de vida de los datos.\\
El rol de los ingenieros de datos se encuentra en rápida evolución. La tendencia está siendo centrarse en herramientas descentralizadas, modulares y abstractas. 
Las tendencias populares a principios de la década de 2020 incluyen el modern data stack (MDS), que representa una colección de productos "listos para usar", tanto de código abierto como de terceros, ensamblados para facilitar el trabajo de los analistas. Al mismo tiempo, las fuentes de datos y los formatos de datos están creciendo tanto en variedad como en tamaño. La ingeniería de datos es, cada vez más, una disciplina de interconexión, que conecta varias tecnologías como si fueran piezas de LEGO, para servir a los objetivos empresariales finales. 

\subsection{Relación entre la Ingeniería de Datos y la Ciencia de Datos}
La ingeniería de datos se sitúa upstream de la ciencia de datos, lo que significa que los ingenieros de datos proporcionan las entradas utilizadas por los científicos de datos.\\
Para muchos lo más interesante de la ciencia de datos es construir y optimizar modelos de Machine Learning; la realidad es que se estima que entre el 70\% y el 80\% del tiempo se dedica a recopilar, limpiar y procesar datos.\\
Además; usualmente los científicos de datos no están entrenados para diseñar sistemas de datos de grado de producción, y terminan haciendo este trabajo improvisadamente porque carecen del soporte y los recursos de un ingeniero de datos.\\
En un mundo ideal, los científicos de datos deberían dedicar más del 90\% de su tiempo al análisis, la experimentación y el ML. Esto se logra cuando los ingenieros de datos se centran en construir una base sólida para que los científicos de datos tengan éxito.

\subsection{Habilidades del Ingeniero de Datos}
El conjunto de habilidades de un ingeniero de datos debe abarcar las ideas subyacentes de la ingeniería de datos: seguridad, gestión de datos, DataOps, arquitectura de datos e ingeniería de software. Se requiere un entendimeinto de como evaluar herramientas de datos y como estas encajan en el ciclo de vida de la ingeniería de datos. 

Un ingeniero de datos maneja una gran cantidad de piezas móviles complejas y debe optimizar constantemente a lo largo de los ejes de costo, agilidad, escalabilidad, simplicidad, reutilización e interoperabilidad. También se espera que el ingeniero de datos cree arquitecturas de datos ágiles que evolucionen a medida que surjan nuevas tendencias.

Por definición, un ingeniero de datos debe comprender tanto los datos como la tecnología. Con respecto a los datos, esto implica conocer varias de las mejores prácticas en torno a la gestión de datos. En el extremo tecnológico, un ingeniero de datos debe estar al tanto de varias opciones de herramientas, su interrelación y sus trade-offs.

% Chapter 2: Conference 2
\chapter{Conferencia 2}
\normalfont\LARGE \textbf{El ciclo de vida de la ingeniería de datos.}
\normalfont\small\\

\section{El Ciclo de Vida de la Ingeniería de Datos (Data Engineering Lifecycle)}
El ciclo de vida de la ingeniería de datos o Data Engineering Lifecycle.

Estudiaremos este ciclo de forma agnóstica a un tipo de software o hardware específico.
La idea es alejarnos de las tecnologías y centrarnos en los datos y el propósito que deben servir.

El ciclo de vida de la ingeniería de datos comprende las siguientes etapas que transforman datos crudos en un producto final con valor listo para ser consumido por los analistas, científicos de datos, ingenieros de ML y otros:

\begin{itemize}
    \item Generación
    \item Almacenamiento
    \item Ingesta
    \item Transformación
    \item Serving
\end{itemize}
Además de las etapas, el ciclo tiene una noción de ideas subyacentes, críticas a o largo de todo el ciclo de vida. Estas incluyen seguridad, gestión de datos, DataOps, arquitectura de datos, orquestación e ingeniería de software.

El almacenamiento ocurre a lo largo de todo el ciclo a medida que los datos fluyen desde el inicio hasta el final.\\
En general, las etapas intermedias (almacenamiento, ingesta, transformación) pueden mezclarse un poco. Varias etapas del ciclo de vida pueden repetirse, ocurrir fuera de orden, superponerse o entrelazarse.

El ciclo de vida de la ingeniería de datos es un subconjunto de todo el ciclo de vida de los datos.

Un ingeniero de datos tiene varios objetivos de alto nivel a lo largo del ciclo de vida de los datos: producir un Return on Investment(ROI) y reducir los costos, reducir el riesgo y maximizar el valor y la utilidad de los datos.

\subsection{Generación}

Un \textit{sistema de origen} es el origen de los datos utilizados en el ciclo de vida de la ingeniería de datos. Por ejemplo, un sistema de origen podría ser un dispositivo IoT, una cola de mensajes de aplicación o una base de datos transaccional.

Un ingeniero de datos consume datos de un sistema de origen, pero normalmente no posee ni controla el sistema de origen en sí. El ingeniero de datos necesita tener un entendimiento funcional de cómo funcionan los sistemas de origen, la forma en que generan los datos, la frecuencia y la velocidad de los datos, y la variedad de datos que se generan.\\
También necesitan mantener una línea de comunicación abierta con los propietarios del sistema de origen sobre los cambios que podrían interrumpir los pipelines y los análisis.

Las fuentes producen datos consumidos por sistemas posteriores, incluidas hojas de cálculo generadas por humanos, sensores IoT y aplicaciones web y móviles. Cada fuente tiene su volumen y cadencia únicos de generación de datos.

Ejemplos de sistemas de origen: 
\begin{itemize}
    \item Dispositivos IoT
    \item Terminales de tarjetas de crédito
    \item Operaciones de la bolsa
    \item Sensores de telescopios
    \item Spreadsheets
\end{itemize}

Tipos de sistemas de origen:
\begin{itemize}
    \item Archivos y datos no estructurados (Excel, CSV, JSON, XML, TXT). Estos archivos tienen sus peculiaridades y pueden ser estructurados (Excel, CSV), semi-estructurados (JSON, XML, CSV) o no estructurados (TXT, CSV).
    \item APIs
    \item Bases de Datos: Pueden ser relacionales(SQL), o no relacionales como: Document stores, Wide-column, Graph databases, 
    \item Application Databases (online transaction processing (OLTP) system) : Una base de datos de aplicaciones almacena el estado de una aplicación. Un ejemplo típico es una base de datos que almacena los saldos de cuentas bancarias. A medida que se producen transacciones y pagos de los clientes, la aplicación actualiza los saldos de las cuentas bancarias. Normalmente, una base de datos de aplicaciones es un sistema de procesamiento de transacciones en línea (OLTP): una base de datos que lee y escribe registros de datos individuales a una alta velocidad.
    %\item Change data capture (CDC) La captura de datos modificados (CDC, por sus siglas en inglés) es un método para extraer cada evento de cambio (inserción, actualización, eliminación) que ocurre en una base de datos. La CDC se utiliza con frecuencia para replicar entre bases de datos casi en tiempo real o para crear un flujo de eventos para el procesamiento posterior.
    \item Fuentes de Datos de Terceros: Debido a que la tecnología se ha integrado en muchas empresas (y agencias gubernamentales), estas buscan ofrecer sus datos a clientes y usuarios. El acceso directo a datos de terceros se realiza comúnmente a través de APIs, mediante el intercambio de datos en una plataforma en la nube o mediante la descarga de datos. 
    \item Colas de Mensajes y Plataformas de Streaming de Eventos: Las arquitecturas basadas en eventos (event-driven) son cada vez más populares en software. Además, las aplicaciones que integran analítica en tiempo real (data apps) se benefician de las arquitecturas basadas en eventos, ya que los eventos disparan acciones en la aplicación y alimentan el análisis en tiempo real.
\end{itemize}

Ahora se presentan preguntas para evaluar sistemas de origen que los ingenieros de datos deben considerar:
\begin{itemize}
    \item ¿Cuáles son las características esenciales de la fuente de datos? ¿Es una aplicación? ¿Un enjambre de dispositivos IoT?
    \item ¿Cómo se persisten los datos en el sistema de origen? ¿Los datos se persisten a largo plazo, o son temporales y se eliminan rápidamente?
    \item ¿A qué velocidad se generan los datos? ¿Cuántos eventos por segundo? ¿Cuántos gigabytes por hora?
    \item ¿Qué nivel de consistencia pueden esperar los ingenieros de datos de los datos de salida? Si se están ejecutando comprobaciones de calidad de datos contra los datos de salida, ¿con qué frecuencia se producen inconsistencias de datos: valores nulos donde no se esperan, formato deficiente, etc.?
    \item ¿Con qué frecuencia se producen errores?
    \item ¿Los datos contendrán duplicados?
    \item ¿Algunos valores de datos llegarán tarde, posiblemente mucho más tarde que otros mensajes producidos simultáneamente?
    \item ¿Cuál es el esquema de los datos ingeridos? ¿Los ingenieros de datos necesitarán unir varias tablas o incluso varios sistemas para obtener una imagen completa de los datos?
    \item Si hay cambios en el esquema (por ejemplo, se agrega una nueva columna), ¿cómo se aborda esto y se comunica a las partes interesadas en el resto del procesamiento?
    \item ¿Con qué frecuencia se deben extraer los datos del sistema de origen?
    \item Para los sistemas con estado (por ejemplo, una base de datos que rastrea la información de la cuenta del cliente), ¿se proporcionan los datos como instantáneas periódicas o eventos de actualización de la captura de datos de cambio (change data capture CDC)? ¿Cuál es la lógica de cómo se realizan los cambios y cómo se rastrean estos en la base de datos de origen?
    \item ¿Quién/qué es el proveedor de datos que transmitirá los datos para el consumo posterior?
    \item ¿Leer de una fuente de datos afectará su rendimiento?
    \item ¿El sistema de origen tiene dependencias anteriores? ¿Cuáles son las características de estos sistemas anteriores?
    \item ¿Existen controles de calidad de datos para verificar datos tardíos o faltantes?
\end{itemize}

\subsection{Almacenamiento}
Whether data is needed seconds, minutes, days, months, or years later, it must persist in storage until systems are ready to consume it for further processing and transmission.

El proceso de elección de almacenamiento es clave para el éxito en el resto del ciclo de vida de los datos, y también es una de las etapas más complicadas. 
Primero, las arquitecturas de datos en la nube a menudo se aprovechan de varias alternativas de almacenamiento. 
Segundo, pocas herramientas de almacenamiento de datos funcionan puramente como almacenamiento, y muchas admiten complejas queries de transformación de datos; incluso el almacenamiento basado en objetos pueden admitir potentes capacidades de consulta, por ejemplo, Amazon S3 Select. 
Tercero, si bien el almacenamiento es una etapa del ciclo de vida de la ingeniería de datos, con frecuencia toca otras etapas, como la ingesta, la transformación y el serving.

El almacenamiento se extiende a lo largo de todo el ciclo de vida de la ingeniería de datos, y a menudo en múltiples lugares del pipeline de datos. En muchos sentidos, la forma en que se almacenan los datos impacta en cómo se utilizan en todas las etapas del ciclo de vida de la ingeniería de datos. Por ejemplo, los almacenes de datos en la nube pueden almacenar datos, procesar datos en pipelines y servirlos a los analistas. 

\textbf{Sistemas de Almacenamiento}:
\begin{itemize}
    \item Almacenamiento de Archivos (File Storage): Estos sistemas organizan los archivos en una estructura de árbol de directorios.
    \item Almacenamiento de Objetos (Object Storage): Contiene objetos de todos los tamaños y formas. El término "almacenamiento de objetos" puede ser confuso debido a los múltiples significados de "objeto" en informática. En este contexto, se refiere a una construcción especializada similar a un archivo.  Puede ser cualquier tipo de archivo: TXT, CSV, JSON, imágenes, videos o audio.  Amazon S3, Azure Blob Storage y Google Cloud Storage (GCS) son almacenes de objetos ampliamente utilizados. Además, muchos almacenes de datos en la nube (y un número creciente de bases de datos) utilizan el almacenamiento de objetos como su capa de almacenamiento, y los data lakes en la nube generalmente se basan en almacenes de objetos.
    \item Sistemas de Almacenamiento Basados en Caché y Memoria: Ofrecen una excelente latencia y velocidades de transferencia. Sin embargo, tradicional son extremadamente vulnerable a la pérdida de datos, ya que un corte de energía, incluso de un segundo, puede borrar los datos. Los sistemas de almacenamiento basados en RAM generalmente se centran en aplicaciones de almacenamiento en caché, presentando datos para un acceso rápido y un alto ancho de banda. Estos sistemas de caché ultrarrápidos son útiles cuando los ingenieros de datos necesitan servir datos con una latencia de recuperación ultrarrápida.
\end{itemize}

\textbf{Abstracciones de Almacenamiento}:
\begin{itemize}
    \item Almacén de Datos (Data Warehouse): El término "almacén de datos" se refiere a plataformas tecnológicas (por ejemplo, Google BigQuery y Teradata), una arquitectura para la centralización de datos y un patrón organizativo dentro de una empresa.
    \item Lago de Datos (Data Lake): Originalmente concebido como un almacén masivo donde los datos se conservaban en forma bruta y sin procesar.
    \item Casa del Lago de Datos (Data Lakehouse): Una arquitectura que combina aspectos del almacén de datos y el lago de datos.  Tal como se concibe generalmente, la "lakehouse" almacena datos en el almacenamiento de objetos al igual que un lago. Sin embargo, la "lakehouse" agrega a esta disposición características diseñadas para optimizar la gestión de datos y crear una experiencia de ingeniería similar a la de un almacén de datos. Esto significa un soporte robusto para tablas y esquemas y características para gestionar actualizaciones y eliminaciones incrementales.  Las "lakehouses" típicamente también soportan el historial de la tabla y la reversión (rollback); esto se logra conservando versiones antiguas de archivos y metadatos.
    \item Plataformas de Datos (Data Platforms): Algo nuevo.
\end{itemize}

Estas son algunas preguntas claves al elegir un sistema de almacenamiento para data warehouse, un data lakehouse, una base de datos o un almacenamiento de objetos(object storage):

\begin{itemize}
    \item ¿Es este sistema de almacenamiento compatible con las velocidades de lectura y escritura requeridas por la arquitectura?
    \item ¿El almacenamiento creará un cuello de botella para los procesos posteriores?
    \item ¿Entienden cómo funciona esta tecnología de almacenamiento? ¿Está utilizando el sistema de almacenamiento de manera óptima o cometiendo actos antinaturales? % Por ejemplo, ¿está aplicando una alta tasa de actualizaciones de acceso aleatorio en un sistema de almacenamiento de objetos? (Este es un antipatrón con una importante sobrecarga de rendimiento).
    \item ¿Este sistema de almacenamiento manejará la prevista escala futura? Debe considerar todos los límites de capacidad en el sistema de almacenamiento: almacenamiento total disponible, tasa de operación de lectura, volumen de escritura, etc.
    \item ¿Los usuarios y procesos posteriores podrán recuperar datos en el service-level agreement (SLA) requerido?
    \item ¿Se está capturando metadatos sobre la evolución del esquema, los flujos de datos, el linaje de datos, etc.? Los metadatos tienen un impacto significativo en la utilidad de los datos. Los metadatos representan una inversión en el futuro, mejorando drásticamente la detectabilidad y el conocimiento institucional para agilizar futuros proyectos y cambios de arquitectura.
    \item ¿Es esta una solución de almacenamiento pura (almacenamiento de objetos) o admite patrones de consulta complejos (por ejemplo, un data warehouse en la nube)?
    \item ¿El sistema de almacenamiento es independiente del esquema (almacenamiento de objetos)? ¿Esquema flexible (Cassandra)? ¿Esquema forzado (un data warehouse en la nube)?
    \item ¿Cómo está rastreando los datos maestros, la calidad de los datos de registros dorados y el linaje de datos para la gobernanza de datos?
    \item ¿Cómo está manejando el cumplimiento normativo y la gobernanza de los datos? Por ejemplo, ¿puede almacenar sus datos en ciertas ubicaciones geográficas pero no en otras?
\end{itemize}

\subsection{Ingesta}
Usualmente, los sistemas de origen y la ingesta representan los cuellos de botella más significativos en el ciclo de vida de la ingeniería de datos. Los sistemas de origen están normalmente fuera de su control directo y podrían dejar de responder aleatoriamente o proveer datos de baja calidad. O, su servicio de ingesta de datos podría misteriosamente dejar de funcionar por muchas razones.

Al prepararse para diseñar o construir un sistema, aquí hay algunas preguntas primarias sobre la etapa de ingesta:
\begin{itemize}
    \item ¿Cuáles son los casos de uso para los datos que estoy ingiriendo? ¿Puedo reutilizar estos datos en lugar de crear múltiples versiones del mismo conjunto de datos?
    \item ¿Los sistemas que generan e ingieren estos datos son fiables, y los datos están disponibles cuando los necesito?
    \item ¿Cuál es el destino de los datos después de la ingesta?
    \item ¿Con qué frecuencia necesitaré acceder a los datos?
    \item ¿En qué volumen llegarán típicamente los datos?
    \item ¿En qué formato están los datos? ¿Pueden mis sistemas de almacenamiento y transformación posteriores manejar este formato?
    \item ¿Están los datos iniciales en buen estado para su inmediato uso posterior? Si es así, ¿por cuánto tiempo, y qué podría causar que se vuelvan inutilizables?
    \item Si los datos provienen de una fuente de \textit{streaming}, ¿necesitan ser transformados antes de llegar a su destino? ¿Sería apropiada una transformación dentro del propio \textit{stream}?
\end{itemize}

\subsubsection{Lotes (\textit{Batch}) vs. \textit{Streaming}}
Virtualmente todos los datos con los que tratamos son inherentemente \textit{streaming}. Los datos son casi siempre producidos y actualizados continuamente en su origen. La ingesta por lotes es simplemente una forma especializada y conveniente de procesar este *stream* en grandes trozos—por ejemplo, manejar el valor de un día completo de datos en un solo lote.

La ingesta de \textit{streaming} nos permite proveer datos a los sistemas posteriores—ya sean otras aplicaciones, bases de datos, o sistemas de analítica—de una forma continua y en tiempo real.

La elección depende en gran medida del caso de uso y las expectativas de puntualidad de los datos.

Las siguientes son algunas preguntas que debe hacerse al determinar si la ingesta de \textit{streaming} es una opción apropiada sobre la ingesta por lotes:
\begin{itemize}
    \item Si se ingieren los datos en tiempo real, ¿pueden los sistemas de almacenamiento posteriores manejar la tasa de flujo de datos?
    \item Es necesaria una ingesta de datos en tiempo real de mili-segundos? ¿O funcionaría un enfoque de micro-lotes, acumulando e ingiriendo datos, por ejemplo, cada minuto?
    \item ¿Cuáles son mis casos de uso para la ingesta de \textit{streaming}? ¿Qué beneficios específicos obtengo al implementar \textit{streaming}? Si obtengo datos en tiempo real, ¿qué acciones puedo tomar sobre esos datos que serían una mejora con respecto al lote?
    \item ¿Mi enfoque de priorizar \textit{streaming} costará más en términos de tiempo, dinero, mantenimiento, tiempo de inactividad y costo de oportunidad que simplemente hacer lotes?
    \item ¿Son mi \textit{pipeline} y sistema de \textit{streaming} confiables y redundantes si falla la infraestructura?
    \item ¿Qué herramientas son las más apropiadas para el caso de uso? ¿Debería usar un servicio gestionado (Amazon Kinesis, Google Cloud Pub/Sub, Google Cloud Dataflow) o levantar mis propias instancias de Kafka, Flink, Spark, Pulsar, etc.? Si hago lo último, ¿quién lo administrará? ¿Cuáles son los costos y las ventajas y desventajas?
    \item Si estoy implementando un modelo de ML, ¿qué beneficios tengo con las predicciones en línea y posiblemente el entrenamiento continuo?
    \item ¿Estoy obteniendo datos de una instancia de producción en vivo? Si es así, ¿cuál es el impacto de mi proceso de ingesta en este sistema de origen?
\end{itemize}

\subsubsection{Push vs. Pull}
En el modelo de ingesta de datos de \textit{push} un sistema de origen envía datos hacia un destino; ya sea una base de datos, un almacén de objetos o un sistema de archivos.
En el modelo de \textit{pull} los datos se recuperan del sistema de origen.

El proceso de extracción, transformación y carga (ETL) comúnmente utilizado en flujos de trabajo de ingesta orientados a lotes, La parte de extracción (E) de ETL aclara que estamos lidiando con un modelo de ingesta de tipo \textit{pull}.

Con la ingesta de \textit{streaming}, los datos evitan una base de datos \textit{backend} y se envían (\textit{push}) directamente a un punto final, típicamente con datos almacenados en búfer por una plataforma de \textit{event-streaming}. Este patrón es útil con flotas de sensores IoT que emiten datos de sensores. En lugar de depender de una base de datos para mantener el estado actual, simplemente pensamos en cada lectura registrada como un evento.
Este patrón también está creciendo en popularidad en aplicaciones de software, ya que simplifica el procesamiento en tiempo real, permite a los desarrolladores de aplicaciones adaptar sus mensajes para análisis posteriores, y simplifica enormemente la vida de los ingenieros de datos.

\subsection{Transformación}
La siguiente etapa en el ciclo de vida de la ingeniería de datos es la transformación, lo que significa que los datos deben ser modificados desde su forma original a algo útil para los casos de uso posteriores.\\
Típicamente, la etapa de transformación es donde los datos comienzan a crear valor para el consumo por parte de los usuarios posteriores.\\
Inmediatamente después de la ingesta, las transformaciones básicas mapean los datos a los tipos correctos (cambiando los datos de tipo cadena ingeridos a tipos numéricos y de fecha, por ejemplo), colocando los registros en formatos estándar y eliminando los incorrectos.\\
Las etapas posteriores de la transformación pueden transformar el esquema de datos y aplicar normalización. \\
En etapas posteriores, podemos aplicar agregaciones a gran escala para la elaboración de informes o la \textit{featurización} de datos para los procesos de aprendizaje automático (ML).\\

Consideraciones clave:
\begin{itemize}
    \item ¿Cuál es el costo y el retorno de la inversión (ROI) de la transformación? ¿Cuál es el valor comercial asociado?
    \item ¿Es la transformación tan simple y auto-aislada como sea posible?
    \item ¿Qué reglas de negocio las transformaciones soportan?
\end{itemize}
Se pueden transformar los datos en lotes o en \textit{streaming} en tiempo real.
Las transformaciones por lotes son abrumadoramente populares, pero dada la creciente popularidad de las soluciones de procesamiento en \textit{streaming} y el aumento general en la cantidad de datos en \textit{streaming}, se espera que la popularidad de las transformaciones en \textit{streaming} continúe creciendo.

La transformación a menudo está entrelazada con otras fases del ciclo de vida. Típicamente, los datos se transforman en los sistemas de origen o en tiempo real durante la ingesta. 
Por ejemplo, un sistema de origen puede agregar una marca de tiempo de evento a un registro antes de reenviarlo a un proceso de ingesta. O un registro dentro de un \texit{pipeline} de \textit{streaming} puede ser "enriquecido" con campos y cálculos adicionales antes de ser enviado a un data warehouse.

\textbf{Lógica de Negocio}
La lógica de negocio es un importante impulsor de la transformación de datos, a menudo en el modelado de datos. Los datos traducen la lógica de negocio en elementos reutilizables.

El \textit{featuring} de datos para ML es otro proceso de transformación de datos. Tiene como objetivo extraer y mejorar las características de los datos que son útiles para el entrenamiento de modelos de ML. Puede ser un arte oscuro, que combina el conocimiento del dominio (para identificar qué características podrían ser importantes para la predicción) con una amplia experiencia en ciencia de datos.\\
El punto principal es que una vez que los científicos de datos determinan cómo \textit{featurize} los datos, los procesos de \textit{featuring} pueden ser automatizados por los ingenieros de datos en la etapa de transformación de un \texit{pipeline} de datos.

Algunos de los elementos fundamentales de la etapa de transformación son: Preparación de datos, manipulación y limpieza de datos; consultas, modelado de datos.

Puede ocurrir en lotes o en stream.

\textbf{Transformaciones por Lotes (Batch)} se ejecutan en fragmentos discretos de datos, y pueden programarse a intervalos fijos (e.g., diario, por hora, o cada 15 minutos) para soportar informes, análisis y modelos de ML. \\
Un patrón de transformación extendido desde los inicios de las bases de datos relacionales es el ETL por lotes. El ETL tradicional se basa en un sistema de transformación externo para extraer, transformar y limpiar los datos, preparándolos para un esquema objetivo; como un almacén de datos, donde se pueden realizar análisis de negocio. La fase de extracción solía ser un cuello de botella importante, limitando la velocidad a la que se podían extraer los datos.\\
Una evolución popular del ETL es el ELT. A medida que los almacenes de datos han crecido en rendimiento y capacidad de almacenamiento, se ha vuelto común simplemente extraer los datos en bruto de un sistema fuente, importarlos al almacén de datos con una transformación mínima, y luego limpiarlos y transformarlos directamente en el sistema del almacén. Una segunda noción de ELT, ligeramente diferente, se popularizó con la aparición de los data lakes. En esta versión, los datos no se transforman al cargarlos. De hecho, se pueden cargar cantidades masivas de datos sin preparación ni plan alguno. La suposición es que el paso de transformación ocurrirá en un momento futuro indeterminado.\\
La data wrangling toma datos desordenados y mal formados y los convierte en datos útiles y limpios. Generalmente, este es un proceso de transformación por lotes.\\

\textbf{Transformaciones en Streaming} los datos se procesan continuamente a medida que llegan.\\
Change Data Capture (CDC): La captura de datos modificados (CDC) es un método para extraer cada evento de cambio (inserción, actualización, eliminación) que ocurre en una base de datos. La CDC se utiliza con frecuencia para replicar entre bases de datos casi en tiempo real o para crear un flujo de eventos para procesamientos posteriores.\\
El enfoque del fast-follower: Las bases de datos de producción generalmente no están equipadas para manejar cargas de trabajo de producción y ejecutar simultáneamente grandes escaneos analíticos sobre cantidades significativas de datos. Ejecutar tales consultas puede ralentizar la aplicación de producción o incluso provocar que se bloquee. Uno de los patrones de consulta de streaming más antiguos consiste simplemente en consultar la base de datos de análisis, recuperando resultados estadísticos y agregaciones con un ligero retraso con respecto a la base de datos de producción.\\

\subsection{Serving}
Ahora que los datos han sido ingeridos, almacenados y transformados en estructuras coherentes y útiles, es hora de obtener valor de los datos. "Obtener valor" de los datos significa diferentes cosas para diferentes usuarios.

\subsubsection{Analítica}
La analítica es el núcleo de la mayoría de los esfuerzos de datos. Una vez que sus datos están almacenados y transformados, está listo para generar informes o *dashboards* y realizar análisis *ad hoc* sobre los datos. Mientras que la mayor parte de la analítica solía abarcar la inteligencia empresarial (BI), ahora incluye otras facetas como la analítica operativa y la analítica integrada.

La Inteligencia Empresarial (BI) aprovecha los datos recopilados para comprender el rendimiento pasado y presente de una empresa.\\
La Analítica Operativa se centra en la información en tiempo real sobre las operaciones de negocio, impulsando la acción inmediata. Piense en vistas de inventario en vivo o *dashboards* en tiempo real que monitorean la salud del sitio web o de la aplicación. A diferencia de la BI, la analítica operativa enfatiza el presente, enfocándose menos en las tendencias históricas.\\
La Analítica Integrada (\texit{Embedded Analytics}), o analítica orientada al cliente, es la práctica de integrar capacidades analíticas directamente en un producto o plataforma de *software*.
Si bien aparentemente similar a la BI, la analítica integrada presenta desafíos únicos. Servir analítica a una gran base de clientes requiere tasas de solicitud significativamente mayores, lo que exige sistemas analíticos escalables y robustos. Lo más crítico es que el control de acceso se vuelve primordial. Cada cliente debe ver solo sus datos, y cualquier fuga de datos es una grave violación de la confianza con consecuencias potencialmente catastróficas.\\

\subsubsection{Machine Learning}
Algunas consideraciones para la fase de servicio de datos específicas para ML:
\begin{itemize}
    \item ¿Son los datos de calidad suficiente para realizar una ingeniería de features fiable? Los requisitos y las evaluaciones de calidad se desarrollan en estrecha colaboración con los equipos que consumen los datos.
    \item ¿Son los datos localizables? ¿Pueden los científicos de datos y los ingenieros de ML encontrar fácilmente datos valiosos?
    \item ¿Dónde están los límites técnicos y organizacionales entre la ingeniería de datos y la ingeniería de ML? Esta cuestión organizativa tiene importantes implicaciones arquitectónicas.
    \item ¿El conjunto de datos representa adecuadamente el \texit{ground truth}? ¿Está sesgado injustamente?
\end{itemize}

\subsubsection{ETL Inversa} % TODO research
El ETL inverso toma los datos procesados del lado de salida del ciclo de vida de la ingeniería de datos y los devuelve a los sistemas de origen. El ETL inverso nos permite tomar analíticas, modelos puntuados, etc., y devolverlos a los sistemas de producción o plataformas SaaS.

\subsection{Principales Corrientes Subyacentes en el Ciclo de Vida de la Ingeniería de Datos}

\subsubsection{Seguridad}
La seguridad debe ser una prioridad para los ingenieros de datos. Deben comprender tanto la seguridad de los datos como la seguridad del acceso, ejerciendo el principio de privilegio mínimo. El principio de privilegio mínimo significa dar a un usuario o sistema acceso solo a los datos y recursos esenciales para realizar una función prevista.

La seguridad de los datos también se trata de sincronización: proporcionar acceso a los datos exactamente a las personas y sistemas que necesitan acceder a ellos y solo durante el tiempo necesario para realizar su trabajo.

\subsubsection{Gestión de Datos}
Los ingenieros de datos gestionan el ciclo de vida de los datos, y la gestión de datos abarca el conjunto de mejores prácticas que los ingenieros de datos utilizarán para llevar a cabo esta tarea, tanto técnica como estratégicamente.

% La gestión de datos tiene bastantes facetas, incluyendo las siguientes:
% \begin{itemize}
%     \item Data governance, including discoverability and accountability
%     \item Modelado y diseño de datos
%     \item Data lineage
%     \item Almacenamiento y operaciones
%     \item Data integration and interoperability
%     \item Data lifecycle management
%     \item Data systems for advanced analytics and ML
%     \item Ethics and privacy
% \end{itemize}


\subsubsection{DataOps}
DataOps mapea las mejores prácticas de la metodología Agile, DevOps y el control estadístico de procesos (SPC) a los datos. Mientras que DevOps tiene como objetivo mejorar la publicación y la calidad de los productos de *software*, DataOps hace lo mismo para los productos de datos.

DataOps tiene tres elementos técnicos centrales: automatización, monitorización y observabilidad, y respuesta a incidentes.

La automatización de DataOps tiene un marco de trabajo y un flujo de trabajo similares a los de DevOps, que consisten en la gestión de cambios (control de versiones de entorno, código y datos), integración continua/despliegue continuo (CI/CD) y configuración como código.\\

\subsubsection{Data Architecture}
Una arquitectura de datos define los planos del sistema de datos de una organización. Es usualmente trabajo para el arquitecto de datos. 
Típicamente es un rol separado del ingeniero de datos, pero este debe implementar los diseños y proveer feedback

\subsubsection{Orquestación}
La orquestación es el proceso de coordinar muchos trabajos para que se ejecuten de la manera más rápida y eficiente posible en una cadencia programada. Un motor de orquestación construye metadatos sobre las dependencias de los trabajos, generalmente en la forma de un grafo acíclico dirigido (DAG). El DAG se puede ejecutar una vez o programarse para que se ejecute a un intervalo fijo de diariamente, semanalmente, cada hora, cada cinco minutos, etc.
% TODO research
\subsubsection{Ingeniería de Software}
Algunas áreas comunes de la ingeniería de software que se aplican al ciclo de vida de la ingeniería de datos.

\textbf{Código de Procesamiento de Datos Central (Core Data Processing Code)}: Escribir código eficiente y comprobable (Spark, SQL, etc.) para la ingesta, transformación y servicio de datos es fundamental para todas las etapas del ciclo de vida de la ingeniería de datos. Las metodologías de prueba adecuadas (pruebas unitarias, de regresión, de integración, de extremo a extremo y de humo) garantizan la calidad y confiabilidad de los datos.

\textbf{Desarrollo de Frameworks de Código Abierto (Open Source Frameworks)}: Los ingenieros de datos a menudo contribuyen y aprovechan las herramientas de código abierto, por lo que comprender su desarrollo y contribuir con mejoras es valioso.

\textbf{Streaming (Procesamiento en Flujo Continuo)}: El procesamiento de datos en tiempo real requiere habilidades y herramientas especializadas. Las tecnologías de streaming son cada vez más importantes en todas las etapas del ciclo de vida de la ingeniería de datos.

\textbf{Infraestructura como Código (IaC - Infrastructure as Code)}: La gestión de la infraestructura (servidores, bases de datos, etc.) a través de código garantiza implementaciones coherentes y repetibles, lo cual es fundamental para las prácticas de DataOps en todo el ciclo de vida de la ingeniería de datos.

\textbf{Pipelines como Código (Pipelines as Code)}: Definir flujos de trabajo de datos y dependencias mediante código permite la orquestación automatizada. Los ingenieros de datos usan código (típicamente Python) para declarar las tareas de datos y las dependencias entre ellas. El motor de orquestación interpreta estas instrucciones para ejecutar los pasos utilizando los recursos disponibles.

\textbf{Desarrollo de Código Personalizado}: Los ingenieros de datos a menudo necesitan escribir código personalizado para manejar escenarios específicos, integrar sistemas y resolver problemas más allá de las capacidades de las herramientas estándar. Son esenciales las sólidas habilidades de ingeniería de software (uso de API, transformación de datos, manejo de excepciones, etc).
\chapter{Conferencia 3}
\normalfont\LARGE \textbf{Tecnologías de la Ingeniería de Datos: Almacenamiento e Ingestión}
\normalfont\small\\
% Architecture is the high-level design, roadmap, and blueprint of data systems that satisfy the strategic aims for the business. Architecture is the what, why, and when. Tools are used to make the architecture a reality; tools are the how.
% Data engineers play a key role in embedding compliance measures into pipelines. These measures include encryption, data lineage, and automated checks, ensuring regulatory compliance at every step.
% Engineers must ensure that systems are scalable, maintainable, and cost-efficient while meeting both business and regulatory requirements.

Abordaremos conceptos generales y haremos un recorrido por las principales tecnologías y herramientas utilizadas en la industria.
El objetivo es que comprendan el panorama general, se familiaricen con los casos de uso clave y, sobre todo, identifiquen qué tecnologías merecen su atención.

\section{Almacenamiento}
\subsection{Tipos de almacenamiento de datos:}
\subsubsection{Bases de Datos Relacionales}
Una base de datos relacional es una colección de información que organiza los datos en tablas (o relaciones). Los datos se almacenan en filas y columnascon una clave única que identifica cada fila. 
Generalmente, cada tabla/relación representa un "tipo de entidad" (como cliente o producto). Las filas representan instancias de ese tipo de entidad (como "Lee" o "silla") y las columnas representan valores atribuidos a esa instancia (como dirección o precio).
Se representan relaciones como conexiones lógicas entre tablas, establecidas en función de sus interacciones.
Cada tabla tiene un esquema predefinido que fuerza a todos los elementos de la tabla a cumplirlos. Utilizan SQL, o variaciones de SQL como lenguaje estándar, que permite crear consultas complejas que abarquen varias tablas.\\

Escalan verticalmente, o sea, para mejorar el rendimiento hay que añadir más recursos (CPU/RAM) al servidor. El enfoque de escalado vertical aumenta la capacidad de una sola máquina incrementando los recursos en el mismo servidor lógico. Esto implica añadir recursos como memoria, almacenamiento y potencia de procesamiento al software existente, mejorando así su rendimiento.

Garantizan el cumplimiento del modelo relacional mediante el concepto de ACID.\\
En los sistemas de bases de datos, ACID (Atomicidad, Consistencia, Aislamiento(isolation), Durabilidad) se refiere a un conjunto estándar de propiedades que garantizan que las transacciones de la base de datos se procesen de forma fiable.
ACID se preocupa especialmente de cómo una base de datos se recupera de cualquier fallo que pueda ocurrir durante el procesamiento de una transacción. Un DBMS compatible con ACID asegura que los datos en la base de datos permanezcan precisos y consistentes a pesar de cualquier fallo de este tipo.

\textbf{Atomicidad} significa que se garantiza que o bien toda la transacción tiene éxito, o bien ninguna parte de ella lo tiene. No se obtiene una parte exitosa y otra que no lo es. Si una parte de la transacción falla, toda la transacción falla. Con la atomicidad, es "todo o nada".\\
\textbf{Consistencia} asegura que se garantiza que todos los datos serán consistentes. Todos los datos serán válidos de acuerdo con todas las reglas definidas, incluyendo cualquier restricción, cascada y disparador (trigger) que se haya aplicado en la base de datos.\\
\textbf{Aislamiento} garantiza que todas las transacciones ocurrirán de forma aislada. Ninguna transacción se verá afectada por ninguna otra transacción. Por lo tanto, una transacción no puede leer datos de ninguna otra transacción que aún no se haya completado.\\
\textbf{Durabilidad} significa que, una vez que una transacción se ha confirmado (committed), permanecerá en el sistema, incluso si hay una caída del sistema inmediatamente después de la transacción. Cualquier cambio de la transacción debe almacenarse permanentemente. Si el sistema le dice al usuario que la transacción ha tenido éxito, la transacción debe, de hecho, haber tenido éxito.\\

RDBMS (Relational Database Management System)=SGBD(R)

\begin{itemize}  % imagenes de los ejemplos de relacionales
    \item Oracle Database: Su primera versión data a 1979. Fue el primer RDBMS SQL disponible comercialmente.
    \item MySQL: MySQL se ofrece en dos ediciones diferentes: MySQL Community Server, de código abierto, y Enterprise Server, de propietario.  
Aunque comenzó como una alternativa de gama baja a bases de datos propietarias más potentes, ha evolucionado gradualmente para dar soporte a necesidades de mayor escala. Todavía se utiliza más comúnmente en implementaciones de un solo servidor, de pequeña a mediana escala. P o como un servidor de bases de datos independiente. Gran parte del atractivo de MySQL se origina en su relativa simplicidad y facilidad de uso, que se ve facilitada por un ecosistema de herramientas de código abierto.
En el rango medio, MySQL se puede escalar desplegándolo en hardware más potente, como un servidor multiprocesador con gigabytes de memoria. 
    \item SQLite: motor de base de datos relacional gratuito y de código abierto. No es una aplicación independiente; más bien, es una biblioteca que los desarrolladores de software incrustan en sus aplicaciones. SQLite fue diseñado para permitir que el programa se opere sin instalar un sistema de gestión de bases de datos o requerir un administrador de bases de datos. Viene instalado default en la mayoría de los sistemas operativos y navegadores (browsers). 
Debido al diseño server-less, las aplicaciones SQLite requieren menos configuración que las bases de datos cliente-servidor. SQLite se conoce como de "zero-configuration"("configuración cero") porque las tareas de configuración, como la gestión de servicios, los scripts de inicio y el control de acceso basado en contraseñas o GRANT, son innecesarias. 
SQLite almacena toda la base de datos, que consta de definiciones, tablas, índices y datos, como un único archivo multiplataforma, lo que permite que varios procesos o subprocesos accedan a la misma base de datos simultáneamente. 
    \item PostgreSQL: es uno de los sistemas de gestión de bases de datos objeto-relacionales de propósito general más avanzados y es de código abierto. Sus características más destacadas incluyen: Herencia de tablas, Replicación asíncrona, Capacidad definir nuevos tipos, así como soporte de muchos tipos que incluyen: 
Numéricos de precisión arbitraria, Arrays (de longitud variable y de cualquier tipo de datos hasta 1 GB de tamaño), Direcciones IPv4 e IPv6,  Identificador único universal (UUID), JSON y un JSONB binario más rápido.\\
Se pueden crear e instalar extensiones o plugins que funcionan como características integradas. A lo largo de los años, se ha desarrollado un rico ecosistema de extensiones que cubren desde la optimización del rendimiento hasta tipos de datos especializados. Ejemplos son:\\
PostGIS: introduce tipos de datos adicionales para la gestión geo-espacial.\\
pgcrypto: agrega funciones criptográficas para el cifrado, el hashing y más.\\
pgvector: Agrega soporte para operaciones vectoriales en Postgres, lo que permite la búsqueda de similitud, la búsqueda del vecino más cercano y más.\\
\end{itemize}

\subsubsection{Bases de Datos No Relacionale (NoSQL)} % imagenes de los ejemplos de NoSQL
NoSQL es un enfoque al diseño de bases de datos que se centra en proporcionar un mecanismo para el almacenamiento y la recuperación de datos que se modelan utilizando medios distintos a las relaciones tabulares empleadas en las bases de datos relacionales.\\
Las motivaciones para este enfoque incluyen la simplicidad del diseño, un escalado "horizontal" más sencillo a clústeres de máquinas, un control más preciso sobre la disponibilidad y la limitación del desajuste de impedancia objeto-relacional. Las estructuras de datos utilizadas por las bases de datos NoSQL permiten que algunas operaciones sean más rápidas y se consideran "más flexibles" que las tablas de las bases de datos relacionales.\\
Se clasifican en cuatro categorías principales:
\begin{itemize}
    \item Key-value stores(clave-valor): utilizan el array asociativo (también llamado mapa o diccionario) como su modelo de datos fundamental. En este modelo, los datos se representan como una colección de pares clave-valor, de tal manera que cada clave posible aparece como máximo una vez en la colección. Ejemplos: Redis, Couchbase, DynamoDB.
    \item Column-family stores(de columnas): Utiliza tablas, filas y columnas, pero a diferencia de una base de datos relacional, los nombres y el formato de las columnas pueden variar de fila a fila en la misma tabla. Un almacén de columnas anchas se puede interpretar como un store clave-valor bidimensional. Ejemplo: Cassandra.
    \item Document databases (de documentos): El concepto central de un almacén de documentos es el de un "documento". Las codificaciones en uso incluyen XML, YAML y JSON, y formas binarias como BSON. Se accede a los documentos en la base de datos a través de una clave única que representa ese documento. Las colecciones podrían considerarse análogas a las tablas y los documentos análogos a los registros(filas). Pero son diferentes: cada registro en una tabla tiene la misma secuencia de campos, mientras que los documentos en una colección pueden tener campos que son completamente diferentes. Ejemplo: MongoDB.
    \item Graph databases (de grafos): Las bases de datos de grafos están diseñadas para datos cuyas relaciones están bien representadas como un grafo, que consiste en elementos conectados por un número finito de relaciones. Ejemplos de datos incluyen relaciones sociales, enlaces de transporte público, mapas de carreteras, topologías de redes, etc. Ejemplo: Neo4j.
\end{itemize}

Actualmente, existen dos tipos principales de sistemas de procesamiento de datos críticos: el Procesamiento de Transacciones en Línea (OLTP) y el Procesamiento Analítico en Línea (OLAP). Cada uno tiene un propósito diferente y, técnicamente, deberían estar separados en cualquier organización.\\
OLTP es prácticamente lo que su nombre indica: una base de datos responsable del procesamiento de transacciones operativas. Debe ser rápido, dinámico y tener capacidad de respuesta ante fallos en la base de datos con un plan de respaldo.\\
OLAP es diferente: su propósito es guardar datos históricos y mantener los procesos de Extracción, Transformación y Carga (ETL) que se utilizan para el análisis de datos. El sistema OLAP es fundamental para las decisiones comerciales críticas, ya que ofrece datos relacionados con el rendimiento diario y la estabilidad organizacional a largo plazo.\\

\subsubsection{Data Warehouse} % imagenes de los ejemplos de warehouse
Los Data Warehouse son almacenes de datos son repositorios centrales de datos integrados, provenientes de fuentes diversas. Almacenan datos actuales e históricos organizados para optimizar el análisis de datos, la generación de informes y, sobre todo, la obtención de información clave a partir de la integración de datos.
Son bases de datos especializadas diseñadas para almacenar y analizar grandes volúmenes de datos estructurados y semiestructurados para fines de inteligencia empresarial (BI) y analítica.
\begin{itemize}
    \item PostgreSQL: Una característica excelente de PostgreSQL es su capacidad para ser utilizado tanto para OLTP como para OLAP. Esto facilita que las bases de datos que utilizan OLAP para almacenar los datos se comuniquen con las bases de datos que utilizan OLTP para crear los datos más recientes. Esta puede ser la razón principal por la que PostgreSQL es tan popular.
    \item Snowflake: Fundada en 2012, Snowflake es un Data Warehouse basado en la nube, conocido por su escalabilidad y flexibilidad. La arquitectura nativa de la nube de Snowflake aprovecha los beneficios de proveedores como AWS, Azure y Google Cloud. La plataforma ofrece funciones de seguridad integradas, incluyendo cifrado y control de acceso, lo que la hace adecuada para organizaciones con necesidades estrictas de seguridad y cumplimiento normativo. En general, Snowflake proporciona una solución robusta para almacenar, gestionar y analizar grandes conjuntos de datos en la nube.
    \item Amazon Redshift: Amazon Redshift es un servicio de almacenamiento de datos rápido y totalmente administrado en la nube, que permite a las empresas ejecutar consultas analíticas complejas sobre grandes volúmenes de datos, minimizando así los retrasos y garantizando un sólido apoyo para la toma de decisiones en todas las organizaciones. Fue lanzado en 2013, creado para solucionar los problemas asociados con el almacenamiento de datos tradicional en las instalaciones, como la escalabilidad, el costo y la complejidad. Redshift se integra perfectamente con Amazon S3, Amazon RDS, AWS Glue y mucho más para crear un ecosistema de datos.
    \item Google BigQuery: Google BigQuery es un Data Warehouse nativo de la nube. Es un almacén de datos en la nube sin servidor, altamente escalable y rentable que permite realizar consultas súper rápidas a escala de petabytes utilizando la potencia de procesamiento de la infraestructura de Google. Su arquitectura sin servidor le permite operar a escala y velocidad para proporcionar análisis SQL increíblemente rápidos sobre grandes conjuntos de datos.
\end{itemize}

\subsubsection{Data Lake}
Repositorios centralizados que almacenan grandes cantidades de datos sin procesar en su formato nativo, lo que permite a las organizaciones realizar análisis avanzados, aprendizaje automático y exploración de datos. 
Tecnologías como Apache Hadoop, Apache Spark y AWS Glue se utilizan comúnmente para construir y administrar lagos de datos.

\subsection{Tipos de almacenamiento de datos:} % imagen con los 3 tipos
\subsubsection{Almacenamiento en Archivos(File Storage)}
Dada la siguiente información como punto de partida, y manteniendo el estilo de escritura, profundiza en la explicación de los tipos de almacenamiento.
\subsubsection{Almacenamiento en Bloques(Block Storage)}
Divide los datos en bloques de tamaño fijo y los almacena sin metadatos adicionales. Cada bloque tiene una dirección única y puede ser gestionado independientemente, lo que lo hace ideal para bases de datos y aplicaciones de alto rendimiento que requieren baja latencia.

Características clave:

Bajo nivel: Los bloques son manejados directamente por el sistema de almacenamiento.

Alto rendimiento: Ideal para bases de datos (ej. Oracle, SQL Server) y máquinas virtuales.

Flexibilidad: Permite configuraciones avanzadas como RAID y snapshots.

\subsubsection{Almacenamiento de Objetos(Object Storage)}
El almacenamiento de objetos es una arquitectura de almacenamiento de datos en la que los datos se almacenan y gestionan como unidades autocontenidas llamadas objetos. Cada objeto contiene una clave, datos y metadatos opcionales. Dado que el almacenamiento de archivos y bloques utiliza jerarquías, el acceso a los datos se ralentiza a medida que los almacenes de datos crecen desde gigabytes y terabytes hasta petabytes e incluso más.\\
Cada objeto se almacena con sus metadatos, que pueden ser bastante detallados. Pueden incluir información como políticas específicas de privacidad y seguridad, reglas de acceso e incluso especificaciones, por ejemplo, sobre dónde se grabó un videoclip o quién creó los datos.\\    
Cada unidad tiene un identificador único o clave, que permite encontrarlas sin importar dónde estén almacenadas en un sistema distribuido. Los objetos se almacenan en un único pool sin una jerarquía de carpetas o directorios.\\

Ejemplos:
\begin{itemize}
    \item Amazon S3
    \item Azure Blob
    \item Google Cloud Storage
\end{itemize}

Los sistemas de archivos distribuidos como Hadoop Distributed File System (HDFS) y Google File System (GFS) proporcionan soluciones de almacenamiento escalables y tolerantes a fallos para entornos de computación distribuida. Están optimizados para almacenar y procesar grandes conjuntos de datos a través de múltiples nodos en un clúster de computación distribuida, soportando el procesamiento de datos en paralelo y la tolerancia a fallos.

\section{Ingesta}
La ingesta de datos es el proceso de mover datos (especialmente datos no estructurados) desde una o más fuentes a un sistema de almacenamiento para su posterior procesamiento y análisis.\\
Es la primera etapa en un pipeline de ingeniería de datos donde los datos provenientes de varias fuentes comienzan su recorrido.

\subsection{APIs}
Una de las formas más frecuentes de ingestión de datos es a través de Application Programming Interface. 
En general, se accede a las APIs a través de la web utilizando una URL. Dentro de la dirección web, se especifica la información que se desea. Para saber cómo formatear la dirección web, es necesario leer la documentación de la API. Algunas APIs también requieren que envíes credenciales de inicio de sesión como parte de tu solicitud.
La biblioteca 'requests' de Python hace que trabajar con APIs sea relativamente sencillo.

A menudo se considera que las ingestas de datos mediante APIs son más rápidas y escalables en ciertas situaciones, y pueden soportar cambios variables en los atributos. Esta funcionalidad puede funcionar con procesamiento en tiempo real o por lotes, lo que la convierte en una herramienta valiosa para muchas industrias diferentes.

Postman 

Herramientas de ingesta de datos
\begin{itemize}
    \item Airbyte: Es una herramienta de ingesta de datos de código abierto que se centra en la extracción y carga de datos, desarrollada para le proceso de ETL. Facilita la configuración de pipelines y mantiene un flujo seguro a lo largo de todo el pipeline. Puede proporcionar acceso tanto a datos brutos como normalizados e integra más de 120 conectores de datos.
    \item Amazon Kinesis: Ayuda en la ingesta de datos en tiempo real de diversas fuentes, y a procesar y analizar los datos a medida que llegan. Además, ofrece diferentes capacidades, como Kinesis Data Streams, Kinesis Data Firehose y más, para la ingesta de datos en streaming a cualquier escala de forma rentable.
    \item Apache Kafka: Es una plataforma de streaming de eventos distribuida de código abierto. El streaming de eventos captura datos en tiempo real de fuentes de eventos como dispositivos móviles, sensores, bases de datos, servicios en la nube y aplicaciones de software. Kafka almacena los datos de forma duradera para su posterior recuperación para procesamiento y análisis. Sus otras capacidades principales incluyen alto rendimiento, escalabilidad y alta disponibilidad.
    \item Apache NiFi:  Herramienta visual para automatizar los flujos de datos entre sistemas.
\end{itemize}

\chapter{Conferencia 4}
\normalfont\LARGE \textbf{Tecnologías de la Ingeniería de Datos: Transformación y Servicio}
\normalfont\small\\

\section{Transformación}
% Descripción de transformación de datos
La transformación de datos es el proceso de convertir, limpiar y estructurar los datos 
brutos en un formato adecuado para análisis y modelado. Esta etapa es crucial en los 
pipelines de datos ya que mejora la calidad, consistencia y utilidad de los datos. Preparándolos 
para su consumo en aplicaciones de business intelligence y machine learning.

Resaltar que ninguna simplemente se modifican los datos originales, sino que se crea un nuevo espacio para contener los datos "limpios y procesados".

\subsection{Ejemplos de operaciones de transformación de datos:}

\subsubsection{Limpieza}
La limpieza de datos es el proceso de detectar y rectificar errores e inconsistencias en 
los datos. Es parte esencial del procesamiento de los datos, y casi en el 100\% de los casos 
es necesario, a diferencia de otras operaciones que veremos a continuación. 
% Para visualizar los datos y detectar los errores

\textbf{Ejemplos:} 
\begin{itemize}
    \item Remover datos duplicados: Registros idénticos (por fallas en la generación o ingestión)
    \begin{verbatim}
    ANTES:                          DESPUÉS:
    ID  Producto  Valor             ID  Producto  Valor
    1   Laptop    $1000             1   Laptop    $1000
    1   Laptop    $1000        →    2   Mouse     $20
    2   Mouse     $20
    \end{verbatim}

    \item Detectar valores atípicos: Ejemplo en temperaturas (℃):
    \begin{verbatim}
    Datos: [22, 23, 21, 25, 19, 28, 31, -273, 24]
    Outlier: -273 (posiblemente error de sensor) 
    \end{verbatim} % → Se puede imputar con la mediana (23)
    
    \item Valores inválidos para el tipo: Ejemplo: Fechas con formato incorrecto o fuera de rango lógico.
    \begin{verbatim}
    ANTES:                             DESPUÉS:
Paciente  Fecha_Nacimiento         Paciente  Fecha_Nacimiento
A         1990-05-15               A         1990-05-15
B         2025-13-02          →    B         NaN (fecha futura/mes inválido)
C         15/07/1985               C         1985-07-15 (formato apropiado)
    \end{verbatim}
    
    \item Atributo con valores de tipo incorrecto: Símbolos mezclados en campos numéricos.
    \begin{verbatim}
    ANTES:                          DESPUÉS:
    Producto  Precio                Producto  Precio
    X         $120.50               X         120.50
    Y         95,99€         →      Y         95.99 
    Z         "1,200"               Z         1200.00
    \end{verbatim}
\end{itemize}
Algunas herramientas que ayudan a la visualización y limpieza de datos incluyen pandas(Python), 

\subsubsection{Normalización}
Proceso de escalar valores numéricos a un rango común (generalmente [0,1]) sin distorsionar las diferencias. Es 
relevante cuando los datos serán utilizados por algún algoritmo sensible a la escala; así todos los features tienen 
la misma escala y no se induce un sesgo en ese sentido (KNN, Redes Neuronales).\\

Min-Max Normalization: transforms the original data linearly.
$$ X_{\text{norm}} = \frac{X - \min(X)}{\max(X) - \min(X)} $$

\textbf{Ejemplos:} 

Unificación de lecturas de sensores con diferentes rangos:

    \begin{verbatim}
        ANTES (Raw values):               DESPUÉS (Normalizado):
        Sensor_A: [200, 400, 600]         Sensor_A: [0.0, 0.5, 1.0]  
        Sensor_B: [10, 20, 30]      →     Sensor_B: [0.0, 0.5, 1.0]
        (Rango A: 200-600)                (Ambos en [0,1])
        (Rango B: 10-30)
    \end{verbatim}

La biblioteca Scikit-learn proveé herramientas para el escalado/normalización automático.

\subsubsection{Estandarización}
Transformación similiar a la Normalización, que ajusta los datos para tener media 0 y desviación estándar 1 (Z-score).\\
Z-Score Normalization: zero-mean normalization.
$$ Z = \frac{X - \mu}{\sigma} $$

\textbf{Ejemplos:} 

Comparar puntuaciones en distintas escalas:
    \begin{verbatim}
        ANTES (Puntajes crudos):         DESPUÉS (Z-scores):
        FICO: [650, 720, 580]           FICO: [0.23, 1.12, -1.35]
        Vantage: [550, 600, 450]   →    Vantage: [0.23, 1.12, -1.35]
        (mu=650, sigma=62.5)            (Comparables directamente)
        (mu=550, sigma=44.7)
    \end{verbatim} 

\textbf{Herramientas:} Scikit-learn (StandardScaler), TensorFlow Transform.

\subsubsection{Encoding}
Conversión de datos categóricos a formato numérico para procesamiento algorítmico.\\
Los datos pueden ser binarios excluyentes, ordinales o nominales (sin orden).\\
Los datos binarios pueden pasar por un encoding que los transforma en 0 y 1.\\
Para los datos ordinales usualmente se utiliza Label Encoding o Ordinal Encoding. 
Sin embargo los datos categóricos nominales, no se puede asumir que tienen algún orden y forzar uno
inevitablemente introducirá un sesgo. En ese caso se sugiere utilizar One-Hot Encoding; que consiste 
en poner cada valor posible como una columna y asignar 0 o 1 indicando si el registro tenía esa clasificación.\\
Otra forma de codificar es Frequency Encoding. Que transforma el valor que era string en una fracción que indica 
la frecuencia de la categoría en la columna.
Hay otros encoding techniques como Binary Encoding, Hash Encoding, Mean/Target Encoding, 

Las herramientas usuales incluyen Pandas, Scikit-learn

One-Hot Encoding: Para categorías sin orden. Ejemplo colores:
    \begin{verbatim}
        ANTES:           DESPUÉS (One-Hot):
        Color            Rojo  Verde  Azul
        Rojo     →       1     0      0
        Verde            0     1      0
        Azul             0     0      1
    \end{verbatim}
Label Encoding: Para categorías ordinales. Ejemplo tallas:
    \begin{verbatim}
        ANTES:           DESPUÉS:
        Talla            Talla_Encoded
        S        →       0
        M                1
        L                2
    \end{verbatim}

\subsubsection{Discretizar}
Transformación de variables continuas en intervalos discretos.\\
Esto puede ser relevante al trabajar con tiempo, temperatura, mediciones, etc.
Algunos de los métodos comunes son:
\begin{itemize}
    \item Equal Width Binning: divide el rango en $n$ intervalos de igual tamaño.
    \item Equal Frequency Binning: divide los datos de forma que cada intervalo tenga la misma cantidad de data points.
    \item Clusters: Dividir en clusters que implican similitud con algoritmos como K-Means. Todos los datos de un cluster se tratan como una única categoría.
    \item etc
\end{itemize}

\textbf{Ejemplo:} 
Agrupar edades para segmentación demográfica:
    \begin{verbatim}
        ANTES (Edad):        DESPUÉS (Bins):
        [15,                 ["0-18",
         25,            →     "19-35",
         70,                  "60+",
         32]                  "19-35"]
    \end{verbatim}

\textbf{Herramientas:} Pandas (cut/qcut), KNIME, RapidMiner.

\subsubsection{Generación de nuevos atributos}
Creación de características derivadas mediante operaciones matemáticas o lógicas. \\
\textbf{Ejemplo:} Calcular el Índice de Masa Corporal (IMC) a partir de peso y altura.
Derivar la edad de la fecha de nacimiento.\\

\subsubsection{Imputación}
Técnicas para manejar valores faltantes mediante estimación o sustitución. Eso incluye técnicas 
estadísticas como remplazar por la media, mediana o moda; algoritmos de ML de regresión, KNN, u 
otros más específicos; utilizar distribuciones probabilísticas; entre muchos otros que dependen 
del tipo de datos y los casos de uso finales.\\

\textbf{Ejemplo:} Rellenar valores nulos en ingresos con la mediana del grupo demográfico.\\

\textbf{Herramientas:} Scikit-learn (SimpleImputer), DataWrangler, IBM SPSS.

\subsubsection{Procesamiento de texto}
Transformación de texto no estructurado a formato analizable. Puede incluir procesos como Tokenización, lematización y vectorización ya sea con TF-IDF o con embeddings.\\

\textbf{Ejemplo:} Tokenización, lematización y vectorización (TF-IDF) de reseñas de clientes.\\

\textbf{Herramientas:} NLTK, SpaCy, Transformers, TensorFlow Text.
% tokenizar, stemming, lemma

% ETL, ELT, and data pipelines

\subsection{ETL y ELT en el Procesamiento de Datos}
\textbf{ETL (Extract-Transform-Load):} Nace en los 70s con sistemas mainframe y consolida 
su modelo en los 90s con data warehouses. Diseñado para hardware limitado: transformación 
\textit{antes} de cargar para reducir la carga en los sistemas destino. \\
\textbf{ELT (Extract-Load-Transform):} Emerge en 2010s con Big Data y tecnologías 
cloud (Snowflake, BigQuery, Redshift). Habilitado por escalamiento elástico y procesamiento 
distribuido (Spark, columnar storage). Respuesta a necesidades de data lakes y análisis 
exploratorio con datos crudos.

\subsubsection{ETL: Transformación en Tránsito}
Transformación en \textbf{staging area} externa (servidores ETL). Caso de uso: Datos 
estructurados con reglas de negocio complejas (ej. consolidación financiera)

\textbf{Usar ETL cuando:} Requieres validación estricta pre-carga (ej. datos regulados: HIPAA, PCI-DSS). 
Los sistemas destino tienen capacidad computacional limitada. Existen transformaciones complejas independientes 
del almacenamiento.

\subsubsection{ELT: Transformación en Destino}
Transformación dentro del \textbf{sistema destino} (data warehouse/lake). Caso de uso: Análisis 
ad-hoc sobre datos semi-estructurados (ej. logs de apps móviles).

\textbf{Usar ELT cuando:} 
Necesitas retener datos crudos (ej. auditorías forenses). Trabajas con datos no 
estructurados/semi-estructurados. Requieres escalamiento horizontal (Big Data > 1TB).

% Views
\subsection{Views}
Las vistas son representaciones virtuales de datos que se  definen mediante consultas 
SQL almacenadas. No almacenan datos físicamente. Proporcionan una capa de abstracción 
sobre tablas base. Y no necesitan actualizarse ante nuevos registros.

Usar vistas para: Capas de presentación (BI tools), Transformaciones ligeras (filtros, 
joins simples), Control de acceso granular. 

This stored query can target both tables and other views (you can create a view that queries other views). This stored query (view definition) represents part of the database, but it doesn't store any physical data! This is the first important difference to a “regular” table – views don't store the data, which means that every time you need the data from the view, the underlying stored query will be executed against the database. Since views are being run each time you “call” them, they will always pick the relevant data from the underlying tables. That means you don't need to worry if something changed in the underlying table (deleted/updated rows), as you will always get the actual data from the tables.

%  dbt (data build tool)
\subsection{dbt (data build tool)}
Es un framework open-source que permite ejecutar transformaciones de datos \textbf{dentro 
del data warehouse} mediante SQL, gestionar pipelines como código versionado (Git), automatizar 
testing, documentación y despliegues.\\
Promueve el principio de "Analytics engineering as software development".\\
dbt es la T en ELT. No extrae o carga datos, sino que transforma los datos en el Data Warehouse 
para ser consumidos por herramientas de BI.\\
dbt transforma mediante views/materialized views\\

Tiene integraciones nativas con:
\begin{itemize}
    \item Warehouses: Snowflake, BigQuery, Redshift, Databricks
    \item Ingestion: Airbyte, Fivetran, Meltano
    \item BI: Metabase, Mode, Preset
\end{itemize}

The utility of dbt that makes it superior to other approaches originates from the combination of Jinja templates with SQL, and re-usable components or models.

Se puede utilizar dbt combinado con spark
% Spark 
\subsection{Spark(https://spark.apache.org)}
% https://medium.com/@atnofordatascience/data-engineering-with-apache-spark-a-beginners-guide-d3190be2b6a4
% https://medium.com/@DataEngineeer/introduction-to-apache-spark-for-data-engineering-d2060166165a
Se ha convertido en uno de los frameworks de procesamiento de grandes volúmenes de 
datos más destacados de la industria. Spark está diseñado para trabajar con una amplia 
gama de fuentes de datos, como HDFS, Apache Cassandra, Apache HBase y Amazon S3.

Unifica el procesamiento de tus datos en lotes y en tiempo real (streaming), utilizando 
tu lenguaje preferido: Python, SQL, Scala, Java o R.

Realiza Análisis Exploratorio de Datos (EDA) en datos a escala de petabytes sin 
tener que recurrir al submuestreo(downsampling) gracias a su escalamiento 
horizontal. Se ejecuta más rápido que la mayoría de los almacenes de datos.
Tiene MLlib, biblioteca integrada de machine learning para tareas como 
clasificación, regresión y clustering.

- Pyspark

\section{Serving}
% Descripción de serving de datos
Fase final del pipeline donde los datos transformados se ponen a disposición de los usuarios 
finales, ya sea BI, dashboards, APIs, microservicios, ML o sistemas externos. Idealmente el 
servicio es con acceso seguro, baja latencia y formatos consumibles.

\subsection{Desplegar un API}
Esto permite entregar datos a clientes en varias modalidades: web, móvil, IoT, otro servicio de 
analítica. Permite consultar datos en formato streaming en tiempo real. Además de permitir un control 
preciso y granular sobre qué datos exponer, a cada tipo de usuario.

\textbf{Retos comunes:}\\
Si el volumen de datos(+100) es grande es necesario implementar paginación, straming, etc. \\
Establecer protocolos de seguridad con JWT, OAuth2, Rate limiting.

\subsubsection{Frameworks Principales}
\begin{itemize}
    \item \textbf{FastAPI}: Async, auto-documentación OpenAPI
        Ejemplo endpoint:
        \begin{verbatim}
@app.get("/sales/{region}")
async def get_sales(region: str, 
                   start: date, 
                   end: date):
    return db.query(Sales)
        .filter(Sales.region == region)
        .filter(Sales.date.between(start, end))
        \end{verbatim}
    
    \item \textbf{Flask-RESTful}: Ligero, ideal para prototipos, Extensible con Flask-SQLAlchemy
    
    \item \textbf{Django REST Framework}: ORM integrado, autenticación robusta, Ideal para CRUD complejos

\end{itemize}

\subsubsection{Librerías Complementarias}
\begin{itemize}
    \item \textbf{Pydantic}: Validación y serialización de modelos.
    \item \textbf{Celery}: Procesamiento asíncrono de requests pesados.
    \item \textbf{Authlib}: Implementación OAuth2/JWT.
    \item \textbf{Redis}: Caché de respuestas frecuentes.
\end{itemize}

\subsubsection{Patrones de Diseño}
\begin{itemize}
    \item \textbf{Modelo de Paginación}:
    \begin{verbatim}
{
  "data": [...],
  "pagination": {
    "total": 1000,
    "page": 2,
    "per_page": 50,
    "next": "/api/v1/sales?page=3"
  }
}
    \end{verbatim}
    
    \item \textbf{Versionado Semántico}:
    \begin{itemize}
        \item \texttt{/api/v1/sales}
        \item \texttt{/api/v2/sales}
    \end{itemize}
    
    \item \textbf{Circuit Breaker}: Usando Tenacity
    \begin{verbatim}
@retry(stop=stop_after_attempt(3), 
      wait=wait_exponential(multiplier=1))
def fetch_external_data():
    ...
    \end{verbatim}
\end{itemize}

\textbf{Buenas prácticas:} Documentación Automática, Monitoreo, Pruebas automáticas, Logging.

\subsection{Herramientas de Análisis y Visualización}

\subsubsection{Tableau}
Plataforma líder de visualización empresarial con capacidades drag-and-drop y soporte para big data.
Tableau es una herramienta de Inteligencia de Negocios (BI) para analizar datos visualmente. Los usuarios 
pueden crear y distribuir dashboards interactivos y compartibles que representan tendencias, 
variaciones y densidad en los datos en forma de gráficos y diagramas. Tableau puede conectarse a 
archivos, fuentes relacionales y de Big Data para adquirir y procesar datos.

Soporta, con Tableau Embedding API, añadir (embed) visualizaciones a una página web.

% https://www.ask.com/news/comprehensive-guide-using-tableau-free-version-data-visualization#:~:text=Known%20for%20its%20intuitive%20interface%20and%20powerful%20features%2C,explore%20and%20visualize%20their%20data%20without%20any%20cost.

\subsubsection{Power BI} (free tier on desktop)
Microsoft Power BI es una plataforma de visualización de datos e informes. Un dashboard de Power BI permite reportar y visualizar 
datos en una amplia gama de estilos, incluyendo gráficos, mapas, diagramas, diagramas de dispersión y más.
Allows to also create highly customizable visualizations with R and Python. 
Gran soporte para mapas.

Power BI en sí está compuesto por varias aplicaciones interrelacionadas: Power BI Desktop, Power BI Pro, Power BI Premium, Power BI Mobile,\\
Power BI Embedded: Power BI White-label para proporcionar paneles y analíticas orientadas al cliente en tus propias aplicaciones.\\
Power BI Report Server: Solución local incluida en Premium; puede ser migrada a la nube.\\

\subsubsection{Looker}
Looker es una plataforma de computación en la nube que proporciona inteligencia empresarial (BI) y análisis de datos.
Integración de datos: Looker tiene la capacidad de establecer conexiones con varias fuentes de datos basadas en la nube, como los data warehouse Snowflake, Amazon Redshift y Google BigQuery. Debido a esto, los usuarios pueden examinar los datos almacenados en estas plataformas sin tener que copiarlos o reubicarlos.
Visualización: Looker ofrece una variedad de opciones de visualización para transmitir eficazmente información valiosa de los datos. Para comunicar mediciones, tendencias y patrones, los usuarios pueden crear paneles, gráficos y diagramas.

\subsubsection{Metabase}
Metabase es una plataforma de inteligencia empresarial de código abierto. Puede usar Metabase para hacer preguntas sobre sus datos, o incrustar Metabase en su aplicación para permitir que sus clientes exploren sus propios datos.

\chapter{Conferencia 5}
\normalfont\LARGE \textbf{Tecnologías de la Ingeniería de Datos: Tecnologías complementarias}
\normalfont\small\\

\section{Gestión de Datos}

Las herramientas de data governance son soluciones de software diseñadas para ayudar a las empresas a gestionar sus datos de manera más efectiva. Estas herramientas respaldan procesos que aseguran que los datos sean precisos, seguros y estén en línea con los requisitos regulatorios. Pueden incluir capacidades tales como:

\begin{itemize}
    \item \textbf{Catalogación de datos:} el proceso de crear un inventario exhaustivo de sus activos de datos. Una herramienta de gobierno de datos con sólidas capacidades de catalogación facilita la búsqueda, comprensión y uso de datos en toda su organización.
    \item \textbf{Seguimiento del linaje de datos:} comprender de dónde provienen sus datos, cómo se transforman y a dónde van es crucial. Las funciones de linaje de datos le permiten rastrear el flujo de datos a través de los sistemas, lo que garantiza la transparencia y la confianza.
    \item \textbf{Gestión del cumplimiento normativo:} muchas industrias están sujetas a regulaciones estrictas como GDPR, HIPAA o CCPA. Las herramientas de gobierno de datos que ofrecen gestión del cumplimiento normativo pueden ayudar a su organización a mantener y demostrar el cumplimiento de estas regulaciones.
    \item \textbf{Control de calidad de los datos:} las herramientas de gobierno de datos deben tener controles de calidad de datos integrados que garanticen la precisión, integridad y consistencia de sus datos. Estas herramientas deben proporcionar alertas automatizadas cuando surjan problemas de calidad de los datos.
    \item \textbf{Controles de acceso basados en roles:} la seguridad de los datos es un componente clave del gobierno. Asegúrese de que su herramienta admita controles de acceso basados en roles para que solo el personal autorizado tenga acceso a datos confidenciales.
\end{itemize}

% Data cataloging: the process of creating a comprehensive inventory of your data assets. A data governance tool with strong cataloging capabilities makes it easier to find, understand, and use data across your organization.

% Data lineage tracking: Understanding where your data comes from, how it's transformed, and where it goes is crucial. Data lineage features allow you to trace the flow of data across systems, ensuring transparency and trust.

% Compliance management: Many industries are subject to strict regulations such as GDPR, HIPAA, or CCPA. Data governance tools that offer compliance management can help your organization maintain and demonstrate adherence to these regulations.

% Data quality control: Data governance tools should have built-in data quality controls that ensure the accuracy, completeness, and consistency of your data. These tools should provide automated alerts when data quality issues arise.

% Role-based access controls:  Data security is a key component of governance. Ensure that your tool supports role-based access controls so that only authorized personnel have access to sensitive data.
% Policy enforcement

% https://rootstack.com/es/blog/dataops-implementacion-herramientas

\section{Orquestación de Datos}
En el ámbito de la ingeniería de datos, el proceso de transformar datos brutos en información valiosa implica la utilización de pipelines y flujos de trabajo de datos. Estos conceptos forman la columna vertebral de la gestión eficiente de datos y permiten a los ingenieros de datos manejar tareas complejas de procesamiento de datos sin problemas.\\
Los pipelines de datos sirven como el tejido conectivo dentro del ecosistema de la ingeniería de datos. Son responsables de orquestar el flujo de datos desde su origen hasta su destino, manejando varias etapas del procesamiento de datos a lo largo del camino. Los ingenieros de datos diseñan e implementan pipelines para asegurar el movimiento fluido de los datos, incluyendo tareas como la ingestión, integración, transformación y carga de datos.\\

\textbf{Apache Airflow:} Es una herramienta de código abierto ampliamente utilizada para la creación, programación y monitoreo de flujos de trabajo complejos. Airflow permite a los equipos de DataOps diseñar pipelines de datos de manera flexible y modular, integrando distintas fuentes y destinos de datos. Es ideal para la automatización de tareas y facilita la colaboración entre equipos.

Airflow proporciona un framework de Python flexible y escalable que permite a los ingenieros de datos crear, programar y monitorizar sus pipelines de datos.\\
Un DAG representa un conjunto de tareas y sus dependencias, donde la dirección de las flechas indica el flujo de datos o ejecución.\\
La parte "Dirigido" significa que cada tarea tiene una relación definida con una o más tareas, formando una cadena de dependencias. Esto asegura que las tareas se ejecuten en el orden correcto, basándose en sus dependencias.\\
La parte "Acíclico" significa que no hay bucles o ciclos en el grafo. En otras palabras, las tareas no pueden tener dependencias circulares que conducirían a bucles infinitos. Esto asegura que los flujos de trabajo puedan completarse exitosamente sin bucles infinitos o dependencias no resueltas.\\

\textbf{Prefect:} Similar a Airflow, permite la orquestación y automatización de flujos de trabajo de datos; así como el monitoreo de pipelines con una interfaz intuitiva para equipos de datos. 

Prefect is an open-source orchestration tool for data engineering. It is a Python-based tool that allows you to define, schedule, and monitor your data pipelines. Prefect is a great tool for data engineers and data scientists who want to automate their data pipelines.\\

\section{DataOps}

La creciente complejidad de los datos en las organizaciones modernas ha dado lugar a la necesidad de prácticas más ágiles y eficientes. DataOps surge como una respuesta para mejorar la gestión de datos, optimizar flujos de trabajo y fomentar una mayor colaboración entre equipos de TI y analistas de datos. 
DataOps es una metodología que combina los principios de DevOps con la gestión de datos. Su objetivo principal es optimizar el ciclo de vida de los datos, desde la integración y almacenamiento hasta su análisis y entrega, asegurando la calidad y confiabilidad. La adopción de herramientas adecuadas es crucial para implementar DataOps con éxito y maximizar los beneficios de esta metodología.

\textbf{Talend:} Plataforma para integración, transformación y migración de datos en tiempo real, con enfoque en automatización y calidad de datos para mantener altos estándares y cumplimiento normativo.

\subsection{Containerization}

La contenerización es una práctica de desarrollo de software que empaqueta una aplicación y sus dependencias en un único contenedor. El contenedor está aislado del sistema host, permitiendo que la aplicación se ejecute en cualquier infraestructura que soporte contenedores, independientemente del sistema operativo subyacente.\\
Inspirados por la idea de las máquinas virtuales, los contenedores proporcionan entornos ligeros y aislados que empaquetan las aplicaciones con sus dependencias. Esta innovación asegura consistencia, portabilidad y eficiencia. Con el tiempo, herramientas como Docker y Kubernetes se han convertido en estándares de la industria para la gestión de aplicaciones contenerizadas, permitiendo una escalabilidad y orquestación sin problemas.\\
Los contenedores pueden moverse fácilmente de un host a otro, lo que facilita la migración de una solución de ingeniería de datos a una infraestructura diferente si es necesario.\\
Los contenedores proporcionan un entorno consistente para que las aplicaciones se ejecuten, asegurando que la solución de ingeniería de datos se comporte de la misma manera independientemente del entorno en el que se despliegue.\\

\textbf{Docker:} Plataforma de código abierto que permite la automatización del despliegue, escalado y gestión de aplicaciones en contenedores. Kubernetes facilita la creación de entornos escalables y resilientes para gestionar grandes volúmenes de datos y flujos de trabajo. Con Kubernetes, los equipos pueden orquestar los recursos necesarios de manera eficiente, garantizando que las aplicaciones de datos operen de manera fluida.

\textbf{Kubernetes:} Plataforma para automatizar despliegue, escalado y gestión de aplicaciones en contenedores, usada para crear entornos escalables y resilientes en proyectos de ingeniería de datos y DataOps

\chapter{Conferencia 6}
\normalfont\LARGE \textbf{Integración de Sistemas en la Ingeniería de Datos}
\normalfont\small\\

\section{System Integration}

% v3: https://www.altexsoft.com/blog/system-integration/

Definición:\\
La Integración de Sistemas es el proceso de ingeniería que consiste en integrar hardware, 
software, redes y fuentes de datos dispares en un sistema unificado y cohesivo que opera 
como una sola entidad funcional. Esto implica establecer canales de comunicación y protocolos 
de intercambio de datos entre sistemas previamente aislados. El objetivo es lograr que estos
sistemas trabajen juntos sin problemas para que puedan compartir información y procesos de 
manera más eficiente. La integración de sistemas se centra típicamente en mejorar la 
funcionalidad, el flujo de datos y la eficiencia operativa.

Con los años, la integración de sistemas ha evolucionado. Si bien antes se trataba principalmente 
de conectar servidores on-premise (locales), la integración actual a menudo se realiza en la nube. 
Ya sea a través de APIs o buses de servicio empresarial, la integración moderna ayuda a que diferentes 
sistemas y herramientas trabajen juntos de manera eficiente en un entorno basado en la nube.

\subsection{Tipos de Integración de Sistemas y Ejemplos}
\subsubsection{Integración de Datos (Data Integration)}
La integración de datos es un tipo de integración de sistemas enfocada en consolidar información de diversas fuentes, como plataformas, servicios y bases de datos, en una visión unificada.

\textbf{Ejemplo:} Starbucks utiliza la integración de datos para combinar información de su aplicación móvil, sistemas de punto de venta (POS) y comentarios de clientes. Esta integración permite promociones personalizadas, seguimiento de inventario en tiempo real y una mejor experiencia del cliente mediante análisis integrales.

\subsubsection{Integración de Aplicaciones (Application Integration)}
La integración de aplicaciones se refiere al proceso de conectar diferentes aplicaciones empresariales para que funcionen de manera eficiente. Esto es crucial para optimizar flujos de trabajo y procesos de negocio, permitiendo que los datos se muevan entre aplicaciones automáticamente sin intervención manual.

\textbf{Ejemplo:} Integrar herramientas como Slack y Jira para que las actualizaciones de proyectos se sincronicen en tiempo real entre plataformas de comunicación y gestión de proyectos.

\subsubsection{Integración de Sistemas Legacy (Legacy System Integration)}
Este tipo de integración conecta sistemas antiguos (legacy) con tecnologías modernas, permitiendo seguir utilizando datos existentes junto con herramientas nuevas.

\textbf{Ejemplo:} Un hospital que conecta un antiguo sistema de gestión de pacientes con un nuevo sistema de Registro Electrónico de Salud (EHR, por sus siglas en inglés). Esto permite a los médicos acceder a toda la información del paciente desde ambos sistemas sin cambiar su rutina.

\subsubsection{Integración de Aplicaciones Empresariales (Enterprise Application Integration, EAI)}
La EAI unifica diferentes aplicaciones de software dentro de una empresa para que funcionen de manera cohesionada. Conecta diversas aplicaciones, permitiéndoles compartir e intercambiar datos en tiempo real. Resuelve el problema de aplicaciones empresariales dispares que almacenan datos por separado, integrando todo en un sistema unificado.

\textbf{Ejemplo:} Una empresa utiliza sistemas separados para ventas, servicio al cliente e inventario. La EAI enlaza estos sistemas, permitiéndoles compartir información y comunicarse en tiempo real. Esta integración hace que las operaciones sean más fluidas y eficientes al eliminar la transferencia manual de datos.

\subsubsection{Integración Empresa a Empresa (Business-to-Business, B2B Integration)}
La integración B2B conecta los sistemas de diferentes empresas para agilizar sus procesos colaborativos. Automatiza el intercambio de transacciones y documentos, como órdenes y facturas, entre negocios.

Este flujo eficiente de información conduce a una cooperación más eficaz, transacciones más rápidas y menos errores, simplificando las interacciones con proveedores, clientes y socios.

\subsubsection{Intercambio de Datos Electrónicos (Electronic Data Interchange, EDI)}
La integración de documentos electrónicos (EDI) es un tipo específico de integración centrado en el intercambio de documentos comerciales entre sistemas, frecuentemente entre empresas (B2B). El EDI automatiza y protege la transferencia de documentos, reduciendo errores manuales y mejorando la eficiencia. En industrias como la salud y las finanzas, donde la confidencialidad de los datos es crítica, el EDI garantiza un manejo seguro de documentos.

\textbf{Ejemplo:} Imagina una empresa que utiliza sistemas separados para ventas, servicio al cliente e inventario. Cada sistema tiene sus propios datos y procesos, lo que dificulta tener una visión completa de lo que ocurre en el negocio. La EAI integra estos sistemas, permitiéndoles compartir información y comunicarse en tiempo real.

\subsection{Integración de Sistemas. Conectando sistemas}
\subsubsection{APIs (Interfaces de Programación de Aplicaciones)}
Las APIs son un mecanismo fundamental para permitir la interoperabilidad entre sistemas de software. Una API define un conjunto de rutinas, protocolos y herramientas para construir aplicaciones. Actúa esencialmente como intermediario, proporcionando una interfaz estandarizada mediante la cual las aplicaciones pueden solicitar servicios e intercambiar datos. Esta capa de abstracción protege a los desarrolladores de la complejidad subyacente de los sistemas que interactúan.

Las APIs especifican formatos de datos (ej. JSON, XML) y protocolos de comunicación (ej. REST, SOAP, gRPC) para la interacción. Las APIs RESTful, en particular, utilizan métodos HTTP (GET, POST, PUT, DELETE) para realizar operaciones sobre recursos identificados por URLs. La seguridad es una consideración crítica, abordada frecuentemente mediante mecanismos de autenticación (ej. claves API, OAuth) y autorización.

Las APIs soportan un enfoque de diseño modular, permitiendo que los sistemas evolucionen independientemente mientras mantienen la interoperabilidad.

\subsubsection{Middleware}
El middleware abarca una amplia categoría de software que facilita la comunicación y gestión de datos entre aplicaciones distribuidas. Proporciona una capa de abstracción que oculta la heterogeneidad de los sistemas subyacentes, simplificando los esfuerzos de integración.

Las soluciones de middleware ofrecen diversos servicios, incluyendo colas de mensajes (ej. RabbitMQ, Apache Kafka), gestión de transacciones, transformación de datos y seguridad. Las colas de mensajes permiten comunicación asíncrona, desacoplando aplicaciones y mejorando la escalabilidad. Los motores de transformación de datos convierten información entre diferentes formatos y esquemas, asegurando compatibilidad entre sistemas.

El middleware desacopla aplicaciones, permitiéndoles operar independientemente y mejorando la resiliencia del sistema. Las colas de mensajes y otros servicios de middleware mejoran la escalabilidad del sistema distribuyendo cargas de trabajo y gestionando comunicación asíncrona.

\subsubsection{Webhooks}
Los webhooks son un mecanismo para permitir comunicación en tiempo real entre aplicaciones. En lugar de consultar constantemente (polling) un servidor por actualizaciones, una aplicación puede registrar un webhook para recibir notificaciones cuando ocurren eventos específicos en otro sistema.

Un webhook es básicamente un callback HTTP definido por el usuario. Cuando ocurre un evento predefinido en un sistema fuente, este envía una petición HTTP POST a una URL especificada (el endpoint del webhook) conteniendo datos relacionados con el evento. La aplicación receptora puede entonces procesar los datos y tomar las acciones correspondientes.

Los webhooks proporcionan actualizaciones en tiempo real, permitiendo respuestas inmediatas a eventos. Eliminan la necesidad de polling constante, reduciendo latencia y mejorando la capacidad de respuesta del sistema. Son más eficientes en recursos que el polling, ya que solo transmiten datos cuando ocurren eventos.

\subsubsection{EDI (Intercambio Electrónico de Datos)}
El EDI es un método estandarizado para intercambiar documentos comerciales electrónicamente entre organizaciones. Reemplaza procesos tradicionales basados en papel con transmisión automatizada de datos, optimizando operaciones comerciales.

Los estándares EDI (ej. ANSI X12, UN/EDIFACT) definen el formato y estructura de documentos comerciales como órdenes de compra, facturas y avisos de envío. Las transacciones EDI típicamente se transmiten sobre redes seguras usando protocolos como AS2 (Applicability Statement 2). Se utiliza software de traducción para convertir datos entre formatos EDI y formatos internos de aplicaciones.

El EDI elimina la entrada manual de datos, reduciendo errores y mejorando la precisión de la información.

\subsubsection{Snaps de SnapLogic}
Los Snaps de SnapLogic son conectores preconstruidos diseñados para simplificar la integración con aplicaciones comúnmente utilizadas. Estos conectores ofrecen una interfaz amigable, permitiendo a los usuarios establecer conexiones sin necesidad de programación personalizada extensa.

Los Snaps abstraen las complejidades de las APIs subyacentes, proporcionando un enfoque visual y basado en configuración para la integración. Típicamente soportan operaciones comunes para interactuar con aplicaciones objetivo, como recuperación de datos, creación de datos y modificación de datos.

\subsection{Enfoques de Integración de Sistemas}
Las metodologías de integración de sistemas definen cómo los datos y procesos se interconectan entre subsistemas, donde cada enfoque ofrece ventajas y limitaciones distintas según escalabilidad, complejidad y requisitos funcionales. A continuación analizamos los modelos de integración predominantes:

\subsubsection{Modelo de Integración Vertical (Basado en Silos)}
Arquitectura jerárquica donde los subsistemas se apilan en capas funcionales, con componentes básicos en la base y procesos de alto nivel en la cima. Esto crea silos aislados optimizados para flujos de trabajo específicos.

\textbf{Ventajas:}
\begin{itemize}
\item \textit{Simplicidad:} Configuración sencilla y directa
\item \textit{Aislamiento Funcional:} Ideal para procesos específicos y aislados
\item La integración vertical es más fácil de gestionar y solucionar dentro del silo
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
\item Difícil escalabilidad debido a su estructura rígida
\item Cada nueva función requiere su propio silo, generando posibles ineficiencias
\item Flexibilidad limitada para compartir datos entre funciones
\end{itemize}

\textbf{Ejemplo:} Procesamiento de pedidos en e-commerce - los subsistemas de ventas, producción y logística se encadenan linealmente sin intercambio lateral de datos.

\subsubsection{Modelo de Integración Horizontal (Hub-and-Spoke)}
Arquitectura centralizada que emplea una capa intermediaria (ej. Enterprise Service Bus, ESB) para enrutar datos entre subsistemas, desacoplando emisores y receptores.

\textbf{Ventajas:}
\begin{itemize}
\item \textit{Modularidad:} Alta flexibilidad para reemplazar o actualizar sistemas individuales
\item \textit{Gestión Unificada:} Administración centralizada que reduce la complejidad de comunicación entre sistemas
\item Soporta amplia gama de funciones y aplicaciones
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
\item Configuración y mantenimiento más complejos
\item Puede ser costoso por requerir una capa adicional de integración
\item \textit{Punto Único de Falla:} Riesgo de un único punto de falla si la capa central tiene problemas
\end{itemize}

\textbf{Ejemplo:} Ecosistemas centrados en CRM (ej. Salesforce) como hubs para subsistemas de marketing, inventario y logística.

\subsubsection{Modelo de Integración Punto a Punto}
Enfoque más simple de conectividad donde se vinculan dos sistemas directamente entre sí. Conexiones directas por pares entre subsistemas, típicamente para transferencia unidireccional de datos o activación de eventos.

\textbf{Ventajas:}
\begin{itemize}
\item Implementación simple y rápida
\item Permite comunicación directa entre sistemas con mínima sobrecarga
\item Bajo costo y esfuerzo inicial
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
\item Escalabilidad limitada, ya que cada nueva conexión añade complejidad
\item Se vuelve difícil de gestionar al crecer el número de conexiones
\item No es adecuado para integraciones a gran escala con muchos sistemas
\end{itemize}

\textbf{Ejemplo:} Formularios de contacto en sitios web que envían datos directamente a un CRM como Salesforce.

\subsubsection{Modelo de Integración en Estrella}
La integración en estrella conecta cada sistema directamente con los demás, creando una red de conexiones punto a punto. Modelo híbrido que combina conexiones punto a punto selectivas entre subsistemas críticos, formando una malla parcial.

\textbf{Ventajas:}
\begin{itemize}
\item \textit{Eficiencia Dirigida:} Permite rutas de datos prioritarias sin interconectividad completa
\item \textit{Flexibilidad:} Permite personalizar conexiones según necesidades específicas
\item Máxima flexibilidad para compartir datos entre sistemas
\item Mejora funcionalidad al permitir múltiples rutas para flujo de datos
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
\item Creciente complejidad al añadir más sistemas
\item Requiere alto esfuerzo de mantenimiento para gestionar todas las conexiones
\item Difícil escalabilidad, ya que cada nuevo sistema incrementa exponencialmente el número de conexiones necesarias
\end{itemize}

\textbf{Ejemplo:} Un cliente realiza un pedido en un sitio web. El sitio envía la orden al sistema de ventas, que a su vez envía la información al departamento de envíos y logística. Simultáneamente, el sitio registra al cliente en el sistema CRM de la empresa, que envía una notificación a un representante de ventas en Slack. Múltiples aplicaciones están conectadas y se comunican entre sí, pero no todas están interconectadas entre ellas.
% \section{System integration vs data integration}
% System integration is not to be confused with data integration. The former means connecting disparate systems to facilitate access to information, while the latter is about gathering data from different sources into one storage to gain a unified view. Please visit our separate post about data integration to learn more about it.  

\chapter{Conferencia 7}
\normalfont\LARGE \textbf{Integración de Datos en la Ingeniería de Datos}
\normalfont\small\\
Es parte de Data Mangement (o Gestión de Datos), una de las corrientes subyacentes del ciclo de vida de la ingeniería de datos.
\section{Integración de Datos}
Data integration is a systematic process that involves the combination of data from heterogeneous sources 
into a unified, coherent, and standardized format. This process is critical in modern computational and 
business environments, where data is generated and stored across diverse platforms, including relational 
databases, applications, spreadsheets, cloud-based services, and application programming interfaces (APIs). 
The primary objective of data integration is to facilitate efficient analytical processing, operational workflows, 
and data-driven decision-making.

It involves extracting, transforming, and loading data into a central repository or database to provide 
a comprehensive view of the data and enable informed decision-making.

Data integration transcends mere aggregation; it requires the reconciliation of structural, semantic, and syntactic differences among source datasets. Advanced middleware solutions or virtual data layers are often employed to orchestrate this process, ensuring seamless interoperability. The resultant unified data ecosystem supports high-precision analytics, real-time operational intelligence, and strategic business insights.

\subsection{Data integration process}
\subsubsection{1. Initiating with Data Discovery and Profiling}
This phase implicates a deep dive into the data landscape, identifying the myriad streams that feed into the organization's informational ecosystem. Here, the focus is on understanding the nuances of each data source—its structure, quality, and peculiarities—laying the groundwork for a seamless integration.
\subsubsection{2. Extracting Data with Precision}
This critical step requires pulling data from its native repositories, which demands precision and sensitivity to avoid disrupting the source systems' functionality. The challenge lies in efficiently marshaling data from its silos, ensuring it's primed for the subsequent transformation phase.
\subsubsection{3. Transforming Data for Uniformity}
The heart of the data integration process lies in data transformation. The diverse data elements are standardized, cleansed, and harmonized in this stage, ensuring they speak a common language. This transformation is pivotal, turning a cacophony of disparate data into a harmonious dataset ready for integration.
An illustrative example could be an e-commerce giant integrating customer interaction data across multiple channels. Here, data from social media interactions, website visits, and purchase histories undergo normalization and deduplication, making it analyzable as a coherent whole.
\subsubsection{4. Loading Data into a Unified Repository}
This phase involves transferring the refined data into a central repository, such as a data warehouse, where it is stored, accessed, and analyzed. The loading must be executed to maintain data integrity and optimize for future access.
\subsubsection{5. Ensuring Data Integrity through Validation}
\subsubsection{6. Maintaining Relevance with Data Synchronization}
Data validation and quality assurance. This step confirms that the integrated data is accurate, consistent, and high-quality. Rigorous validation checks are applied to detect and rectify any anomalies, ensuring the data's reliability for decision-making.
\subsubsection{7. Facilitating Data Access and Consumption}
The culmination of the data integration process ensures that the integrated data is readily accessible for analysis and business intelligence. This involves making the data available in formats and through channels that support efficient consumption, analysis, and reporting.

\subsection{Types of Data Integration}
There are several methods of data integration, each suited to different use cases. The main types are as follows:

\subsubsection{Manual Data Integration}

Manual data integration involves the extraction, transformation, and loading (ETL) of data from disparate 
sources through human intervention. This approach, while conceptually simple, necessitates a significant 
investment of human resources.  While resource-intensive, it can be an effective solution for smaller-scale 
or specialized integration projects where automated tools are not feasible or cost-effective.

The core process involves manually collecting data from various sources (e.g., online forms, customer surveys, 
social media feeds). Typically, this involves using manual techniques such as copy-pasting data into spreadsheet 
software. The collected data is then transformed and cleaned, usually using spreadsheet software or similar 
tools, to ensure consistency and accuracy. Finally, the transformed data is loaded into a central repository 
or database, potentially using simple ETL scripts or custom-built solutions.

Organizations may employ manual integration when dealing with data complexity or sensitivity necessitates 
human oversight. Common applications include customer segmentation, fraud detection, and compliance adherence. 
For instance, manually combining data exported from different databases within a spreadsheet environment 
represents a typical, albeit potentially error-prone, example.

The fundamental definition of manual data integration is the process of combining data from multiple sources 
without the use of automated tools or software. This reliance on manual intervention at each step renders the 
process time-consuming and susceptible to errors.

\textbf{Pros:}

\begin{itemize}
    \item \textbf{Simplicity and Accessibility:}  No specialized tools or software are required. Widely available spreadsheet software provides a readily accessible platform for manual data integration.
    \item \textbf{Flexibility and Customization:} Allows for a high degree of customization and flexibility, unconstrained by the limitations of automated tools, enabling tailored transformations and handling of unique data nuances.
    \item \textbf{Suitability for Small-Scale Projects:}  Effective for small-scale projects with limited data sources and low data volumes where the overhead of automation may not be justified.
\end{itemize}

\textbf{Cons:}

\begin{itemize}
    \item \textbf{Time Consumption:}  A labor-intensive process requiring significant manual effort at each stage of the ETL process.
    \item \textbf{Error Proneness:}  Human involvement introduces the risk of errors during data extraction, transformation, and loading.
    \item \textbf{Inefficiency at Scale:}  Becomes increasingly impractical and inefficient as data volume, data source complexity, and the required transformations increase.  Scalability is severely limited.
    \item \textbf{Limited Scalability:} Not suitable for large-scale data integration projects due to inherent limitations in handling large datasets and complex transformations.
\end{itemize}

In summary, manual data integration presents a viable option for small-scale projects where specialized tools are unnecessary, and data volumes are manageable.  However, the process's inherent limitations regarding scalability, efficiency, and error proneness necessitate careful consideration before employing it for complex or large-scale data integration endeavors. Deciding between automating data integration or continuing with manual methods requires careful consideration of these trade-offs.

\subsubsection{Middleware Data Integration}

Middleware solutions are essential intermediaries in complex integration scenarios, facilitating data communication and transformation between disparate systems. They act as a software layer that bridges the gap between various applications, enabling seamless data exchange and translation without manual intervention. Middleware solutions commonly provide real-time synchronization, automated processing, and guaranteed data delivery. Organizations leverage them to power Enterprise Application Integration (EAI) and system-to-system interactions.

Consider a scenario where one system stores customer data in XML format, while another expects JSON. Middleware enables easy translation between these formats, automating the transformation process instead of requiring manual coding. Tools like ETL (Extract, Transform, Load) further simplify data integration by automating data extraction, transformation, and loading processes.

Data integration using middleware involves utilizing middleware software to integrate data from diverse sources. Middleware extracts data, transforms it into a common format, and loads it into a central repository or database. It manages the data flow between systems, ensuring efficient processing and storage.

Different types of middleware cater to specific data integration needs:

\begin{itemize}
    \item \textbf{Message-oriented middleware:} Focuses on asynchronous communication using message queues.
    \item \textbf{Database middleware:} Provides a unified interface for accessing different databases.
    \item \textbf{Application integration middleware:} Connects and orchestrates applications through APIs and services.
\end{itemize}

Each type offers unique capabilities suitable for different data integration projects.

\textbf{Pros:}

\begin{itemize}
    \item \textbf{Improved Efficiency:}  Middleware centralizes data flow management, enhancing the efficiency of the integration process.
    \item \textbf{Enhanced Scalability:}  Provides a flexible and scalable platform for integrating data from numerous sources, enabling easier scaling.
    \item \textbf{Simplified Data Integration:}  Offers a common interface for accessing and manipulating data, simplifying the integration process.
    \item \textbf{Improved Data Quality:}  Offers error handling and data cleansing features, improving the integrated data quality.
\end{itemize}

\textbf{Cons:}

\begin{itemize}
    \item \textbf{Complexity:}  Implementing middleware requires integrating additional software, which can be complex.
    \item \textbf{Cost:}  Purchasing, setting up, and maintaining middleware requires specialized skills and resources, leading to potential expenses.
    \item \textbf{Dependency:}  Creates a dependency on the middleware platform, potentially limiting the flexibility and agility of the data integration process.
    \item \textbf{Integration Challenges:} Integrating middleware into existing IT infrastructures necessitates careful coordination and collaboration amongst stakeholders.
\end{itemize}

In conclusion, middleware can significantly improve the efficiency and scalability of data integration, providing a centralized platform for managing data flow. It reduces complexity by providing a common interface for data access and manipulation. The choice of using middleware depends on a careful evaluation of its advantages against the potential drawbacks, considering the specific requirements and constraints of the integration project.

\subsubsection{Data Warehousing}

Data warehousing involves the extraction, transformation, and loading (ETL) of data from heterogeneous sources into a centralized repository, known as a data warehouse. This process establishes a structured environment optimized for systematic querying, reporting, and advanced analysis.

Specifically, a data warehouse provides a historical and integrated view of organizational data, facilitating trend analysis and informed decision-making. Unlike transactional databases designed for operational efficiency, data warehouses are architected for analytical performance. This is achieved through denormalized schemas, optimized indexing strategies, and columnar storage formats, which enhance query speeds for large datasets.

Data warehouses are crucial for organizations requiring the storage and analysis of substantial data volumes, enabling use cases such as:

\begin{itemize}
    \item \textbf{Customer Analytics:} Understanding customer behavior, identifying market segments, and personalizing marketing campaigns.
    \item \textbf{Financial Reporting:} Generating comprehensive financial statements, tracking key performance indicators (KPIs), and ensuring regulatory compliance.
    \item \textbf{Predictive Modeling:} Developing statistical models to forecast future trends, assess risk, and optimize resource allocation.
\end{itemize}

Consider a scenario where sales data is fragmented across multiple operational databases. A data warehouse consolidates this data into a unified repository, providing rapid access to historical sales information for comprehensive reporting and analysis. This aggregation enables complex analytical queries that would be inefficient or infeasible within traditional database environments. Furthermore, integrating sales, marketing, and customer relationship management (CRM) data within the data warehouse fosters unified reporting capabilities, delivering a holistic view of the business landscape.

\subsubsection{Application-based Integration}

Application-based data integration is a design paradigm where data integration functionalities are directly embedded within an application, enabling seamless data connectivity and synchronization between different systems in real-time. This approach fosters inherent data integration capabilities, eliminating the need for separate data integration tools or layers.

Unlike traditional methods relying on external integration mechanisms, application-based integration grants applications intrinsic control over data connectivity and harmonization. By incorporating data integration logic directly into the application fabric, organizations achieve enhanced agility, efficiency, and data interoperability. This tighter integration allows for customized workflows and fine-grained control over data transformations.

For instance, a Customer Relationship Management (CRM) system integrating directly with email marketing tools exemplifies application-based integration. Synchronizing customer data in real-time improves data accuracy and streamlines marketing efforts.

Furthermore, application-based integration facilitates the incorporation of various data integration patterns, such as batch processing or real-time streaming, directly within the application's architecture. This unlocks the potential to derive deeper insights from data, fostering innovation and maximizing the value extracted from organizational systems. Cloud data integration, specifically, exemplifies this approach by unifying data from multiple cloud sources, facilitating enhanced decision-making and analytics.

\textbf{Pros:}

\begin{itemize}
    \item \textbf{Simplicity:} Users interact directly with the application, abstracting away the complexities of data integration tools and interfaces, simplifying system usage and understanding, particularly for those unfamiliar with data integration concepts.
    \item \textbf{Performance:} Embedding integration capabilities directly into the application can yield greater efficiency than separate data integration layers, potentially leading to faster performance, especially when processing large data volumes. Tighter coupling and reduced overhead can improve data access speeds.
    \item \textbf{Customization:} Application-based integration allows for tailoring data integration logic to meet the specific requirements of the application, enabling greater flexibility and customization based on specific data sources and integration needs.
\end{itemize}

\textbf{Cons:}

\begin{itemize}
    \item \textbf{Complexity:} Implementing data integration within an application can be intricate, particularly with diverse or technologically disparate data sources. This necessitates considerable resources and technical expertise.
    \item \textbf{Interoperability:} Application-based integration might not be as effective in facilitating data exchange and interoperability across diverse systems compared to standalone integration layers. Integration is confined to the specific application, limiting broad data sharing.
    \item \textbf{Maintenance:} Maintaining and updating data integration logic embedded within an application can be challenging as the application evolves, leading to increased maintenance costs. Architectural changes may necessitate modifications to the data integration code.
\end{itemize}

In summary, application-based data integration embodies building data integration capabilities directly into an application rather than employing separate tools. While offering potential simplicity and performance benefits, this approach poses challenges in terms of implementation complexity and maintainability.

\subsubsection{Data Federation}
Data federation, also referred to as virtual data integration, facilitates real-time data access and querying across heterogeneous data sources without requiring physical data migration or replication.  Instead of consolidating data into a centralized repository, data federation establishes a virtual data layer that presents a unified, logical view of data residing in disparate systems, databases, or applications. This approach avoids the complexities and overhead associated with ETL (Extract, Transform, Load) processes.

The virtual data layer abstracts the underlying data sources, enabling users to interact with a single, integrated representation.  For instance, consider a scenario where customer data is stored in a CRM system, sales data in an ERP system, and inventory data in a separate database. Data federation allows access to this information through a unified interface, providing a holistic view of the data landscape. Common applications of data federation include advanced customer analytics, personalized product recommendation engines, and sophisticated fraud detection systems. As an illustrative example, federating access to multiple databases allows querying data as though it resided within a single, unified database, thereby streamlining data management and analysis.

\subsection{Data Integration Approaches}
\subsubsection{Batch Integration}
Batch integration is a foundational element of the data integration process, tailored for scenarios where real-time data is not paramount. This approach, integral to the data integration process, involves aggregating data at predetermined intervals, thus optimizing for efficiency and minimizing the load on source systems. Despite its simplicity, the batch integration method within the data integration process may introduce delays in data availability, posing challenges for time-sensitive decision-making.

\subsubsection{Real-time Integration}
The data integration process is significantly enhanced by real-time integration, which ensures immediate data availability, a critical requirement for operations demanding up-to-the-minute data. This facet of the data integration process is essential for enabling dynamic decision-making and operational responsiveness. However, the complexity and resource intensity of real-time integration within the data integration process must be considered.

\subsubsection{Hybrid Integration}
Hybrid integration represents a strategic blend within the data integration process, combining the strengths of batch and real-time methods to offer a versatile solution tailored to diverse business needs. This approach enriches the data integration process by providing flexibility, allowing organizations to balance immediacy and efficiency based on specific data integration conditions.

\subsubsection{Cloud-based Integration}
Cloud computing has introduced cloud-based integration into the data integration process, offering scalability, cost-efficiency, and remote accessibility. This approach has become increasingly relevant in data integration, especially for organizations with geographically dispersed teams requiring access to integrated data. Data security and internet dependence are inherent challenges within this data integration approach.

\subsection{Data Integration Techniques}

Effective data integration is crucial for consolidating information from disparate sources. Several techniques are available, each with its own strengths and weaknesses.

\subsubsection{ETL (Extract, Transform, Load)}

ETL is a well-established data integration process consisting of three primary phases:

\begin{itemize}
    \item \textbf{Extract:} Data is retrieved from various source systems.
    \item \textbf{Transform:} The extracted data undergoes cleaning, formatting, and transformation to ensure consistency and suitability for the target system.  This may include standardization, deduplication, and enrichment.
    \item \textbf{Load:} The transformed data is loaded into the designated target system, such as a data warehouse.
\end{itemize}

ETL tools facilitate the movement and transformation of data between systems. Examples include Rivery Data Integration, Talend Data Integration, Informatica PowerCenter, and Oracle Data Integrator. A practical example involves extracting customer information from an e-commerce platform, standardizing the address formats, and loading it into a centralized data warehouse. This process ensures data accuracy and consistency for reporting and analysis.

\subsubsection{ELT (Extract, Load, Transform)}

ELT represents a more modern approach to data integration. Unlike ETL, ELT reverses the order of the transformation and loading steps:

\begin{itemize}
    \item \textbf{Extract:} Data is retrieved from various source systems.
    \item \textbf{Load:} Raw data is loaded directly into the target system, often a data lake or data warehouse that offers significant computational resources.
    \item \textbf{Transform:} Data transformation is performed within the target system using its processing capabilities. This often leverages technologies like SQL or other data processing frameworks.
\end{itemize}

An example of ELT involves loading raw log data into a data lake and then using SQL queries to transform and analyze the data. This approach leverages the processing power of the data lake for efficient data manipulation.

\subsubsection{Comparison of Integration Strategies}

The selection of a suitable integration strategy depends on specific requirements, including data volume, latency tolerance, and data complexity. Table \ref{tab:integration_strategies} summarizes several data integration strategies and their characteristics.

\begin{table}[h!]
\centering
\caption{Comparison of Data Integration Strategies}
\label{tab:integration_strategies}
\begin{tabular}{|p{4cm}|p{3cm}|p{5cm}|p{4cm}|p{4cm}|}
\hline
\textbf{Integration Strategy} & \textbf{Suitable for} & \textbf{How it Works} & \textbf{Pros} & \textbf{Cons} \\
\hline
Batch ETL & Non-real-time integration & Extract, Transform, Load in batches & Cost-effective, suitable for large data volumes & Non-real-time, potential data latency \\
\hline
Real-time ETL & Near real-time integration & Continuous extraction, transformation, loading & Up-to-the-minute data, critical for real-time decision-making & More complex and potentially expensive \\
\hline
Change Data Capture (CDC) & Real-time data synchronization & Captures and replicates source system changes & Real-time updates, minimizes data latency & Complex setup, resource-intensive \\
\hline
Data Federation/Virtualization & Heterogeneous data sources & Provides a unified view without physically integrating data & Minimizes data duplication, simplifies access & Performance challenges for complex queries, depends on underlying system performance\\
\hline
Data Replication & Distributed data synchronization & Copies data between systems, maintains synchronization & Ensures data consistency across locations & Resource-intensive, potential data conflicts \\
\hline
API-Based Integration & Third-party services/cloud apps & Connects systems via APIs for data exchange & Efficient for cloud services and external partners & Limited control over third-party APIs, custom development may be required, relies on API availability and stability \\
\hline
\end{tabular}
\end{table}

\noindent \textbf{Batch ETL} is suitable for scenarios where data latency is not critical. It processes data in batches, making it cost-effective for large volumes.

\noindent \textbf{Real-time ETL} provides near real-time data integration, crucial for applications requiring up-to-the-minute information. However, it is more complex and potentially expensive to implement.

\noindent \textbf{Change Data Capture (CDC)} focuses on real-time data synchronization by capturing and replicating changes in source systems. It minimizes data latency but requires a complex and resource-intensive setup.

\noindent \textbf{Data Federation/Virtualization} offers a unified view of data from heterogeneous sources without physical integration. While it minimizes data duplication and simplifies access, performance can be a challenge for complex queries. Its performance largely depends on the capabilities of underlying systems.

\noindent \textbf{Data Replication} ensures data consistency across different locations by copying data between systems.  While providing data consistency, it is resource-intensive and can lead to data conflicts if not managed carefully.

\noindent \textbf{API-Based Integration} utilizes APIs for data exchange, particularly useful for integrating with third-party services and cloud applications. While efficient for such services, control over third-party APIs is limited, and custom development may be necessary. The reliance on API availability and stability introduces potential vulnerabilities.

\subsection{Tools and Technologies for Data Integration}
We can simplify the data integration process using various tools and technologies, they are −

\subsubsection{ETL Tools}
ETL tools automate the extract, transform, and load processes, making data integration more efficient.

For example, Talend, Apache NiFi, and Informatica are popular ETL tools used to streamline data integration.

\subsubsection{Data Warehousing Solutions}
Data warehousing solutions provide a central repository for integrated data, which enables for an organized querying and analysis.

Amazon Redshift, Google BigQuery, and Snowflake are widely used data warehousing solutions.

\subsubsection{Data Virtualization Tools}
virtualization tools create a virtual data layer, allowing real-time access to integrated data.

For instance, Denodo, IBM Data Virtualization, and Red Hat JBoss Data Virtualization are examples of data virtualization tools.

\subsubsection{Enterprise Information Integration(EII)}
EII tools provide a unified view of data from disparate sources. They allow users to access and query data without having to move or copy the data. Popular EII solutions include IBM’s WebSphere Information Integrator, TIBCO ActiveMatrix BusinessWorks, and Microsoft SQL Server Integration Services

\subsection{Best Practices for Data Integration}
To ensure successful data integration, follow these best practices −

\subsubsection{Define Clear Objectives}
Clearly define your data integration objectives, such as improving data accuracy, enhancing decision-making, or streamlining operations.

Setting a goal to integrate sales and customer data for better customer insights is an example of a clear objective.

\subsubsection{Choose the Right Tools}
Select tools that fit your integration needs, considering factors like data volume, complexity, and real-time requirements.

For example, using an ETL tool for batch processing large datasets and a data virtualization tool for real-time data access can optimize data integration.

\subsubsection{Ensure Data Quality}
Implement data quality checks to ensure the accuracy and consistency of integrated data.

For example, using data validation rules to check for duplicates and missing values ensures high data quality.

\subsubsection{Maintain Data Security}
Ensure that data integration processes act in accordance with data security and privacy regulations, protecting sensitive information.

For example, encrypting data during transfer and ensuring compliance with GDPR are critical for maintaining data security.

\subsubsection{Monitor and Optimize}
Regularly monitor data integration processes and optimize them for performance and efficiency.

For example, using performance monitoring tools to identify bottlenecks and improve data processing speed can enhance efficiency.

\subsection{Challenges in Data Integration}
Data integration can present several challenges, including:

\begin{itemize}
    \item Data Silos: Data stored in isolated systems can be difficult to integrate, leading to incomplete or inconsistent data views.\\
    Example: Different departments using separate databases without a unified data integration strategy can create data silos. 
    \item Data Quality Issues: Poor data quality can lead to inaccurate analysis and decision-making, undermining the value of integrated data.\\
    Example: Inconsistent data formats and duplicate records can cause errors in reporting.
    \item Complex Data Transformation: Complex data transformation processes can be time-consuming and require specialized skills.\\
    Example: Converting data from various formats and structures to a common format for integration can be challenging.
    \item Scalability: Integrating large volumes of data from multiple sources can be challenging, requiring scalable solutions.\\
    Example: Handling the integration of high-frequency transactional data from e-commerce and financial systems demands scalable data integration solutions.
\end{itemize}


Bibliografía: 

- El libro "Fundamentals of Data Engineering" va de : the data engineering lifecycle: data generation, storage, ingestion, transformation, and serving. Since the dawn of data, we've seen the rise and fall of innumerable specific technologies and vendor products, but the data engineering lifecycle stages have remained essentially unchanged. 

\end{document}
