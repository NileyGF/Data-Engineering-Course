\documentclass[12pt]{book}

% Packages
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
\usepackage{amsmath}  % For math equations
\usepackage{hyperref} % For hyperlinks
\usepackage{titlesec} % For customizing section titles
\usepackage{graphicx} % Required for inserting images
\usepackage{changepage}

\title{Ingeniería de Datos. Conferencias}
\author{Lic. Niley González}
\date{2024 - 2025}

% Customize Chapter and Section Titles
\titleformat{\chapter}[display]
  {\normalfont\LARGE\bfseries}{\thechapter}{20pt}{}
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}

\begin{document}

\maketitle

% Chapter 1: Conference 1
\chapter{Conferencia 1}
\normalfont\LARGE \textbf{Introducción a la Ingeniería de Datos. Metodologías para la Ciencia de Datos}
\normalfont\small\\

¿Cómo definirían ustedes la ciencia de datos?\\

\textit{Pausa para que los estudiantes compartan sus ideas.}\\

Diferentes autores han intentado definir el campo de la ciencia de datos; algunas de las prespectivas son:

\begin{itemize}
    \item campo multidisciplinario que combina áreas como la informática, las matemáticas y la estadística. Su objetivo principal es utilizar métodos y técnicas científicas para extraer conocimiento y valor de grandes volúmenes de datos, estructurados o no estructurados. (2019)
    \item campo interdisciplinario que integra disciplinas como las ciencias de la computación, el aprendizaje automático, las matemáticas y las estadísticas. (2021)
    \item tema multidisciplinario cuyo propósito es descubrir conocimiento para apoyar la toma de decisiones en diversos contextos empresariales.(2020)
\end{itemize}
En general, hay un consenso en que la ciencia de datos es un campo multidisciplinario o interdisciplinario que se nutre de áreas como la informática, la estadística y las ciencias de la computación. Su enfoque principal es el estudio de los datos, con el propósito de extraer conocimiento y valor a partir de ellos.\\

Ahora, ¿qué entienden ustedes por una metodología?\\

\textit{Pausa para que los estudiantes compartan sus ideas.}\\

Una metodología puede entenderse como una \textbf{estrategia, guía o conjunto de pautas} que nos ayudan a desarrollar un proceso o actividad de manera estructurada. A diferencia de las herramientas o tecnologías específicas, las metodologías no están ligadas a un software o hardware en particular. En cambio, proporcionan un \textbf{marco de trabajo} que nos indica cómo proceder de manera sistemática para alcanzar nuestros objetivos.

¿Por qué es importante esto?\\
En el contexto de la ciencia de datos, las metodologías ayudan a abordar problemas complejos de manera organizada, intentando que cada paso del proceso esté bien definido y alineado con los objetivos del proyecto. 

% https://medium.datadriveninvestor.com/data-science-project-management-methodologies-f6913c6b29eb

\section{Metodologías en Ciencia de Datos}
\label{sec:metodologias}

A continuación se presentan varias metodologías de la Ciencia de Datos, analizando críticamente su estructura, enfoque y aplicabilidad en diferentes contextos. El objetivo es comprender cómo cada metodología se adapta -o no- a distintos escenarios organizacionales, técnicos y de complejidad. A través de este análisis, descubriremos por qué ninguna metodología puede aplicarse universalmente a todas las circunstancias, y cómo su implementación rígida y sin adaptación puede llevar a resultados subóptimos.

\subsection{KDD (Knowledge Discovery in Databases)}
Definido como un proceso interactivo e iterativo de 5 fases para descubrir conocimiento por sus creadores en 1996:
\begin{itemize}
    \item \textbf{Selección}: Los datos cambian de acuerdo con los objetivos del proceso. Se establece un grupo de datos con el que proceder.
    \item \textbf{Procesamiento/Limpieza}: En la fase de data cleaning se examina la calidad de los datos y se manejan situaciones como     datos con parámetros faltantes/nulos, datos duplicados, etc.
    \item \textbf{Transformación/Reducción}: Convertir datos pre-procesados en utilizables, identificando características importantes según los objetivos del proceso.
    \item \textbf{Minería de Datos}: Búsqueda de patrones de interés mediante técnicas como clasificación, regresión, clustering, correlaciones, etc.
    \item \textbf{Interpretación/Evaluación}: Fase final, de consolidación del conocimiento encontrado en los datos. Se preparan los resultados para documentación y toma de decisiones. Los datos se han transformado en visualizaciones para facilitar la evaluación del resultado depurado.
\end{itemize}

\subsection{CRISP-DM (Cross-Industry Standard Process for Data Mining)}
Publicada en 1999 con el objetivo de estandarizar los procesos de minería de datos en diversos sectores, se ha consolidado como la metodología más utilizada en proyectos de minería de datos, análisis y ciencia de datos:
\begin{itemize}
    \item \textbf{Comprensión Empresarial}: se centra en entender los objetivos del proyecto, para luego ser evaluados y descubrir si     los datos son aptos para cumplir con los objetivos y producir un plan de proyecto.
    \item \textbf{Comprensión de Datos}: Recopilación, descripción y exploración de los datos iniciales.
    \item \textbf{Preparación de Datos}: 5 tareas: selección de datos, limpieza de datos, construcción de datos, integración de datos y ajuste del formato.
    \item \textbf{Modelado}: Construcción y evaluación de varios modelos con diferentes técnicas algorítmicas. Se determina que algoritmos probar (por ejemplo, regresión, red neuronal); se realiza un     diseño de experimentos; construir el modelo y evaluar el modelo.
    \item \textbf{Evaluación}: Se evalúa y revisa la creación de modelos respecto a los objetivos comerciales. Para ello se evaluan los resultados, se revisan los procesos y se determinan los próximos pasos.
    \item \textbf{Implementación}: Despliegue, seguimiento, mantenimiento y revisión final de los resultados obtenidos.
\end{itemize}
CRISP-DM presenta un proceso iterativo estructurado, definido y documentado. Es una metodología empleada como referencia por otras metodologías.

\subsection{SEMMA (Sample, Explore, Modify, Model, Assess)}
Propuesta para manejo de grandes volúmenes de datos:
\begin{itemize}
    \item \textbf{Muestreo}: Selección de una muestra representativa de datos del problema que está investigando. La forma correcta de obtener una muestra es la selección aleatoria.
    \item \textbf{Exploración}: Exploración de información útil, con la finalidad de sintetizar el problema y mejorar la eficiencia del modelo.
    \item \textbf{Modificación}: Manipulación de los datos con base en la investigación realizada para que los datos ingresados al modelo estén definidos y en un formato adecuado.
    \item \textbf{Modelado}: Modelado de datos, con el propósito de establecer una relación entre las variables explicativas y el objeto de estudio.
    \item \textbf{Evaluación}: Validación comparativa de los resultados, a través del análisis de los modelos, comparado con otros modelos estadísticos o una nueva muestra poblacional. 
\end{itemize}

\subsection{RAMSYS (Rapid collaborative data Mining System)}
Desarrollada por Steve Moyle en 2002. Es una metodología que apoya proyectos de minería de datos, por ello amplía el método CRISP-DM.\\
Define su metodología en tres roles:
\begin{itemize}
    \item \textbf{Modeladores}: encargados de probar la viabilidad de las hipótesis y generar nuevos conocimientos.
    \item \textbf{Data Master}: responsable de mantener la versión actual de la base de datos, las transformaciones y la información sobre los datos, como metadatos e información sobre la calidad de los datos.
    \item \textbf{Comité de Dirección}: responsable de establecer los desafíos del proyecto, definir criterios, recibir y seleccionar las presentaciones.
    %\item Enfatiza colaboración e intercambio de conocimiento
    %\item Permite experimentación con diversas técnicas de resolución de problemas
\end{itemize}

% \subsection{CATALYST (P3TQ)}
% Conocida como P3TQ por sus siglas en inglés product, place, price, time and quantity, que existen en la cadena de valor organizacional. Está formada por dos partes o sub-metodologías. La primera denominada modelado de negocio o MII, y la segunda llamada minería de datos o MIII. \\
% \begin{itemize}
%     \item \textbf{Metodología para el Modelado del Negocio (MII)}:
    
%     Brinda una guía de pasos para reconocer un problema y las necesidades reales de la empresa. Con ella se busca modelar el problema/oportunidad que aborda el proyecto.\\
%     El proceso incluye: exploración de datos en búsqueda de patrones de interés, identificación de problemas/oportunidades, prospección de capacidades de minería de datos y el desarrollo de estrategias organizacionales.
%     \item \textbf{Metodología para la Minería de Datos (MIII)}: se divide en los siguientes pasos:\\
%     - Preparación y exploración de datos para encontrar relaciones útiles.\\
%     - Selección de herramientas y modelos para analizar el problema.\\
%     - Refinamiento iterativo del modelo. Verificar los resultados, matrices, gráficos u observaciones según sea el método exploratorio o predictivo.\\
%     - Implementación del modelo  incluyendo una revisión de la retroalimentación con los usuarios.
% \end{itemize}

\subsection{TDSP (Team Data Science Process)}
Metodología de Microsoft de 2017. Es de cierta forma una combinación de Scrum y CRISP-DM. El ciclo de vida de TDSP se compone de cinco etapas principales:
\begin{itemize}
    \item \textbf{Comprensión empresarial}: Se definen los objetivos y se identifican las fuentes de datos.
    \item \textbf{Adquisición y comprensión de datos}: Se incorporan los datos y se determina si se puede responder a la pregunta 
    planteada (combina efectivamente la Comprensión de los Datos y la Limpieza de los Datos de CRISP-DM).
    \item \textbf{Modelado}: Ingeniería de características (feature engineering) y entrenamiento de modelos (model training). Combina Modelado y Evaluación de CRISP-DM).
    \item \textbf{Implementación}: Implementar en un entorno de producción.
    \item \textbf{Aceptación del cliente}: Validación por parte del cliente de si el sistema satisface las necesidades del negocio (una fase no cubierta explícitamente por CRISP-DM).
\end{itemize}
TDSP aborda la debilidad de CRISP-DM en cuanto a la falta de definición del equipo, definiendo seis roles:
\begin{itemize}
    \item Arquitecto de soluciones
    \item Project Manager
    \item Ingeniero de datos
    \item Científico de datos
    \item Desarrollador de aplicaciones
    \item Líder de proyecto (Project lead)
\end{itemize}

\subsection{Conclusiones del tema}

A lo largo de esta conferencia, hemos explorado diversas metodologías utilizadas en la Ciencia de Datos. Aunque cada una tiene sus particularidades, todas comparten elementos comunes:

\begin{itemize}
    \item \textbf{Comprensión del Negocio}: Definir el problema empresarial y se identifican los objetivos del análisis. El equipo de ciencia de datos debe trabajar en estrecha colaboración con los clientes para entender el problema y definir los objetivos.
    \item \textbf{Comprensión de los Datos}: Identificar y recopilar los datos requerido para el análisis. Exploración de los datos para entender su estructura, calidad y completitud.
    \item \textbf{Preparación de los Datos}: Limpiar, transformar y preparar los datos para garantizar que estén en el formato y calidad adecuados para el análisis.
    \item \textbf{Modelado de Datos}: Seleccionar técnicas de modelado adecuadas para analizar los datos e implementar modelos predictivos. Esta etapa también involucra la selección de algoritmos, ajuste de parámetros y validación del modelo.
    \item \textbf{Evaluación}: Evaluar el rendimiento del modelo y su capacidad para resolver el problema empresarial. Utilizando métricas de evaluación adecuadas y realizando mejoras al modelo si es necesario.
    \item \textbf{Despliegue}: Desplegar el modelo en un entorno de producción, integrándolo en los procesos de la negocio y asegurando su correcto funcionamiento.
    \item \textbf{Monitoreo y Mantenimiento}: Supervisar el rendimiento del modelo en producción y realizar ajustes para mantener su efectividad.
\end{itemize}

En resumen, una metodología de Ciencia de Datos es un enfoque estructurado que combina comprensión del negocio, manejo de datos, modelado y evaluación para transformar datos en soluciones efectivas. Sin embargo, es crucial recordar que:

\begin{itemize}
    \item \textbf{No existe una metodología universal}: Cada proyecto tiene características únicas que pueden requerir adaptaciones o combinaciones de enfoques.
    \item \textbf{La flexibilidad es clave}: Las metodologías no deben aplicarse de manera rígida, sino como guías que permitan ajustarse a las necesidades específicas del problema.
    \item \textbf{La colaboración es esencial}: La comunicación entre científicos de datos, ingenieros y stakeholders es fundamental para alinear objetivos y garantizar resultados útiles.
    \item \textbf{El ciclo nunca termina}: La Ciencia de Datos es un proceso iterativo, donde el monitoreo y la mejora continua son parte integral del éxito.
\end{itemize}

\section{Ingeniería de Datos}

Para concluir vamos a definir en que consiste el trabajo de un ingeniero de datos. 

En el libro \textit{Fundamentals of Data Engineering} de \textit{Joe Reis and Matt Housley} se define el término como:

La Ingeniería de Datos consiste en el desarrollo, interpretación y mantenimiento de sistemas y procesos que toman datos crudos y producen información consistente de alta calidad que soporta downstream casos de uso como analíticas y machine learning.

Un ingeniero de datos maneja el ciclo de vida de la ingeniería de datos que abarca el proceso que comienza en obtener los datos de las fuentes y termina sirviéndolos, después de procesados para los casos de uso.

\textbf{En este curso estaremos viendo la ingeniería de datos como proceso integral que abarca la recolección, almacenamiento, procesamiento, y disponibilidad de datos.}\\

\subsection{Breve historia de la evolución de la ingeniería de datos}
\textbf{Los comienzos: 1980 a 2000} de \textemp{Data Warehouse} a la Web.\\
El nacimiento de la ingeniería de datos tiene sus raíces en data warehousing\footnote{Centralized repository that aggregates data from various sources to support data analysis, data mining and artificial intelligence}.\\
Que data desde la década de 1970 y toma forma en los 80s cuando se crea el término data warehouse. Luego surge el Structured Query Language (SQL).
Mientras crecían los nacientes sistemas de datos, los negocios necesitaban dedicar herramientas a reportar y a business intelligence (BI). 
Surgiendo roles como BI ingeniero\footnote{A business intelligence engineer designs, implements, and maintains systems used to collect and analyze business intelligence data.}, desarrolladores ETL\footnote{Responsible for designing, building, managing, and maintaining ETL (Extract, Transform, Load) processes.} e ingenieros de data warehouse.
Precursores de lo que se considera actualmente los ingenieros de datos. También se popularizó el internet a mediados de los 90s, surgiendo una ola de compañías centradas en la web.

\textbf{Los inicios de los 2000}: El nacimiento de la ingeniería de datos contemporánea.\\
Las grandes compañías sobrevivientes como Yahoo, Google y Amazon crecerían hasta convertirse en gigantes tecnológicos. 
La necesidad de sistemas escalables, con alta disponibilidad, confiables y cost-effective llevó a innovaciones en sistemas distribuidos y almacenamiento, marcando el inicio de la era del 'big data'. La combinación del surgimiento de nuevos algoritmos y metodologías como: 'Google File System', 'MapReduce', Apache Hadoop, Amazon Elastic Compute Cloud y su apertura como servicio al público a través de Amazon Web Services (AWS) creó una nueva era en el manejo y tratamiento de datos.
Nacía la era de ingeniero de big data.\\
Mientras AWS se volvió muy rentable otras compañías lanzaros sus propios ecosistemas en la nube: Google Cloud, Microsoft Azure, DigitalOcean. La nube es discutiblemente una de las innovaciones más significativas del siglo 21; iniciando una revolución en la forma en que el software y las aplicaciones de datos se desarrollan y despliegan

\textbf{Finales de los 2000 y la década de 2010}: The Big Data Engineering Era\\
Surgen herramientas open source que democratizan el acceso a tecnologías de big data; que ya no estarían limitadas solo a las grandes compañías. \\
También comienza la transformación de procesamiento en batch a streaming a partir de eventos. \\
Ocurre una explosión de herramientas de manejo de datos. Y los ingenieros de bog data debían ser proficientes en desarrollo de software y configuración de infraestructuras de bajo nivel. \\
Big data engineers se centraban en manejar sistemas de datos de gran escala, pero la complejidad y el costo de mantener  estas nuevas herramientas impulsaron a optar por simplificaciones. \\
El término "big data" perdió su brillo mientras se volvían más accesibles las herramientas para procesarlos; los ingenieros de big data engineers pasan a ser simplemente ingenieros de datos.

\textbf{2020s}: Ingeniería por el ciclo de vida de los datos.\\
El rol de los ingenieros de datos se encuentra en rápida evolución. La tendencia está siendo centrarse en herramientas descentralizadas, modulares y abstractas. 
Las tendencias populares a principios de la década de 2020 incluyen el modern data stack (MDS), que representa una colección de productos "listos para usar", tanto de código abierto como de terceros, ensamblados para facilitar el trabajo de los analistas. Al mismo tiempo, las fuentes de datos y los formatos de datos están creciendo tanto en variedad como en tamaño. La ingeniería de datos es, cada vez más, una disciplina de interconexión, que conecta varias tecnologías como si fueran piezas de LEGO, para servir a los objetivos empresariales finales. 

\subsection{Relación entre la Ingeniería de Datos y la Ciencia de Datos}
La ingeniería de datos se sitúa upstream de la ciencia de datos, lo que significa que los ingenieros de datos proporcionan las entradas utilizadas por los científicos de datos.\\
Para muchos lo más interesante de la ciencia de datos es construir y optimizar modelos de Machine Learning; la realidad es que se estima que entre el 70\% y el 80\% del tiempo se dedica a recopilar, limpiar y procesar datos.\\
Además; usualmente los científicos de datos no están entrenados para diseñar sistemas de datos de grado de producción, y terminan haciendo este trabajo improvisadamente porque carecen del soporte y los recursos de un ingeniero de datos.\\
En un mundo ideal, los científicos de datos deberían dedicar más del 90\% de su tiempo al análisis, la experimentación y el ML. Esto se logra cuando los ingenieros de datos se centran en construir una base sólida para que los científicos de datos tengan éxito.

\subsection{Habilidades del Ingeniero de Datos}
El conjunto de habilidades de un ingeniero de datos debe abarcar las ideas subyacentes de la ingeniería de datos: seguridad, gestión de datos, DataOps, arquitectura de datos e ingeniería de software. Se requiere un entendimeinto de como evaluar herramientas de datos y como estas encajan en el ciclo de vida de la ingeniería de datos. 

Un ingeniero de datos maneja una gran cantidad de piezas móviles complejas y debe optimizar constantemente a lo largo de los ejes de costo, agilidad, escalabilidad, simplicidad, reutilización e interoperabilidad. También se espera que el ingeniero de datos cree arquitecturas de datos ágiles que evolucionen a medida que surjan nuevas tendencias.

Por definición, un ingeniero de datos debe comprender tanto los datos como la tecnología. Con respecto a los datos, esto implica conocer varias de las mejores prácticas en torno a la gestión de datos. En el extremo tecnológico, un ingeniero de datos debe estar al tanto de varias opciones de herramientas, su interrelación y sus trade-offs.

% Chapter 2: Conference 2
\chapter{Conferencia 2}
\normalfont\LARGE \textbf{El ciclo de vida de la ingeniería de datos.}
\normalfont\small\\

\section{El Ciclo de Vida de la Ingeniería de Datos (Data Engineering Lifecycle)}
El ciclo de vida de la ingeniería de datos o Data Engineering Lifecycle.

Estudiaremos este ciclo de forma agnóstica a un tipo de software o hardware específico.
La idea es alejarnos de las tecnologías y centrarnos en los datos y el propósito que deben servir.

El ciclo de vida de la ingeniería de datos comprende las siguientes etapas que transforman datos crudos en un producto final con valor listo para ser consumido por los analistas, científicos de datos, ingenieros de ML y otros:

\begin{itemize}
    \item Generación
    \item Almacenamiento
    \item Ingesta
    \item Transformación
    \item Serving
\end{itemize}
Además de las etapas, el ciclo tiene una noción de ideas subyacentes, críticas a o largo de todo el ciclo de vida. Estas incluyen seguridad, gestión de datos, DataOps, arquitectura de datos, orquestación e ingeniería de software.

El almacenamiento ocurre a lo largo de todo el ciclo a medida que los datos fluyen desde el inicio hasta el final.\\
En general, las etapas intermedias (almacenamiento, ingesta, transformación) pueden mezclarse un poco. Varias etapas del ciclo de vida pueden repetirse, ocurrir fuera de orden, superponerse o entrelazarse.

El ciclo de vida de la ingeniería de datos es un subconjunto de todo el ciclo de vida de los datos.

Un ingeniero de datos tiene varios objetivos de alto nivel a lo largo del ciclo de vida de los datos: producir un Return on Investment(ROI) y reducir los costos, reducir el riesgo y maximizar el valor y la utilidad de los datos.

\subsection{Generación}

Un \textit{sistema de origen} es el origen de los datos utilizados en el ciclo de vida de la ingeniería de datos. Por ejemplo, un sistema de origen podría ser un dispositivo IoT, una cola de mensajes de aplicación o una base de datos transaccional.

Un ingeniero de datos consume datos de un sistema de origen, pero normalmente no posee ni controla el sistema de origen en sí. El ingeniero de datos necesita tener un entendimiento funcional de cómo funcionan los sistemas de origen, la forma en que generan los datos, la frecuencia y la velocidad de los datos, y la variedad de datos que se generan.\\
También necesitan mantener una línea de comunicación abierta con los propietarios del sistema de origen sobre los cambios que podrían interrumpir los pipelines y los análisis.

Las fuentes producen datos consumidos por sistemas posteriores, incluidas hojas de cálculo generadas por humanos, sensores IoT y aplicaciones web y móviles. Cada fuente tiene su volumen y cadencia únicos de generación de datos.

Ejemplos de sistemas de origen: 
\begin{itemize}
    \item Dispositivos IoT
    \item Terminales de tarjetas de crédito
    \item Operaciones de la bolsa
    \item Sensores de telescopios
    \item Spreadsheets
\end{itemize}

Tipos de sistemas de origen:
\begin{itemize}
    \item Archivos y datos no estructurados (Excel, CSV, JSON, XML, TXT). Estos archivos tienen sus peculiaridades y pueden ser estructurados (Excel, CSV), semi-estructurados (JSON, XML, CSV) o no estructurados (TXT, CSV).
    \item APIs
    \item Bases de Datos: Pueden ser relacionales(SQL), o no relacionales como: Document stores, Wide-column, Graph databases, 
    \item Application Databases (online transaction processing (OLTP) system) : Una base de datos de aplicaciones almacena el estado de una aplicación. Un ejemplo típico es una base de datos que almacena los saldos de cuentas bancarias. A medida que se producen transacciones y pagos de los clientes, la aplicación actualiza los saldos de las cuentas bancarias. Normalmente, una base de datos de aplicaciones es un sistema de procesamiento de transacciones en línea (OLTP): una base de datos que lee y escribe registros de datos individuales a una alta velocidad.
    %\item Change data capture (CDC) La captura de datos modificados (CDC, por sus siglas en inglés) es un método para extraer cada evento de cambio (inserción, actualización, eliminación) que ocurre en una base de datos. La CDC se utiliza con frecuencia para replicar entre bases de datos casi en tiempo real o para crear un flujo de eventos para el procesamiento posterior.
    \item Fuentes de Datos de Terceros: Debido a que la tecnología se ha integrado en muchas empresas (y agencias gubernamentales), estas buscan ofrecer sus datos a clientes y usuarios. El acceso directo a datos de terceros se realiza comúnmente a través de APIs, mediante el intercambio de datos en una plataforma en la nube o mediante la descarga de datos. 
    \item Colas de Mensajes y Plataformas de Streaming de Eventos: Las arquitecturas basadas en eventos (event-driven) son cada vez más populares en software. Además, las aplicaciones que integran analítica en tiempo real (data apps) se benefician de las arquitecturas basadas en eventos, ya que los eventos disparan acciones en la aplicación y alimentan el análisis en tiempo real.
\end{itemize}

Ahora se presentan preguntas para evaluar sistemas de origen que los ingenieros de datos deben considerar:
\begin{itemize}
    \item ¿Cuáles son las características esenciales de la fuente de datos? ¿Es una aplicación? ¿Un enjambre de dispositivos IoT?
    \item ¿Cómo se persisten los datos en el sistema de origen? ¿Los datos se persisten a largo plazo, o son temporales y se eliminan rápidamente?
    \item ¿A qué velocidad se generan los datos? ¿Cuántos eventos por segundo? ¿Cuántos gigabytes por hora?
    \item ¿Qué nivel de consistencia pueden esperar los ingenieros de datos de los datos de salida? Si se están ejecutando comprobaciones de calidad de datos contra los datos de salida, ¿con qué frecuencia se producen inconsistencias de datos: valores nulos donde no se esperan, formato deficiente, etc.?
    \item ¿Con qué frecuencia se producen errores?
    \item ¿Los datos contendrán duplicados?
    \item ¿Algunos valores de datos llegarán tarde, posiblemente mucho más tarde que otros mensajes producidos simultáneamente?
    \item ¿Cuál es el esquema de los datos ingeridos? ¿Los ingenieros de datos necesitarán unir varias tablas o incluso varios sistemas para obtener una imagen completa de los datos?
    \item Si hay cambios en el esquema (por ejemplo, se agrega una nueva columna), ¿cómo se aborda esto y se comunica a las partes interesadas en el resto del procesamiento?
    \item ¿Con qué frecuencia se deben extraer los datos del sistema de origen?
    \item Para los sistemas con estado (por ejemplo, una base de datos que rastrea la información de la cuenta del cliente), ¿se proporcionan los datos como instantáneas periódicas o eventos de actualización de la captura de datos de cambio (change data capture CDC)? ¿Cuál es la lógica de cómo se realizan los cambios y cómo se rastrean estos en la base de datos de origen?
    \item ¿Quién/qué es el proveedor de datos que transmitirá los datos para el consumo posterior?
    \item ¿Leer de una fuente de datos afectará su rendimiento?
    \item ¿El sistema de origen tiene dependencias anteriores? ¿Cuáles son las características de estos sistemas anteriores?
    \item ¿Existen controles de calidad de datos para verificar datos tardíos o faltantes?
\end{itemize}

\subsection{Almacenamiento}
Whether data is needed seconds, minutes, days, months, or years later, it must persist in storage until systems are ready to consume it for further processing and transmission.

El proceso de elección de almacenamiento es clave para el éxito en el resto del ciclo de vida de los datos, y también es una de las etapas más complicadas. 
Primero, las arquitecturas de datos en la nube a menudo se aprovechan de varias alternativas de almacenamiento. 
Segundo, pocas herramientas de almacenamiento de datos funcionan puramente como almacenamiento, y muchas admiten complejas queries de transformación de datos; incluso el almacenamiento basado en objetos pueden admitir potentes capacidades de consulta, por ejemplo, Amazon S3 Select. 
Tercero, si bien el almacenamiento es una etapa del ciclo de vida de la ingeniería de datos, con frecuencia toca otras etapas, como la ingesta, la transformación y el serving.

El almacenamiento se extiende a lo largo de todo el ciclo de vida de la ingeniería de datos, y a menudo en múltiples lugares del pipeline de datos. En muchos sentidos, la forma en que se almacenan los datos impacta en cómo se utilizan en todas las etapas del ciclo de vida de la ingeniería de datos. Por ejemplo, los almacenes de datos en la nube pueden almacenar datos, procesar datos en pipelines y servirlos a los analistas. 

\textbf{Sistemas de Almacenamiento}:
\begin{itemize}
    \item Almacenamiento de Archivos (File Storage): Estos sistemas organizan los archivos en una estructura de árbol de directorios.
    \item Almacenamiento de Objetos (Object Storage): Contiene objetos de todos los tamaños y formas. El término "almacenamiento de objetos" puede ser confuso debido a los múltiples significados de "objeto" en informática. En este contexto, se refiere a una construcción especializada similar a un archivo.  Puede ser cualquier tipo de archivo: TXT, CSV, JSON, imágenes, videos o audio.  Amazon S3, Azure Blob Storage y Google Cloud Storage (GCS) son almacenes de objetos ampliamente utilizados. Además, muchos almacenes de datos en la nube (y un número creciente de bases de datos) utilizan el almacenamiento de objetos como su capa de almacenamiento, y los data lakes en la nube generalmente se basan en almacenes de objetos.
    \item Sistemas de Almacenamiento Basados en Caché y Memoria: Ofrecen una excelente latencia y velocidades de transferencia. Sin embargo, tradicional son extremadamente vulnerable a la pérdida de datos, ya que un corte de energía, incluso de un segundo, puede borrar los datos. Los sistemas de almacenamiento basados en RAM generalmente se centran en aplicaciones de almacenamiento en caché, presentando datos para un acceso rápido y un alto ancho de banda. Estos sistemas de caché ultrarrápidos son útiles cuando los ingenieros de datos necesitan servir datos con una latencia de recuperación ultrarrápida.
\end{itemize}

\textbf{Abstracciones de Almacenamiento}:
\begin{itemize}
    \item Almacén de Datos (Data Warehouse): El término "almacén de datos" se refiere a plataformas tecnológicas (por ejemplo, Google BigQuery y Teradata), una arquitectura para la centralización de datos y un patrón organizativo dentro de una empresa.
    \item Lago de Datos (Data Lake): Originalmente concebido como un almacén masivo donde los datos se conservaban en forma bruta y sin procesar.
    \item Casa del Lago de Datos (Data Lakehouse): Una arquitectura que combina aspectos del almacén de datos y el lago de datos.  Tal como se concibe generalmente, la "lakehouse" almacena datos en el almacenamiento de objetos al igual que un lago. Sin embargo, la "lakehouse" agrega a esta disposición características diseñadas para optimizar la gestión de datos y crear una experiencia de ingeniería similar a la de un almacén de datos. Esto significa un soporte robusto para tablas y esquemas y características para gestionar actualizaciones y eliminaciones incrementales.  Las "lakehouses" típicamente también soportan el historial de la tabla y la reversión (rollback); esto se logra conservando versiones antiguas de archivos y metadatos.
    \item Plataformas de Datos (Data Platforms): Algo nuevo.
\end{itemize}

Estas son algunas preguntas claves al elegir un sistema de almacenamiento para data warehouse, un data lakehouse, una base de datos o un almacenamiento de objetos(object storage):

\begin{itemize}
    \item ¿Es este sistema de almacenamiento compatible con las velocidades de lectura y escritura requeridas por la arquitectura?
    \item ¿El almacenamiento creará un cuello de botella para los procesos posteriores?
    \item ¿Entienden cómo funciona esta tecnología de almacenamiento? ¿Está utilizando el sistema de almacenamiento de manera óptima o cometiendo actos antinaturales? % Por ejemplo, ¿está aplicando una alta tasa de actualizaciones de acceso aleatorio en un sistema de almacenamiento de objetos? (Este es un antipatrón con una importante sobrecarga de rendimiento).
    \item ¿Este sistema de almacenamiento manejará la prevista escala futura? Debe considerar todos los límites de capacidad en el sistema de almacenamiento: almacenamiento total disponible, tasa de operación de lectura, volumen de escritura, etc.
    \item ¿Los usuarios y procesos posteriores podrán recuperar datos en el service-level agreement (SLA) requerido?
    \item ¿Se está capturando metadatos sobre la evolución del esquema, los flujos de datos, el linaje de datos, etc.? Los metadatos tienen un impacto significativo en la utilidad de los datos. Los metadatos representan una inversión en el futuro, mejorando drásticamente la detectabilidad y el conocimiento institucional para agilizar futuros proyectos y cambios de arquitectura.
    \item ¿Es esta una solución de almacenamiento pura (almacenamiento de objetos) o admite patrones de consulta complejos (por ejemplo, un data warehouse en la nube)?
    \item ¿El sistema de almacenamiento es independiente del esquema (almacenamiento de objetos)? ¿Esquema flexible (Cassandra)? ¿Esquema forzado (un data warehouse en la nube)?
    \item ¿Cómo está rastreando los datos maestros, la calidad de los datos de registros dorados y el linaje de datos para la gobernanza de datos?
    \item ¿Cómo está manejando el cumplimiento normativo y la gobernanza de los datos? Por ejemplo, ¿puede almacenar sus datos en ciertas ubicaciones geográficas pero no en otras?
\end{itemize}

\subsection{Ingesta}
Usualmente, los sistemas de origen y la ingesta representan los cuellos de botella más significativos en el ciclo de vida de la ingeniería de datos. Los sistemas de origen están normalmente fuera de su control directo y podrían dejar de responder aleatoriamente o proveer datos de baja calidad. O, su servicio de ingesta de datos podría misteriosamente dejar de funcionar por muchas razones.

Al prepararse para diseñar o construir un sistema, aquí hay algunas preguntas primarias sobre la etapa de ingesta:
\begin{itemize}
    \item ¿Cuáles son los casos de uso para los datos que estoy ingiriendo? ¿Puedo reutilizar estos datos en lugar de crear múltiples versiones del mismo conjunto de datos?
    \item ¿Los sistemas que generan e ingieren estos datos son fiables, y los datos están disponibles cuando los necesito?
    \item ¿Cuál es el destino de los datos después de la ingesta?
    \item ¿Con qué frecuencia necesitaré acceder a los datos?
    \item ¿En qué volumen llegarán típicamente los datos?
    \item ¿En qué formato están los datos? ¿Pueden mis sistemas de almacenamiento y transformación posteriores manejar este formato?
    \item ¿Están los datos iniciales en buen estado para su inmediato uso posterior? Si es así, ¿por cuánto tiempo, y qué podría causar que se vuelvan inutilizables?
    \item Si los datos provienen de una fuente de \textit{streaming}, ¿necesitan ser transformados antes de llegar a su destino? ¿Sería apropiada una transformación dentro del propio \textit{stream}?
\end{itemize}

\subsubsection{Lotes (\textit{Batch}) vs. \textit{Streaming}}
Virtualmente todos los datos con los que tratamos son inherentemente \textit{streaming}. Los datos son casi siempre producidos y actualizados continuamente en su origen. La ingesta por lotes es simplemente una forma especializada y conveniente de procesar este *stream* en grandes trozos—por ejemplo, manejar el valor de un día completo de datos en un solo lote.

La ingesta de \textit{streaming} nos permite proveer datos a los sistemas posteriores—ya sean otras aplicaciones, bases de datos, o sistemas de analítica—de una forma continua y en tiempo real.

La elección depende en gran medida del caso de uso y las expectativas de puntualidad de los datos.

Las siguientes son algunas preguntas que debe hacerse al determinar si la ingesta de \textit{streaming} es una opción apropiada sobre la ingesta por lotes:
\begin{itemize}
    \item Si se ingieren los datos en tiempo real, ¿pueden los sistemas de almacenamiento posteriores manejar la tasa de flujo de datos?
    \item Es necesaria una ingesta de datos en tiempo real de mili-segundos? ¿O funcionaría un enfoque de micro-lotes, acumulando e ingiriendo datos, por ejemplo, cada minuto?
    \item ¿Cuáles son mis casos de uso para la ingesta de \textit{streaming}? ¿Qué beneficios específicos obtengo al implementar \textit{streaming}? Si obtengo datos en tiempo real, ¿qué acciones puedo tomar sobre esos datos que serían una mejora con respecto al lote?
    \item ¿Mi enfoque de priorizar \textit{streaming} costará más en términos de tiempo, dinero, mantenimiento, tiempo de inactividad y costo de oportunidad que simplemente hacer lotes?
    \item ¿Son mi \textit{pipeline} y sistema de \textit{streaming} confiables y redundantes si falla la infraestructura?
    \item ¿Qué herramientas son las más apropiadas para el caso de uso? ¿Debería usar un servicio gestionado (Amazon Kinesis, Google Cloud Pub/Sub, Google Cloud Dataflow) o levantar mis propias instancias de Kafka, Flink, Spark, Pulsar, etc.? Si hago lo último, ¿quién lo administrará? ¿Cuáles son los costos y las ventajas y desventajas?
    \item Si estoy implementando un modelo de ML, ¿qué beneficios tengo con las predicciones en línea y posiblemente el entrenamiento continuo?
    \item ¿Estoy obteniendo datos de una instancia de producción en vivo? Si es así, ¿cuál es el impacto de mi proceso de ingesta en este sistema de origen?
\end{itemize}

\subsubsection{Push vs. Pull}
En el modelo de ingesta de datos de \textit{push} un sistema de origen envía datos hacia un destino; ya sea una base de datos, un almacén de objetos o un sistema de archivos.
En el modelo de \textit{pull} los datos se recuperan del sistema de origen.

El proceso de extracción, transformación y carga (ETL) comúnmente utilizado en flujos de trabajo de ingesta orientados a lotes, La parte de extracción (E) de ETL aclara que estamos lidiando con un modelo de ingesta de tipo \textit{pull}.

Con la ingesta de \textit{streaming}, los datos evitan una base de datos \textit{backend} y se envían (\textit{push}) directamente a un punto final, típicamente con datos almacenados en búfer por una plataforma de \textit{event-streaming}. Este patrón es útil con flotas de sensores IoT que emiten datos de sensores. En lugar de depender de una base de datos para mantener el estado actual, simplemente pensamos en cada lectura registrada como un evento.
Este patrón también está creciendo en popularidad en aplicaciones de software, ya que simplifica el procesamiento en tiempo real, permite a los desarrolladores de aplicaciones adaptar sus mensajes para análisis posteriores, y simplifica enormemente la vida de los ingenieros de datos.

\subsection{Transformación}
La siguiente etapa en el ciclo de vida de la ingeniería de datos es la transformación, lo que significa que los datos deben ser modificados desde su forma original a algo útil para los casos de uso posteriores.\\
Típicamente, la etapa de transformación es donde los datos comienzan a crear valor para el consumo por parte de los usuarios posteriores.\\
Inmediatamente después de la ingesta, las transformaciones básicas mapean los datos a los tipos correctos (cambiando los datos de tipo cadena ingeridos a tipos numéricos y de fecha, por ejemplo), colocando los registros en formatos estándar y eliminando los incorrectos.\\
Las etapas posteriores de la transformación pueden transformar el esquema de datos y aplicar normalización. \\
En etapas posteriores, podemos aplicar agregaciones a gran escala para la elaboración de informes o la \textit{featurización} de datos para los procesos de aprendizaje automático (ML).\\

Consideraciones clave:
\begin{itemize}
    \item ¿Cuál es el costo y el retorno de la inversión (ROI) de la transformación? ¿Cuál es el valor comercial asociado?
    \item ¿Es la transformación tan simple y auto-aislada como sea posible?
    \item ¿Qué reglas de negocio las transformaciones soportan?
\end{itemize}
Se pueden transformar los datos en lotes o en \textit{streaming} en tiempo real.
Las transformaciones por lotes son abrumadoramente populares, pero dada la creciente popularidad de las soluciones de procesamiento en \textit{streaming} y el aumento general en la cantidad de datos en \textit{streaming}, se espera que la popularidad de las transformaciones en \textit{streaming} continúe creciendo.

La transformación a menudo está entrelazada con otras fases del ciclo de vida. Típicamente, los datos se transforman en los sistemas de origen o en tiempo real durante la ingesta. 
Por ejemplo, un sistema de origen puede agregar una marca de tiempo de evento a un registro antes de reenviarlo a un proceso de ingesta. O un registro dentro de un \texit{pipeline} de \textit{streaming} puede ser "enriquecido" con campos y cálculos adicionales antes de ser enviado a un data warehouse.

\textbf{Lógica de Negocio}
La lógica de negocio es un importante impulsor de la transformación de datos, a menudo en el modelado de datos. Los datos traducen la lógica de negocio en elementos reutilizables.

El \textit{featuring} de datos para ML es otro proceso de transformación de datos. Tiene como objetivo extraer y mejorar las características de los datos que son útiles para el entrenamiento de modelos de ML. Puede ser un arte oscuro, que combina el conocimiento del dominio (para identificar qué características podrían ser importantes para la predicción) con una amplia experiencia en ciencia de datos.\\
El punto principal es que una vez que los científicos de datos determinan cómo \textit{featurize} los datos, los procesos de \textit{featuring} pueden ser automatizados por los ingenieros de datos en la etapa de transformación de un \texit{pipeline} de datos.

Algunos de los elementos fundamentales de la etapa de transformación son: Preparación de datos, manipulación y limpieza de datos; consultas, modelado de datos.

Puede ocurrir en lotes o en stream.

\textbf{Transformaciones por Lotes (Batch)} se ejecutan en fragmentos discretos de datos, y pueden programarse a intervalos fijos (e.g., diario, por hora, o cada 15 minutos) para soportar informes, análisis y modelos de ML. \\
Un patrón de transformación extendido desde los inicios de las bases de datos relacionales es el ETL por lotes. El ETL tradicional se basa en un sistema de transformación externo para extraer, transformar y limpiar los datos, preparándolos para un esquema objetivo; como un almacén de datos, donde se pueden realizar análisis de negocio. La fase de extracción solía ser un cuello de botella importante, limitando la velocidad a la que se podían extraer los datos.\\
Una evolución popular del ETL es el ELT. A medida que los almacenes de datos han crecido en rendimiento y capacidad de almacenamiento, se ha vuelto común simplemente extraer los datos en bruto de un sistema fuente, importarlos al almacén de datos con una transformación mínima, y luego limpiarlos y transformarlos directamente en el sistema del almacén. Una segunda noción de ELT, ligeramente diferente, se popularizó con la aparición de los data lakes. En esta versión, los datos no se transforman al cargarlos. De hecho, se pueden cargar cantidades masivas de datos sin preparación ni plan alguno. La suposición es que el paso de transformación ocurrirá en un momento futuro indeterminado.\\
La data wrangling toma datos desordenados y mal formados y los convierte en datos útiles y limpios. Generalmente, este es un proceso de transformación por lotes.\\

\textbf{Transformaciones en Streaming} los datos se procesan continuamente a medida que llegan.\\
Change Data Capture (CDC): La captura de datos modificados (CDC) es un método para extraer cada evento de cambio (inserción, actualización, eliminación) que ocurre en una base de datos. La CDC se utiliza con frecuencia para replicar entre bases de datos casi en tiempo real o para crear un flujo de eventos para procesamientos posteriores.\\
El enfoque del fast-follower: Las bases de datos de producción generalmente no están equipadas para manejar cargas de trabajo de producción y ejecutar simultáneamente grandes escaneos analíticos sobre cantidades significativas de datos. Ejecutar tales consultas puede ralentizar la aplicación de producción o incluso provocar que se bloquee. Uno de los patrones de consulta de streaming más antiguos consiste simplemente en consultar la base de datos de análisis, recuperando resultados estadísticos y agregaciones con un ligero retraso con respecto a la base de datos de producción.\\

\subsection{Serving}
Ahora que los datos han sido ingeridos, almacenados y transformados en estructuras coherentes y útiles, es hora de obtener valor de los datos. "Obtener valor" de los datos significa diferentes cosas para diferentes usuarios.

\subsubsection{Analítica}
La analítica es el núcleo de la mayoría de los esfuerzos de datos. Una vez que sus datos están almacenados y transformados, está listo para generar informes o *dashboards* y realizar análisis *ad hoc* sobre los datos. Mientras que la mayor parte de la analítica solía abarcar la inteligencia empresarial (BI), ahora incluye otras facetas como la analítica operativa y la analítica integrada.

La Inteligencia Empresarial (BI) aprovecha los datos recopilados para comprender el rendimiento pasado y presente de una empresa.\\
La Analítica Operativa se centra en la información en tiempo real sobre las operaciones de negocio, impulsando la acción inmediata. Piense en vistas de inventario en vivo o *dashboards* en tiempo real que monitorean la salud del sitio web o de la aplicación. A diferencia de la BI, la analítica operativa enfatiza el presente, enfocándose menos en las tendencias históricas.\\
La Analítica Integrada (\texit{Embedded Analytics}), o analítica orientada al cliente, es la práctica de integrar capacidades analíticas directamente en un producto o plataforma de *software*.
Si bien aparentemente similar a la BI, la analítica integrada presenta desafíos únicos. Servir analítica a una gran base de clientes requiere tasas de solicitud significativamente mayores, lo que exige sistemas analíticos escalables y robustos. Lo más crítico es que el control de acceso se vuelve primordial. Cada cliente debe ver solo sus datos, y cualquier fuga de datos es una grave violación de la confianza con consecuencias potencialmente catastróficas.\\

\subsubsection{Machine Learning}
Algunas consideraciones para la fase de servicio de datos específicas para ML:
\begin{itemize}
    \item ¿Son los datos de calidad suficiente para realizar una ingeniería de features fiable? Los requisitos y las evaluaciones de calidad se desarrollan en estrecha colaboración con los equipos que consumen los datos.
    \item ¿Son los datos localizables? ¿Pueden los científicos de datos y los ingenieros de ML encontrar fácilmente datos valiosos?
    \item ¿Dónde están los límites técnicos y organizacionales entre la ingeniería de datos y la ingeniería de ML? Esta cuestión organizativa tiene importantes implicaciones arquitectónicas.
    \item ¿El conjunto de datos representa adecuadamente el \texit{ground truth}? ¿Está sesgado injustamente?
\end{itemize}

\subsubsection{ETL Inversa} % TODO research
El ETL inverso toma los datos procesados del lado de salida del ciclo de vida de la ingeniería de datos y los devuelve a los sistemas de origen. El ETL inverso nos permite tomar analíticas, modelos puntuados, etc., y devolverlos a los sistemas de producción o plataformas SaaS.

\subsection{Principales Corrientes Subyacentes en el Ciclo de Vida de la Ingeniería de Datos}

\subsubsection{Seguridad}
La seguridad debe ser una prioridad para los ingenieros de datos. Deben comprender tanto la seguridad de los datos como la seguridad del acceso, ejerciendo el principio de privilegio mínimo. El principio de privilegio mínimo significa dar a un usuario o sistema acceso solo a los datos y recursos esenciales para realizar una función prevista.

La seguridad de los datos también se trata de sincronización: proporcionar acceso a los datos exactamente a las personas y sistemas que necesitan acceder a ellos y solo durante el tiempo necesario para realizar su trabajo.

\subsubsection{Gestión de Datos}
Los ingenieros de datos gestionan el ciclo de vida de los datos, y la gestión de datos abarca el conjunto de mejores prácticas que los ingenieros de datos utilizarán para llevar a cabo esta tarea, tanto técnica como estratégicamente.

% La gestión de datos tiene bastantes facetas, incluyendo las siguientes:
% \begin{itemize}
%     \item Data governance, including discoverability and accountability
%     \item Modelado y diseño de datos
%     \item Data lineage
%     \item Almacenamiento y operaciones
%     \item Data integration and interoperability
%     \item Data lifecycle management
%     \item Data systems for advanced analytics and ML
%     \item Ethics and privacy
% \end{itemize}


\subsubsection{DataOps}
DataOps mapea las mejores prácticas de la metodología Agile, DevOps y el control estadístico de procesos (SPC) a los datos. Mientras que DevOps tiene como objetivo mejorar la publicación y la calidad de los productos de *software*, DataOps hace lo mismo para los productos de datos.

DataOps tiene tres elementos técnicos centrales: automatización, monitorización y observabilidad, y respuesta a incidentes.

La automatización de DataOps tiene un marco de trabajo y un flujo de trabajo similares a los de DevOps, que consisten en la gestión de cambios (control de versiones de entorno, código y datos), integración continua/despliegue continuo (CI/CD) y configuración como código.\\

\subsubsection{Data Architecture}
Una arquitectura de datos define los planos del sistema de datos de una organización. Es usualmente trabajo para el arquitecto de datos. 
Típicamente es un rol separado del ingeniero de datos, pero este debe implementar los diseños y proveer feedback

\subsubsection{Orquestación}
La orquestación es el proceso de coordinar muchos trabajos para que se ejecuten de la manera más rápida y eficiente posible en una cadencia programada. Un motor de orquestación construye metadatos sobre las dependencias de los trabajos, generalmente en la forma de un grafo acíclico dirigido (DAG). El DAG se puede ejecutar una vez o programarse para que se ejecute a un intervalo fijo de diariamente, semanalmente, cada hora, cada cinco minutos, etc.
% TODO research
\subsubsection{Ingeniería de Software}
Algunas áreas comunes de la ingeniería de software que se aplican al ciclo de vida de la ingeniería de datos.

\textbf{Código de Procesamiento de Datos Central (Core Data Processing Code)}: Escribir código eficiente y comprobable (Spark, SQL, etc.) para la ingesta, transformación y servicio de datos es fundamental para todas las etapas del ciclo de vida de la ingeniería de datos. Las metodologías de prueba adecuadas (pruebas unitarias, de regresión, de integración, de extremo a extremo y de humo) garantizan la calidad y confiabilidad de los datos.

\textbf{Desarrollo de Frameworks de Código Abierto (Open Source Frameworks)}: Los ingenieros de datos a menudo contribuyen y aprovechan las herramientas de código abierto, por lo que comprender su desarrollo y contribuir con mejoras es valioso.

\textbf{Streaming (Procesamiento en Flujo Continuo)}: El procesamiento de datos en tiempo real requiere habilidades y herramientas especializadas. Las tecnologías de streaming son cada vez más importantes en todas las etapas del ciclo de vida de la ingeniería de datos.

\textbf{Infraestructura como Código (IaC - Infrastructure as Code)}: La gestión de la infraestructura (servidores, bases de datos, etc.) a través de código garantiza implementaciones coherentes y repetibles, lo cual es fundamental para las prácticas de DataOps en todo el ciclo de vida de la ingeniería de datos.

\textbf{Pipelines como Código (Pipelines as Code)}: Definir flujos de trabajo de datos y dependencias mediante código permite la orquestación automatizada. Los ingenieros de datos usan código (típicamente Python) para declarar las tareas de datos y las dependencias entre ellas. El motor de orquestación interpreta estas instrucciones para ejecutar los pasos utilizando los recursos disponibles.

\textbf{Desarrollo de Código Personalizado}: Los ingenieros de datos a menudo necesitan escribir código personalizado para manejar escenarios específicos, integrar sistemas y resolver problemas más allá de las capacidades de las herramientas estándar. Son esenciales las sólidas habilidades de ingeniería de software (uso de API, transformación de datos, manejo de excepciones, etc).
\chapter{Conferencia 3}
\normalfont\LARGE \textbf{Tecnologías de la Ingeniería de Datos: Almacenamiento e Ingestión}
\normalfont\small\\
% Architecture is the high-level design, roadmap, and blueprint of data systems that satisfy the strategic aims for the business. Architecture is the what, why, and when. Tools are used to make the architecture a reality; tools are the how.
% Data engineers play a key role in embedding compliance measures into pipelines. These measures include encryption, data lineage, and automated checks, ensuring regulatory compliance at every step.
% Engineers must ensure that systems are scalable, maintainable, and cost-efficient while meeting both business and regulatory requirements.

Abordaremos conceptos generales y haremos un recorrido por las principales tecnologías y herramientas utilizadas en la industria.
El objetivo es que comprendan el panorama general, se familiaricen con los casos de uso clave y, sobre todo, identifiquen qué tecnologías merecen su atención.

\section{Almacenamiento}
\subsection{Tipos de almacenamiento de datos:}
\subsubsection{Bases de Datos Relacionales}
Una base de datos relacional es una colección de información que organiza los datos en tablas (o relaciones). Los datos se almacenan en filas y columnascon una clave única que identifica cada fila. 
Generalmente, cada tabla/relación representa un "tipo de entidad" (como cliente o producto). Las filas representan instancias de ese tipo de entidad (como "Lee" o "silla") y las columnas representan valores atribuidos a esa instancia (como dirección o precio).
Se representan relaciones como conexiones lógicas entre tablas, establecidas en función de sus interacciones.
Cada tabla tiene un esquema predefinido que fuerza a todos los elementos de la tabla a cumplirlos. Utilizan SQL, o variaciones de SQL como lenguaje estándar, que permite crear consultas complejas que abarquen varias tablas.\\

Escalan verticalmente, o sea, para mejorar el rendimiento hay que añadir más recursos (CPU/RAM) al servidor. El enfoque de escalado vertical aumenta la capacidad de una sola máquina incrementando los recursos en el mismo servidor lógico. Esto implica añadir recursos como memoria, almacenamiento y potencia de procesamiento al software existente, mejorando así su rendimiento.

Garantizan el cumplimiento del modelo relacional mediante el concepto de ACID.\\
En los sistemas de bases de datos, ACID (Atomicidad, Consistencia, Aislamiento(isolation), Durabilidad) se refiere a un conjunto estándar de propiedades que garantizan que las transacciones de la base de datos se procesen de forma fiable.
ACID se preocupa especialmente de cómo una base de datos se recupera de cualquier fallo que pueda ocurrir durante el procesamiento de una transacción. Un DBMS compatible con ACID asegura que los datos en la base de datos permanezcan precisos y consistentes a pesar de cualquier fallo de este tipo.

\textbf{Atomicidad} significa que se garantiza que o bien toda la transacción tiene éxito, o bien ninguna parte de ella lo tiene. No se obtiene una parte exitosa y otra que no lo es. Si una parte de la transacción falla, toda la transacción falla. Con la atomicidad, es "todo o nada".\\
\textbf{Consistencia} asegura que se garantiza que todos los datos serán consistentes. Todos los datos serán válidos de acuerdo con todas las reglas definidas, incluyendo cualquier restricción, cascada y disparador (trigger) que se haya aplicado en la base de datos.\\
\textbf{Aislamiento} garantiza que todas las transacciones ocurrirán de forma aislada. Ninguna transacción se verá afectada por ninguna otra transacción. Por lo tanto, una transacción no puede leer datos de ninguna otra transacción que aún no se haya completado.\\
\textbf{Durabilidad} significa que, una vez que una transacción se ha confirmado (committed), permanecerá en el sistema, incluso si hay una caída del sistema inmediatamente después de la transacción. Cualquier cambio de la transacción debe almacenarse permanentemente. Si el sistema le dice al usuario que la transacción ha tenido éxito, la transacción debe, de hecho, haber tenido éxito.\\

RDBMS (Relational Database Management System)=SGBD(R)

\begin{itemize}  % imagenes de los ejemplos de relacionales
    \item Oracle Database: Su primera versión data a 1979. Fue el primer RDBMS SQL disponible comercialmente.
    \item MySQL: MySQL se ofrece en dos ediciones diferentes: MySQL Community Server, de código abierto, y Enterprise Server, de propietario.  
Aunque comenzó como una alternativa de gama baja a bases de datos propietarias más potentes, ha evolucionado gradualmente para dar soporte a necesidades de mayor escala. Todavía se utiliza más comúnmente en implementaciones de un solo servidor, de pequeña a mediana escala. P o como un servidor de bases de datos independiente. Gran parte del atractivo de MySQL se origina en su relativa simplicidad y facilidad de uso, que se ve facilitada por un ecosistema de herramientas de código abierto.
En el rango medio, MySQL se puede escalar desplegándolo en hardware más potente, como un servidor multiprocesador con gigabytes de memoria. 
    \item SQLite: motor de base de datos relacional gratuito y de código abierto. No es una aplicación independiente; más bien, es una biblioteca que los desarrolladores de software incrustan en sus aplicaciones. SQLite fue diseñado para permitir que el programa se opere sin instalar un sistema de gestión de bases de datos o requerir un administrador de bases de datos. Viene instalado default en la mayoría de los sistemas operativos y navegadores (browsers). 
Debido al diseño server-less, las aplicaciones SQLite requieren menos configuración que las bases de datos cliente-servidor. SQLite se conoce como de "zero-configuration"("configuración cero") porque las tareas de configuración, como la gestión de servicios, los scripts de inicio y el control de acceso basado en contraseñas o GRANT, son innecesarias. 
SQLite almacena toda la base de datos, que consta de definiciones, tablas, índices y datos, como un único archivo multiplataforma, lo que permite que varios procesos o subprocesos accedan a la misma base de datos simultáneamente. 
    \item PostgreSQL: es uno de los sistemas de gestión de bases de datos objeto-relacionales de propósito general más avanzados y es de código abierto. Sus características más destacadas incluyen: Herencia de tablas, Replicación asíncrona, Capacidad definir nuevos tipos, así como soporte de muchos tipos que incluyen: 
Numéricos de precisión arbitraria, Arrays (de longitud variable y de cualquier tipo de datos hasta 1 GB de tamaño), Direcciones IPv4 e IPv6,  Identificador único universal (UUID), JSON y un JSONB binario más rápido.\\
Se pueden crear e instalar extensiones o plugins que funcionan como características integradas. A lo largo de los años, se ha desarrollado un rico ecosistema de extensiones que cubren desde la optimización del rendimiento hasta tipos de datos especializados. Ejemplos son:\\
PostGIS: introduce tipos de datos adicionales para la gestión geo-espacial.\\
pgcrypto: agrega funciones criptográficas para el cifrado, el hashing y más.\\
pgvector: Agrega soporte para operaciones vectoriales en Postgres, lo que permite la búsqueda de similitud, la búsqueda del vecino más cercano y más.\\
\end{itemize}

\subsubsection{Bases de Datos No Relacionale (NoSQL)} % imagenes de los ejemplos de NoSQL
NoSQL es un enfoque al diseño de bases de datos que se centra en proporcionar un mecanismo para el almacenamiento y la recuperación de datos que se modelan utilizando medios distintos a las relaciones tabulares empleadas en las bases de datos relacionales.\\
Las motivaciones para este enfoque incluyen la simplicidad del diseño, un escalado "horizontal" más sencillo a clústeres de máquinas, un control más preciso sobre la disponibilidad y la limitación del desajuste de impedancia objeto-relacional. Las estructuras de datos utilizadas por las bases de datos NoSQL permiten que algunas operaciones sean más rápidas y se consideran "más flexibles" que las tablas de las bases de datos relacionales.\\
Se clasifican en cuatro categorías principales:
\begin{itemize}
    \item Key-value stores(clave-valor): utilizan el array asociativo (también llamado mapa o diccionario) como su modelo de datos fundamental. En este modelo, los datos se representan como una colección de pares clave-valor, de tal manera que cada clave posible aparece como máximo una vez en la colección. Ejemplos: Redis, Couchbase, DynamoDB.
    \item Column-family stores(de columnas): Utiliza tablas, filas y columnas, pero a diferencia de una base de datos relacional, los nombres y el formato de las columnas pueden variar de fila a fila en la misma tabla. Un almacén de columnas anchas se puede interpretar como un store clave-valor bidimensional. Ejemplo: Cassandra.
    \item Document databases (de documentos): El concepto central de un almacén de documentos es el de un "documento". Las codificaciones en uso incluyen XML, YAML y JSON, y formas binarias como BSON. Se accede a los documentos en la base de datos a través de una clave única que representa ese documento. Las colecciones podrían considerarse análogas a las tablas y los documentos análogos a los registros(filas). Pero son diferentes: cada registro en una tabla tiene la misma secuencia de campos, mientras que los documentos en una colección pueden tener campos que son completamente diferentes. Ejemplo: MongoDB.
    \item Graph databases (de grafos): Las bases de datos de grafos están diseñadas para datos cuyas relaciones están bien representadas como un grafo, que consiste en elementos conectados por un número finito de relaciones. Ejemplos de datos incluyen relaciones sociales, enlaces de transporte público, mapas de carreteras, topologías de redes, etc. Ejemplo: Neo4j.
\end{itemize}

Actualmente, existen dos tipos principales de sistemas de procesamiento de datos críticos: el Procesamiento de Transacciones en Línea (OLTP) y el Procesamiento Analítico en Línea (OLAP). Cada uno tiene un propósito diferente y, técnicamente, deberían estar separados en cualquier organización.\\
OLTP es prácticamente lo que su nombre indica: una base de datos responsable del procesamiento de transacciones operativas. Debe ser rápido, dinámico y tener capacidad de respuesta ante fallos en la base de datos con un plan de respaldo.\\
OLAP es diferente: su propósito es guardar datos históricos y mantener los procesos de Extracción, Transformación y Carga (ETL) que se utilizan para el análisis de datos. El sistema OLAP es fundamental para las decisiones comerciales críticas, ya que ofrece datos relacionados con el rendimiento diario y la estabilidad organizacional a largo plazo.\\

\subsubsection{Data Warehouse} % imagenes de los ejemplos de warehouse
Los Data Warehouse son almacenes de datos son repositorios centrales de datos integrados, provenientes de fuentes diversas. Almacenan datos actuales e históricos organizados para optimizar el análisis de datos, la generación de informes y, sobre todo, la obtención de información clave a partir de la integración de datos.
Son bases de datos especializadas diseñadas para almacenar y analizar grandes volúmenes de datos estructurados y semiestructurados para fines de inteligencia empresarial (BI) y analítica.
\begin{itemize}
    \item PostgreSQL: Una característica excelente de PostgreSQL es su capacidad para ser utilizado tanto para OLTP como para OLAP. Esto facilita que las bases de datos que utilizan OLAP para almacenar los datos se comuniquen con las bases de datos que utilizan OLTP para crear los datos más recientes. Esta puede ser la razón principal por la que PostgreSQL es tan popular.
    \item Snowflake: Fundada en 2012, Snowflake es un Data Warehouse basado en la nube, conocido por su escalabilidad y flexibilidad. La arquitectura nativa de la nube de Snowflake aprovecha los beneficios de proveedores como AWS, Azure y Google Cloud. La plataforma ofrece funciones de seguridad integradas, incluyendo cifrado y control de acceso, lo que la hace adecuada para organizaciones con necesidades estrictas de seguridad y cumplimiento normativo. En general, Snowflake proporciona una solución robusta para almacenar, gestionar y analizar grandes conjuntos de datos en la nube.
    \item Amazon Redshift: Amazon Redshift es un servicio de almacenamiento de datos rápido y totalmente administrado en la nube, que permite a las empresas ejecutar consultas analíticas complejas sobre grandes volúmenes de datos, minimizando así los retrasos y garantizando un sólido apoyo para la toma de decisiones en todas las organizaciones. Fue lanzado en 2013, creado para solucionar los problemas asociados con el almacenamiento de datos tradicional en las instalaciones, como la escalabilidad, el costo y la complejidad. Redshift se integra perfectamente con Amazon S3, Amazon RDS, AWS Glue y mucho más para crear un ecosistema de datos.
    \item Google BigQuery: Google BigQuery es un Data Warehouse nativo de la nube. Es un almacén de datos en la nube sin servidor, altamente escalable y rentable que permite realizar consultas súper rápidas a escala de petabytes utilizando la potencia de procesamiento de la infraestructura de Google. Su arquitectura sin servidor le permite operar a escala y velocidad para proporcionar análisis SQL increíblemente rápidos sobre grandes conjuntos de datos.
\end{itemize}

\subsubsection{Data Lake}
Repositorios centralizados que almacenan grandes cantidades de datos sin procesar en su formato nativo, lo que permite a las organizaciones realizar análisis avanzados, aprendizaje automático y exploración de datos. 
Tecnologías como Apache Hadoop, Apache Spark y AWS Glue se utilizan comúnmente para construir y administrar lagos de datos.

\subsection{Tipos de almacenamiento de datos:} % imagen con los 3 tipos
\subsubsection{Almacenamiento en Archivos(File Storage)}
Dada la siguiente información como punto de partida, y manteniendo el estilo de escritura, profundiza en la explicación de los tipos de almacenamiento.
\subsubsection{Almacenamiento en Bloques(Block Storage)}
Divide los datos en bloques de tamaño fijo y los almacena sin metadatos adicionales. Cada bloque tiene una dirección única y puede ser gestionado independientemente, lo que lo hace ideal para bases de datos y aplicaciones de alto rendimiento que requieren baja latencia.

Características clave:

Bajo nivel: Los bloques son manejados directamente por el sistema de almacenamiento.

Alto rendimiento: Ideal para bases de datos (ej. Oracle, SQL Server) y máquinas virtuales.

Flexibilidad: Permite configuraciones avanzadas como RAID y snapshots.

\subsubsection{Almacenamiento de Objetos(Object Storage)}
El almacenamiento de objetos es una arquitectura de almacenamiento de datos en la que los datos se almacenan y gestionan como unidades autocontenidas llamadas objetos. Cada objeto contiene una clave, datos y metadatos opcionales. Dado que el almacenamiento de archivos y bloques utiliza jerarquías, el acceso a los datos se ralentiza a medida que los almacenes de datos crecen desde gigabytes y terabytes hasta petabytes e incluso más.\\
Cada objeto se almacena con sus metadatos, que pueden ser bastante detallados. Pueden incluir información como políticas específicas de privacidad y seguridad, reglas de acceso e incluso especificaciones, por ejemplo, sobre dónde se grabó un videoclip o quién creó los datos.\\    
Cada unidad tiene un identificador único o clave, que permite encontrarlas sin importar dónde estén almacenadas en un sistema distribuido. Los objetos se almacenan en un único pool sin una jerarquía de carpetas o directorios.\\

Ejemplos:
\begin{itemize}
    \item Amazon S3
    \item Azure Blob
    \item Google Cloud Storage
\end{itemize}

Los sistemas de archivos distribuidos como Hadoop Distributed File System (HDFS) y Google File System (GFS) proporcionan soluciones de almacenamiento escalables y tolerantes a fallos para entornos de computación distribuida. Están optimizados para almacenar y procesar grandes conjuntos de datos a través de múltiples nodos en un clúster de computación distribuida, soportando el procesamiento de datos en paralelo y la tolerancia a fallos.

\section{Ingesta}
La ingesta de datos es el proceso de mover datos (especialmente datos no estructurados) desde una o más fuentes a un sistema de almacenamiento para su posterior procesamiento y análisis.\\
Es la primera etapa en un pipeline de ingeniería de datos donde los datos provenientes de varias fuentes comienzan su recorrido.

\subsection{APIs}
Una de las formas más frecuentes de ingestión de datos es a través de Application Programming Interface. 
En general, se accede a las APIs a través de la web utilizando una URL. Dentro de la dirección web, se especifica la información que se desea. Para saber cómo formatear la dirección web, es necesario leer la documentación de la API. Algunas APIs también requieren que envíes credenciales de inicio de sesión como parte de tu solicitud.
La biblioteca 'requests' de Python hace que trabajar con APIs sea relativamente sencillo.

A menudo se considera que las ingestas de datos mediante APIs son más rápidas y escalables en ciertas situaciones, y pueden soportar cambios variables en los atributos. Esta funcionalidad puede funcionar con procesamiento en tiempo real o por lotes, lo que la convierte en una herramienta valiosa para muchas industrias diferentes.

Postman 

Herramientas de ingesta de datos
\begin{itemize}
    \item Airbyte: Es una herramienta de ingesta de datos de código abierto que se centra en la extracción y carga de datos, desarrollada para le proceso de ETL. Facilita la configuración de pipelines y mantiene un flujo seguro a lo largo de todo el pipeline. Puede proporcionar acceso tanto a datos brutos como normalizados e integra más de 120 conectores de datos.
    \item Amazon Kinesis: Ayuda en la ingesta de datos en tiempo real de diversas fuentes, y a procesar y analizar los datos a medida que llegan. Además, ofrece diferentes capacidades, como Kinesis Data Streams, Kinesis Data Firehose y más, para la ingesta de datos en streaming a cualquier escala de forma rentable.
    \item Apache Kafka: Es una plataforma de streaming de eventos distribuida de código abierto. El streaming de eventos captura datos en tiempo real de fuentes de eventos como dispositivos móviles, sensores, bases de datos, servicios en la nube y aplicaciones de software. Kafka almacena los datos de forma duradera para su posterior recuperación para procesamiento y análisis. Sus otras capacidades principales incluyen alto rendimiento, escalabilidad y alta disponibilidad.
    \item Apache NiFi:  Herramienta visual para automatizar los flujos de datos entre sistemas.
\end{itemize}

\chapter{Conferencia 4}
\normalfont\LARGE \textbf{Tecnologías de la Ingeniería de Datos: Transformación y Servicio}
\normalfont\small\\

\section{Transformación}
% Descripción de transformación de datos
La transformación de datos es el proceso de convertir, limpiar y estructurar los datos 
brutos en un formato adecuado para análisis y modelado. Esta etapa es crucial en los 
pipelines de datos ya que mejora la calidad, consistencia y utilidad de los datos. Preparándolos 
para su consumo en aplicaciones de business intelligence y machine learning.

Resaltar que ninguna simplemente se modifican los datos originales, sino que se crea un nuevo espacio para contener los datos "limpios y procesados".

\subsection{Ejemplos de operaciones de transformación de datos:}

\subsubsection{Limpieza}
La limpieza de datos es el proceso de detectar y rectificar errores e inconsistencias en 
los datos. Es parte esencial del procesamiento de los datos, y casi en el 100\% de los casos 
es necesario, a diferencia de otras operaciones que veremos a continuación. 
% Para visualizar los datos y detectar los errores

\textbf{Ejemplos:} 
\begin{itemize}
    \item Remover datos duplicados: Registros idénticos (por fallas en la generación o ingestión)
    \begin{verbatim}
    ANTES:                          DESPUÉS:
    ID  Producto  Valor             ID  Producto  Valor
    1   Laptop    $1000             1   Laptop    $1000
    1   Laptop    $1000        →    2   Mouse     $20
    2   Mouse     $20
    \end{verbatim}

    \item Detectar valores atípicos: Ejemplo en temperaturas (℃):
    \begin{verbatim}
    Datos: [22, 23, 21, 25, 19, 28, 31, -273, 24]
    Outlier: -273 (posiblemente error de sensor) 
    \end{verbatim} % → Se puede imputar con la mediana (23)
    
    \item Valores inválidos para el tipo: Ejemplo: Fechas con formato incorrecto o fuera de rango lógico.
    \begin{verbatim}
    ANTES:                             DESPUÉS:
Paciente  Fecha_Nacimiento         Paciente  Fecha_Nacimiento
A         1990-05-15               A         1990-05-15
B         2025-13-02          →    B         NaN (fecha futura/mes inválido)
C         15/07/1985               C         1985-07-15 (formato apropiado)
    \end{verbatim}
    
    \item Atributo con valores de tipo incorrecto: Símbolos mezclados en campos numéricos.
    \begin{verbatim}
    ANTES:                          DESPUÉS:
    Producto  Precio                Producto  Precio
    X         $120.50               X         120.50
    Y         95,99€         →      Y         95.99 
    Z         "1,200"               Z         1200.00
    \end{verbatim}
\end{itemize}
Algunas herramientas que ayudan a la visualización y limpieza de datos incluyen pandas(Python), 

\subsubsection{Normalización}
Proceso de escalar valores numéricos a un rango común (generalmente [0,1]) sin distorsionar las diferencias. Es 
relevante cuando los datos serán utilizados por algún algoritmo sensible a la escala; así todos los features tienen 
la misma escala y no se induce un sesgo en ese sentido (KNN, Redes Neuronales).\\

Min-Max Normalization: transforms the original data linearly.
$$ X_{\text{norm}} = \frac{X - \min(X)}{\max(X) - \min(X)} $$

\textbf{Ejemplos:} 

Unificación de lecturas de sensores con diferentes rangos:

    \begin{verbatim}
        ANTES (Raw values):               DESPUÉS (Normalizado):
        Sensor_A: [200, 400, 600]         Sensor_A: [0.0, 0.5, 1.0]  
        Sensor_B: [10, 20, 30]      →     Sensor_B: [0.0, 0.5, 1.0]
        (Rango A: 200-600)                (Ambos en [0,1])
        (Rango B: 10-30)
    \end{verbatim}

La biblioteca Scikit-learn proveé herramientas para el escalado/normalización automático.

\subsubsection{Estandarización}
Transformación similiar a la Normalización, que ajusta los datos para tener media 0 y desviación estándar 1 (Z-score).\\
Z-Score Normalization: zero-mean normalization.
$$ Z = \frac{X - \mu}{\sigma} $$

\textbf{Ejemplos:} 

Comparar puntuaciones en distintas escalas:
    \begin{verbatim}
        ANTES (Puntajes crudos):         DESPUÉS (Z-scores):
        FICO: [650, 720, 580]           FICO: [0.23, 1.12, -1.35]
        Vantage: [550, 600, 450]   →    Vantage: [0.23, 1.12, -1.35]
        (mu=650, sigma=62.5)            (Comparables directamente)
        (mu=550, sigma=44.7)
    \end{verbatim} 

\textbf{Herramientas:} Scikit-learn (StandardScaler), TensorFlow Transform.

\subsubsection{Encoding}
Conversión de datos categóricos a formato numérico para procesamiento algorítmico.\\
Los datos pueden ser binarios excluyentes, ordinales o nominales (sin orden).\\
Los datos binarios pueden pasar por un encoding que los transforma en 0 y 1.\\
Para los datos ordinales usualmente se utiliza Label Encoding o Ordinal Encoding. 
Sin embargo los datos categóricos nominales, no se puede asumir que tienen algún orden y forzar uno
inevitablemente introducirá un sesgo. En ese caso se sugiere utilizar One-Hot Encoding; que consiste 
en poner cada valor posible como una columna y asignar 0 o 1 indicando si el registro tenía esa clasificación.\\
Otra forma de codificar es Frequency Encoding. Que transforma el valor que era string en una fracción que indica 
la frecuencia de la categoría en la columna.
Hay otros encoding techniques como Binary Encoding, Hash Encoding, Mean/Target Encoding, 

Las herramientas usuales incluyen Pandas, Scikit-learn

One-Hot Encoding: Para categorías sin orden. Ejemplo colores:
    \begin{verbatim}
        ANTES:           DESPUÉS (One-Hot):
        Color            Rojo  Verde  Azul
        Rojo     →       1     0      0
        Verde            0     1      0
        Azul             0     0      1
    \end{verbatim}
Label Encoding: Para categorías ordinales. Ejemplo tallas:
    \begin{verbatim}
        ANTES:           DESPUÉS:
        Talla            Talla_Encoded
        S        →       0
        M                1
        L                2
    \end{verbatim}

\subsubsection{Discretizar}
Transformación de variables continuas en intervalos discretos.\\
Esto puede ser relevante al trabajar con tiempo, temperatura, mediciones, etc.
Algunos de los métodos comunes son:
\begin{itemize}
    \item Equal Width Binning: divide el rango en $n$ intervalos de igual tamaño.
    \item Equal Frequency Binning: divide los datos de forma que cada intervalo tenga la misma cantidad de data points.
    \item Clusters: Dividir en clusters que implican similitud con algoritmos como K-Means. Todos los datos de un cluster se tratan como una única categoría.
    \item etc
\end{itemize}

\textbf{Ejemplo:} 
Agrupar edades para segmentación demográfica:
    \begin{verbatim}
        ANTES (Edad):        DESPUÉS (Bins):
        [15,                 ["0-18",
         25,            →     "19-35",
         70,                  "60+",
         32]                  "19-35"]
    \end{verbatim}

\textbf{Herramientas:} Pandas (cut/qcut), KNIME, RapidMiner.

\subsubsection{Generación de nuevos atributos}
Creación de características derivadas mediante operaciones matemáticas o lógicas. \\
\textbf{Ejemplo:} Calcular el Índice de Masa Corporal (IMC) a partir de peso y altura.
Derivar la edad de la fecha de nacimiento.\\

\subsubsection{Imputación}
Técnicas para manejar valores faltantes mediante estimación o sustitución. Eso incluye técnicas 
estadísticas como remplazar por la media, mediana o moda; algoritmos de ML de regresión, KNN, u 
otros más específicos; utilizar distribuciones probabilísticas; entre muchos otros que dependen 
del tipo de datos y los casos de uso finales.\\

\textbf{Ejemplo:} Rellenar valores nulos en ingresos con la mediana del grupo demográfico.\\

\textbf{Herramientas:} Scikit-learn (SimpleImputer), DataWrangler, IBM SPSS.

\subsubsection{Procesamiento de texto}
Transformación de texto no estructurado a formato analizable. Puede incluir procesos como Tokenización, lematización y vectorización ya sea con TF-IDF o con embeddings.\\

\textbf{Ejemplo:} Tokenización, lematización y vectorización (TF-IDF) de reseñas de clientes.\\

\textbf{Herramientas:} NLTK, SpaCy, Transformers, TensorFlow Text.
% tokenizar, stemming, lemma

% ETL, ELT, and data pipelines

\subsection{ETL y ELT en el Procesamiento de Datos}
\textbf{ETL (Extract-Transform-Load):} Nace en los 70s con sistemas mainframe y consolida 
su modelo en los 90s con data warehouses. Diseñado para hardware limitado: transformación 
\textit{antes} de cargar para reducir la carga en los sistemas destino. \\
\textbf{ELT (Extract-Load-Transform):} Emerge en 2010s con Big Data y tecnologías 
cloud (Snowflake, BigQuery, Redshift). Habilitado por escalamiento elástico y procesamiento 
distribuido (Spark, columnar storage). Respuesta a necesidades de data lakes y análisis 
exploratorio con datos crudos.

\subsubsection{ETL: Transformación en Tránsito}
Transformación en \textbf{staging area} externa (servidores ETL). Caso de uso: Datos 
estructurados con reglas de negocio complejas (ej. consolidación financiera)

\textbf{Usar ETL cuando:} Requieres validación estricta pre-carga (ej. datos regulados: HIPAA, PCI-DSS). 
Los sistemas destino tienen capacidad computacional limitada. Existen transformaciones complejas independientes 
del almacenamiento.

\subsubsection{ELT: Transformación en Destino}
Transformación dentro del \textbf{sistema destino} (data warehouse/lake). Caso de uso: Análisis 
ad-hoc sobre datos semi-estructurados (ej. logs de apps móviles).

\textbf{Usar ELT cuando:} 
Necesitas retener datos crudos (ej. auditorías forenses). Trabajas con datos no 
estructurados/semi-estructurados. Requieres escalamiento horizontal (Big Data > 1TB).

% Views
\subsection{Views}
Las vistas son representaciones virtuales de datos que se  definen mediante consultas 
SQL almacenadas. No almacenan datos físicamente. Proporcionan una capa de abstracción 
sobre tablas base. Y no necesitan actualizarse ante nuevos registros.

Usar vistas para: Capas de presentación (BI tools), Transformaciones ligeras (filtros, 
joins simples), Control de acceso granular. 

This stored query can target both tables and other views (you can create a view that queries other views). This stored query (view definition) represents part of the database, but it doesn't store any physical data! This is the first important difference to a “regular” table – views don't store the data, which means that every time you need the data from the view, the underlying stored query will be executed against the database. Since views are being run each time you “call” them, they will always pick the relevant data from the underlying tables. That means you don't need to worry if something changed in the underlying table (deleted/updated rows), as you will always get the actual data from the tables.

%  dbt (data build tool)
\subsection{dbt (data build tool)}
Es un framework open-source que permite ejecutar transformaciones de datos \textbf{dentro 
del data warehouse} mediante SQL, gestionar pipelines como código versionado (Git), automatizar 
testing, documentación y despliegues.\\
Promueve el principio de "Analytics engineering as software development".\\
dbt es la T en ELT. No extrae o carga datos, sino que transforma los datos en el Data Warehouse 
para ser consumidos por herramientas de BI.\\
dbt transforma mediante views/materialized views\\

Tiene integraciones nativas con:
\begin{itemize}
    \item Warehouses: Snowflake, BigQuery, Redshift, Databricks
    \item Ingestion: Airbyte, Fivetran, Meltano
    \item BI: Metabase, Mode, Preset
\end{itemize}

The utility of dbt that makes it superior to other approaches originates from the combination of Jinja templates with SQL, and re-usable components or models.

Se puede utilizar dbt combinado con spark
% Spark 
\subsection{Spark(https://spark.apache.org)}
% https://medium.com/@atnofordatascience/data-engineering-with-apache-spark-a-beginners-guide-d3190be2b6a4
% https://medium.com/@DataEngineeer/introduction-to-apache-spark-for-data-engineering-d2060166165a
Se ha convertido en uno de los frameworks de procesamiento de grandes volúmenes de 
datos más destacados de la industria. Spark está diseñado para trabajar con una amplia 
gama de fuentes de datos, como HDFS, Apache Cassandra, Apache HBase y Amazon S3.

Unifica el procesamiento de tus datos en lotes y en tiempo real (streaming), utilizando 
tu lenguaje preferido: Python, SQL, Scala, Java o R.

Realiza Análisis Exploratorio de Datos (EDA) en datos a escala de petabytes sin 
tener que recurrir al submuestreo(downsampling) gracias a su escalamiento 
horizontal. Se ejecuta más rápido que la mayoría de los almacenes de datos.
Tiene MLlib, biblioteca integrada de machine learning para tareas como 
clasificación, regresión y clustering.

- Pyspark

\section{Serving}
% Descripción de serving de datos
Fase final del pipeline donde los datos transformados se ponen a disposición de los usuarios 
finales, ya sea BI, dashboards, APIs, microservicios, ML o sistemas externos. Idealmente el 
servicio es con acceso seguro, baja latencia y formatos consumibles.

\subsection{Desplegar un API}
Esto permite entregar datos a clientes en varias modalidades: web, móvil, IoT, otro servicio de 
analítica. Permite consultar datos en formato streaming en tiempo real. Además de permitir un control 
preciso y granular sobre qué datos exponer, a cada tipo de usuario.

\textbf{Retos comunes:}\\
Si el volumen de datos(+100) es grande es necesario implementar paginación, straming, etc. \\
Establecer protocolos de seguridad con JWT, OAuth2, Rate limiting.

\subsubsection{Frameworks Principales}
\begin{itemize}
    \item \textbf{FastAPI}: Async, auto-documentación OpenAPI
        Ejemplo endpoint:
        \begin{verbatim}
@app.get("/sales/{region}")
async def get_sales(region: str, 
                   start: date, 
                   end: date):
    return db.query(Sales)
        .filter(Sales.region == region)
        .filter(Sales.date.between(start, end))
        \end{verbatim}
    
    \item \textbf{Flask-RESTful}: Ligero, ideal para prototipos, Extensible con Flask-SQLAlchemy
    
    \item \textbf{Django REST Framework}: ORM integrado, autenticación robusta, Ideal para CRUD complejos

\end{itemize}

\subsubsection{Librerías Complementarias}
\begin{itemize}
    \item \textbf{Pydantic}: Validación y serialización de modelos.
    \item \textbf{Celery}: Procesamiento asíncrono de requests pesados.
    \item \textbf{Authlib}: Implementación OAuth2/JWT.
    \item \textbf{Redis}: Caché de respuestas frecuentes.
\end{itemize}

\subsubsection{Patrones de Diseño}
\begin{itemize}
    \item \textbf{Modelo de Paginación}:
    \begin{verbatim}
{
  "data": [...],
  "pagination": {
    "total": 1000,
    "page": 2,
    "per_page": 50,
    "next": "/api/v1/sales?page=3"
  }
}
    \end{verbatim}
    
    \item \textbf{Versionado Semántico}:
    \begin{itemize}
        \item \texttt{/api/v1/sales}
        \item \texttt{/api/v2/sales}
    \end{itemize}
    
    \item \textbf{Circuit Breaker}: Usando Tenacity
    \begin{verbatim}
@retry(stop=stop_after_attempt(3), 
      wait=wait_exponential(multiplier=1))
def fetch_external_data():
    ...
    \end{verbatim}
\end{itemize}

\textbf{Buenas prácticas:} Documentación Automática, Monitoreo, Pruebas automáticas, Logging.

\subsection{Herramientas de Análisis y Visualización}

\subsubsection{Tableau}
Plataforma líder de visualización empresarial con capacidades drag-and-drop y soporte para big data.
Tableau es una herramienta de Inteligencia de Negocios (BI) para analizar datos visualmente. Los usuarios 
pueden crear y distribuir dashboards interactivos y compartibles que representan tendencias, 
variaciones y densidad en los datos en forma de gráficos y diagramas. Tableau puede conectarse a 
archivos, fuentes relacionales y de Big Data para adquirir y procesar datos.

Soporta, con Tableau Embedding API, añadir (embed) visualizaciones a una página web.

% https://www.ask.com/news/comprehensive-guide-using-tableau-free-version-data-visualization#:~:text=Known%20for%20its%20intuitive%20interface%20and%20powerful%20features%2C,explore%20and%20visualize%20their%20data%20without%20any%20cost.

\subsubsection{Power BI} (free tier on desktop)
Microsoft Power BI es una plataforma de visualización de datos e informes. Un dashboard de Power BI permite reportar y visualizar 
datos en una amplia gama de estilos, incluyendo gráficos, mapas, diagramas, diagramas de dispersión y más.
Allows to also create highly customizable visualizations with R and Python. 
Gran soporte para mapas.

Power BI en sí está compuesto por varias aplicaciones interrelacionadas: Power BI Desktop, Power BI Pro, Power BI Premium, Power BI Mobile,\\
Power BI Embedded: Power BI White-label para proporcionar paneles y analíticas orientadas al cliente en tus propias aplicaciones.\\
Power BI Report Server: Solución local incluida en Premium; puede ser migrada a la nube.\\

\subsubsection{Looker}
Looker es una plataforma de computación en la nube que proporciona inteligencia empresarial (BI) y análisis de datos.
Integración de datos: Looker tiene la capacidad de establecer conexiones con varias fuentes de datos basadas en la nube, como los data warehouse Snowflake, Amazon Redshift y Google BigQuery. Debido a esto, los usuarios pueden examinar los datos almacenados en estas plataformas sin tener que copiarlos o reubicarlos.
Visualización: Looker ofrece una variedad de opciones de visualización para transmitir eficazmente información valiosa de los datos. Para comunicar mediciones, tendencias y patrones, los usuarios pueden crear paneles, gráficos y diagramas.

\subsubsection{Metabase}
Metabase es una plataforma de inteligencia empresarial de código abierto. Puede usar Metabase para hacer preguntas sobre sus datos, o incrustar Metabase en su aplicación para permitir que sus clientes exploren sus propios datos.

\chapter{Conferencia 5}
\normalfont\LARGE \textbf{Tecnologías de la Ingeniería de Datos: Tecnologías complementarias}
\normalfont\small\\

\section{Gestión de Datos}

Las herramientas de data governance son soluciones de software diseñadas para ayudar a las empresas a gestionar sus datos de manera más efectiva. Estas herramientas respaldan procesos que aseguran que los datos sean precisos, seguros y estén en línea con los requisitos regulatorios. Pueden incluir capacidades tales como:

\begin{itemize}
    \item \textbf{Catalogación de datos:} el proceso de crear un inventario exhaustivo de sus activos de datos. Una herramienta de gobierno de datos con sólidas capacidades de catalogación facilita la búsqueda, comprensión y uso de datos en toda su organización.
    \item \textbf{Seguimiento del linaje de datos:} comprender de dónde provienen sus datos, cómo se transforman y a dónde van es crucial. Las funciones de linaje de datos le permiten rastrear el flujo de datos a través de los sistemas, lo que garantiza la transparencia y la confianza.
    \item \textbf{Gestión del cumplimiento normativo:} muchas industrias están sujetas a regulaciones estrictas como GDPR, HIPAA o CCPA. Las herramientas de gobierno de datos que ofrecen gestión del cumplimiento normativo pueden ayudar a su organización a mantener y demostrar el cumplimiento de estas regulaciones.
    \item \textbf{Control de calidad de los datos:} las herramientas de gobierno de datos deben tener controles de calidad de datos integrados que garanticen la precisión, integridad y consistencia de sus datos. Estas herramientas deben proporcionar alertas automatizadas cuando surjan problemas de calidad de los datos.
    \item \textbf{Controles de acceso basados en roles:} la seguridad de los datos es un componente clave del gobierno. Asegúrese de que su herramienta admita controles de acceso basados en roles para que solo el personal autorizado tenga acceso a datos confidenciales.
\end{itemize}

% Data cataloging: the process of creating a comprehensive inventory of your data assets. A data governance tool with strong cataloging capabilities makes it easier to find, understand, and use data across your organization.

% Data lineage tracking: Understanding where your data comes from, how it's transformed, and where it goes is crucial. Data lineage features allow you to trace the flow of data across systems, ensuring transparency and trust.

% Compliance management: Many industries are subject to strict regulations such as GDPR, HIPAA, or CCPA. Data governance tools that offer compliance management can help your organization maintain and demonstrate adherence to these regulations.

% Data quality control: Data governance tools should have built-in data quality controls that ensure the accuracy, completeness, and consistency of your data. These tools should provide automated alerts when data quality issues arise.

% Role-based access controls:  Data security is a key component of governance. Ensure that your tool supports role-based access controls so that only authorized personnel have access to sensitive data.
% Policy enforcement

% https://rootstack.com/es/blog/dataops-implementacion-herramientas

\section{Orquestación de Datos}
En el ámbito de la ingeniería de datos, el proceso de transformar datos brutos en información valiosa implica la utilización de pipelines y flujos de trabajo de datos. Estos conceptos forman la columna vertebral de la gestión eficiente de datos y permiten a los ingenieros de datos manejar tareas complejas de procesamiento de datos sin problemas.\\
Los pipelines de datos sirven como el tejido conectivo dentro del ecosistema de la ingeniería de datos. Son responsables de orquestar el flujo de datos desde su origen hasta su destino, manejando varias etapas del procesamiento de datos a lo largo del camino. Los ingenieros de datos diseñan e implementan pipelines para asegurar el movimiento fluido de los datos, incluyendo tareas como la ingestión, integración, transformación y carga de datos.\\

\textbf{Apache Airflow:} Es una herramienta de código abierto ampliamente utilizada para la creación, programación y monitoreo de flujos de trabajo complejos. Airflow permite a los equipos de DataOps diseñar pipelines de datos de manera flexible y modular, integrando distintas fuentes y destinos de datos. Es ideal para la automatización de tareas y facilita la colaboración entre equipos.

Airflow proporciona un framework de Python flexible y escalable que permite a los ingenieros de datos crear, programar y monitorizar sus pipelines de datos.\\
Un DAG representa un conjunto de tareas y sus dependencias, donde la dirección de las flechas indica el flujo de datos o ejecución.\\
La parte "Dirigido" significa que cada tarea tiene una relación definida con una o más tareas, formando una cadena de dependencias. Esto asegura que las tareas se ejecuten en el orden correcto, basándose en sus dependencias.\\
La parte "Acíclico" significa que no hay bucles o ciclos en el grafo. En otras palabras, las tareas no pueden tener dependencias circulares que conducirían a bucles infinitos. Esto asegura que los flujos de trabajo puedan completarse exitosamente sin bucles infinitos o dependencias no resueltas.\\

\textbf{Prefect:} Similar a Airflow, permite la orquestación y automatización de flujos de trabajo de datos; así como el monitoreo de pipelines con una interfaz intuitiva para equipos de datos. 

Prefect is an open-source orchestration tool for data engineering. It is a Python-based tool that allows you to define, schedule, and monitor your data pipelines. Prefect is a great tool for data engineers and data scientists who want to automate their data pipelines.\\

\section{DataOps}

La creciente complejidad de los datos en las organizaciones modernas ha dado lugar a la necesidad de prácticas más ágiles y eficientes. DataOps surge como una respuesta para mejorar la gestión de datos, optimizar flujos de trabajo y fomentar una mayor colaboración entre equipos de TI y analistas de datos. 
DataOps es una metodología que combina los principios de DevOps con la gestión de datos. Su objetivo principal es optimizar el ciclo de vida de los datos, desde la integración y almacenamiento hasta su análisis y entrega, asegurando la calidad y confiabilidad. La adopción de herramientas adecuadas es crucial para implementar DataOps con éxito y maximizar los beneficios de esta metodología.

\textbf{Talend:} Plataforma para integración, transformación y migración de datos en tiempo real, con enfoque en automatización y calidad de datos para mantener altos estándares y cumplimiento normativo.

\subsection{Containerization}

La contenerización es una práctica de desarrollo de software que empaqueta una aplicación y sus dependencias en un único contenedor. El contenedor está aislado del sistema host, permitiendo que la aplicación se ejecute en cualquier infraestructura que soporte contenedores, independientemente del sistema operativo subyacente.\\
Inspirados por la idea de las máquinas virtuales, los contenedores proporcionan entornos ligeros y aislados que empaquetan las aplicaciones con sus dependencias. Esta innovación asegura consistencia, portabilidad y eficiencia. Con el tiempo, herramientas como Docker y Kubernetes se han convertido en estándares de la industria para la gestión de aplicaciones contenerizadas, permitiendo una escalabilidad y orquestación sin problemas.\\
Los contenedores pueden moverse fácilmente de un host a otro, lo que facilita la migración de una solución de ingeniería de datos a una infraestructura diferente si es necesario.\\
Los contenedores proporcionan un entorno consistente para que las aplicaciones se ejecuten, asegurando que la solución de ingeniería de datos se comporte de la misma manera independientemente del entorno en el que se despliegue.\\

\textbf{Docker:} Plataforma de código abierto que permite la automatización del despliegue, escalado y gestión de aplicaciones en contenedores. Kubernetes facilita la creación de entornos escalables y resilientes para gestionar grandes volúmenes de datos y flujos de trabajo. Con Kubernetes, los equipos pueden orquestar los recursos necesarios de manera eficiente, garantizando que las aplicaciones de datos operen de manera fluida.

\textbf{Kubernetes:} Plataforma para automatizar despliegue, escalado y gestión de aplicaciones en contenedores, usada para crear entornos escalables y resilientes en proyectos de ingeniería de datos y DataOps

\chapter{Conferencia 6}
\normalfont\LARGE \textbf{Integración de Sistemas en la Ingeniería de Datos}
\normalfont\small\\

\section{System Integration}

% v3: https://www.altexsoft.com/blog/system-integration/

Definición:\\
La Integración de Sistemas es el proceso de ingeniería que consiste en integrar hardware, 
software, redes y fuentes de datos dispares en un sistema unificado y cohesivo que opera 
como una sola entidad funcional. Esto implica establecer canales de comunicación y protocolos 
de intercambio de datos entre sistemas previamente aislados. El objetivo es lograr que estos
sistemas trabajen juntos sin problemas para que puedan compartir información y procesos de 
manera más eficiente. La integración de sistemas se centra típicamente en mejorar la 
funcionalidad, el flujo de datos y la eficiencia operativa.

Con los años, la integración de sistemas ha evolucionado. Si bien antes se trataba principalmente 
de conectar servidores on-premise (locales), la integración actual a menudo se realiza en la nube. 
Ya sea a través de APIs o buses de servicio empresarial, la integración moderna ayuda a que diferentes 
sistemas y herramientas trabajen juntos de manera eficiente en un entorno basado en la nube.

\subsection{Tipos de Integración de Sistemas y Ejemplos}
\subsubsection{Integración de Datos (Data Integration)}
La integración de datos es un tipo de integración de sistemas enfocada en consolidar información de diversas fuentes, como plataformas, servicios y bases de datos, en una visión unificada.

\textbf{Ejemplo:} Starbucks utiliza la integración de datos para combinar información de su aplicación móvil, sistemas de punto de venta (POS) y comentarios de clientes. Esta integración permite promociones personalizadas, seguimiento de inventario en tiempo real y una mejor experiencia del cliente mediante análisis integrales.

\subsubsection{Integración de Aplicaciones (Application Integration)}
La integración de aplicaciones se refiere al proceso de conectar diferentes aplicaciones empresariales para que funcionen de manera eficiente. Esto es crucial para optimizar flujos de trabajo y procesos de negocio, permitiendo que los datos se muevan entre aplicaciones automáticamente sin intervención manual.

\textbf{Ejemplo:} Integrar herramientas como Slack y Jira para que las actualizaciones de proyectos se sincronicen en tiempo real entre plataformas de comunicación y gestión de proyectos.

\subsubsection{Integración de Sistemas Legacy (Legacy System Integration)}
Este tipo de integración conecta sistemas antiguos (legacy) con tecnologías modernas, permitiendo seguir utilizando datos existentes junto con herramientas nuevas.

\textbf{Ejemplo:} Un hospital que conecta un antiguo sistema de gestión de pacientes con un nuevo sistema de Registro Electrónico de Salud (EHR, por sus siglas en inglés). Esto permite a los médicos acceder a toda la información del paciente desde ambos sistemas sin cambiar su rutina.

\subsubsection{Integración de Aplicaciones Empresariales (Enterprise Application Integration, EAI)}
La EAI unifica diferentes aplicaciones de software dentro de una empresa para que funcionen de manera cohesionada. Conecta diversas aplicaciones, permitiéndoles compartir e intercambiar datos en tiempo real. Resuelve el problema de aplicaciones empresariales dispares que almacenan datos por separado, integrando todo en un sistema unificado.

\textbf{Ejemplo:} Una empresa utiliza sistemas separados para ventas, servicio al cliente e inventario. La EAI enlaza estos sistemas, permitiéndoles compartir información y comunicarse en tiempo real. Esta integración hace que las operaciones sean más fluidas y eficientes al eliminar la transferencia manual de datos.

\subsubsection{Integración Empresa a Empresa (Business-to-Business, B2B Integration)}
La integración B2B conecta los sistemas de diferentes empresas para agilizar sus procesos colaborativos. Automatiza el intercambio de transacciones y documentos, como órdenes y facturas, entre negocios.

Este flujo eficiente de información conduce a una cooperación más eficaz, transacciones más rápidas y menos errores, simplificando las interacciones con proveedores, clientes y socios.

\subsubsection{Intercambio de Datos Electrónicos (Electronic Data Interchange, EDI)}
La integración de documentos electrónicos (EDI) es un tipo específico de integración centrado en el intercambio de documentos comerciales entre sistemas, frecuentemente entre empresas (B2B). El EDI automatiza y protege la transferencia de documentos, reduciendo errores manuales y mejorando la eficiencia. En industrias como la salud y las finanzas, donde la confidencialidad de los datos es crítica, el EDI garantiza un manejo seguro de documentos.

\textbf{Ejemplo:} Imagina una empresa que utiliza sistemas separados para ventas, servicio al cliente e inventario. Cada sistema tiene sus propios datos y procesos, lo que dificulta tener una visión completa de lo que ocurre en el negocio. La EAI integra estos sistemas, permitiéndoles compartir información y comunicarse en tiempo real.

\subsection{Integración de Sistemas. Conectando sistemas}
\subsubsection{APIs (Interfaces de Programación de Aplicaciones)}
Las APIs son un mecanismo fundamental para permitir la interoperabilidad entre sistemas de software. Una API define un conjunto de rutinas, protocolos y herramientas para construir aplicaciones. Actúa esencialmente como intermediario, proporcionando una interfaz estandarizada mediante la cual las aplicaciones pueden solicitar servicios e intercambiar datos. Esta capa de abstracción protege a los desarrolladores de la complejidad subyacente de los sistemas que interactúan.

Las APIs especifican formatos de datos (ej. JSON, XML) y protocolos de comunicación (ej. REST, SOAP, gRPC) para la interacción. Las APIs RESTful, en particular, utilizan métodos HTTP (GET, POST, PUT, DELETE) para realizar operaciones sobre recursos identificados por URLs. La seguridad es una consideración crítica, abordada frecuentemente mediante mecanismos de autenticación (ej. claves API, OAuth) y autorización.

Las APIs soportan un enfoque de diseño modular, permitiendo que los sistemas evolucionen independientemente mientras mantienen la interoperabilidad.

\subsubsection{Middleware}
El middleware abarca una amplia categoría de software que facilita la comunicación y gestión de datos entre aplicaciones distribuidas. Proporciona una capa de abstracción que oculta la heterogeneidad de los sistemas subyacentes, simplificando los esfuerzos de integración.

Las soluciones de middleware ofrecen diversos servicios, incluyendo colas de mensajes (ej. RabbitMQ, Apache Kafka), gestión de transacciones, transformación de datos y seguridad. Las colas de mensajes permiten comunicación asíncrona, desacoplando aplicaciones y mejorando la escalabilidad. Los motores de transformación de datos convierten información entre diferentes formatos y esquemas, asegurando compatibilidad entre sistemas.

El middleware desacopla aplicaciones, permitiéndoles operar independientemente y mejorando la resiliencia del sistema. Las colas de mensajes y otros servicios de middleware mejoran la escalabilidad del sistema distribuyendo cargas de trabajo y gestionando comunicación asíncrona.

\subsubsection{Webhooks}
Los webhooks son un mecanismo para permitir comunicación en tiempo real entre aplicaciones. En lugar de consultar constantemente (polling) un servidor por actualizaciones, una aplicación puede registrar un webhook para recibir notificaciones cuando ocurren eventos específicos en otro sistema.

Un webhook es básicamente un callback HTTP definido por el usuario. Cuando ocurre un evento predefinido en un sistema fuente, este envía una petición HTTP POST a una URL especificada (el endpoint del webhook) conteniendo datos relacionados con el evento. La aplicación receptora puede entonces procesar los datos y tomar las acciones correspondientes.

Los webhooks proporcionan actualizaciones en tiempo real, permitiendo respuestas inmediatas a eventos. Eliminan la necesidad de polling constante, reduciendo latencia y mejorando la capacidad de respuesta del sistema. Son más eficientes en recursos que el polling, ya que solo transmiten datos cuando ocurren eventos.

\subsubsection{EDI (Intercambio Electrónico de Datos)}
El EDI es un método estandarizado para intercambiar documentos comerciales electrónicamente entre organizaciones. Reemplaza procesos tradicionales basados en papel con transmisión automatizada de datos, optimizando operaciones comerciales.

Los estándares EDI (ej. ANSI X12, UN/EDIFACT) definen el formato y estructura de documentos comerciales como órdenes de compra, facturas y avisos de envío. Las transacciones EDI típicamente se transmiten sobre redes seguras usando protocolos como AS2 (Applicability Statement 2). Se utiliza software de traducción para convertir datos entre formatos EDI y formatos internos de aplicaciones.

El EDI elimina la entrada manual de datos, reduciendo errores y mejorando la precisión de la información.

\subsubsection{Snaps de SnapLogic}
Los Snaps de SnapLogic son conectores preconstruidos diseñados para simplificar la integración con aplicaciones comúnmente utilizadas. Estos conectores ofrecen una interfaz amigable, permitiendo a los usuarios establecer conexiones sin necesidad de programación personalizada extensa.

Los Snaps abstraen las complejidades de las APIs subyacentes, proporcionando un enfoque visual y basado en configuración para la integración. Típicamente soportan operaciones comunes para interactuar con aplicaciones objetivo, como recuperación de datos, creación de datos y modificación de datos.

\subsection{Enfoques de Integración de Sistemas}
Las metodologías de integración de sistemas definen cómo los datos y procesos se interconectan entre subsistemas, donde cada enfoque ofrece ventajas y limitaciones distintas según escalabilidad, complejidad y requisitos funcionales. A continuación analizamos los modelos de integración predominantes:

\subsubsection{Modelo de Integración Vertical (Basado en Silos)}
Arquitectura jerárquica donde los subsistemas se apilan en capas funcionales, con componentes básicos en la base y procesos de alto nivel en la cima. Esto crea silos aislados optimizados para flujos de trabajo específicos.

\textbf{Ventajas:}
\begin{itemize}
\item \textit{Simplicidad:} Configuración sencilla y directa
\item \textit{Aislamiento Funcional:} Ideal para procesos específicos y aislados
\item La integración vertical es más fácil de gestionar y solucionar dentro del silo
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
\item Difícil escalabilidad debido a su estructura rígida
\item Cada nueva función requiere su propio silo, generando posibles ineficiencias
\item Flexibilidad limitada para compartir datos entre funciones
\end{itemize}

\textbf{Ejemplo:} Procesamiento de pedidos en e-commerce - los subsistemas de ventas, producción y logística se encadenan linealmente sin intercambio lateral de datos.

\subsubsection{Modelo de Integración Horizontal (Hub-and-Spoke)}
Arquitectura centralizada que emplea una capa intermediaria (ej. Enterprise Service Bus, ESB) para enrutar datos entre subsistemas, desacoplando emisores y receptores.

\textbf{Ventajas:}
\begin{itemize}
\item \textit{Modularidad:} Alta flexibilidad para reemplazar o actualizar sistemas individuales
\item \textit{Gestión Unificada:} Administración centralizada que reduce la complejidad de comunicación entre sistemas
\item Soporta amplia gama de funciones y aplicaciones
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
\item Configuración y mantenimiento más complejos
\item Puede ser costoso por requerir una capa adicional de integración
\item \textit{Punto Único de Falla:} Riesgo de un único punto de falla si la capa central tiene problemas
\end{itemize}

\textbf{Ejemplo:} Ecosistemas centrados en CRM (ej. Salesforce) como hubs para subsistemas de marketing, inventario y logística.

\subsubsection{Modelo de Integración Punto a Punto}
Enfoque más simple de conectividad donde se vinculan dos sistemas directamente entre sí. Conexiones directas por pares entre subsistemas, típicamente para transferencia unidireccional de datos o activación de eventos.

\textbf{Ventajas:}
\begin{itemize}
\item Implementación simple y rápida
\item Permite comunicación directa entre sistemas con mínima sobrecarga
\item Bajo costo y esfuerzo inicial
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
\item Escalabilidad limitada, ya que cada nueva conexión añade complejidad
\item Se vuelve difícil de gestionar al crecer el número de conexiones
\item No es adecuado para integraciones a gran escala con muchos sistemas
\end{itemize}

\textbf{Ejemplo:} Formularios de contacto en sitios web que envían datos directamente a un CRM como Salesforce.

\subsubsection{Modelo de Integración en Estrella}
La integración en estrella conecta cada sistema directamente con los demás, creando una red de conexiones punto a punto. Modelo híbrido que combina conexiones punto a punto selectivas entre subsistemas críticos, formando una malla parcial.

\textbf{Ventajas:}
\begin{itemize}
\item \textit{Eficiencia Dirigida:} Permite rutas de datos prioritarias sin interconectividad completa
\item \textit{Flexibilidad:} Permite personalizar conexiones según necesidades específicas
\item Máxima flexibilidad para compartir datos entre sistemas
\item Mejora funcionalidad al permitir múltiples rutas para flujo de datos
\end{itemize}

\textbf{Desventajas:}
\begin{itemize}
\item Creciente complejidad al añadir más sistemas
\item Requiere alto esfuerzo de mantenimiento para gestionar todas las conexiones
\item Difícil escalabilidad, ya que cada nuevo sistema incrementa exponencialmente el número de conexiones necesarias
\end{itemize}

\textbf{Ejemplo:} Un cliente realiza un pedido en un sitio web. El sitio envía la orden al sistema de ventas, que a su vez envía la información al departamento de envíos y logística. Simultáneamente, el sitio registra al cliente en el sistema CRM de la empresa, que envía una notificación a un representante de ventas en Slack. Múltiples aplicaciones están conectadas y se comunican entre sí, pero no todas están interconectadas entre ellas.
% \section{System integration vs data integration}
% System integration is not to be confused with data integration. The former means connecting disparate systems to facilitate access to information, while the latter is about gathering data from different sources into one storage to gain a unified view. Please visit our separate post about data integration to learn more about it.  

\chapter{Conferencia 7}
\normalfont\LARGE \textbf{Integración de Datos en la Ingeniería de Datos}
\normalfont\small\\
Es parte de Data Mangement (o Gestión de Datos), una de las corrientes subyacentes del ciclo de vida de la ingeniería de datos.
\section{Integración de Datos}
La integración de datos es un proceso sistemático que implica la combinación de datos provenientes de fuentes heterogéneas 
en un formato unificado, coherente y estandarizado. Este proceso es crítico en entornos computacionales y empresariales modernos, 
donde los datos se generan y almacenan en diversas plataformas, incluyendo bases de datos relacionales, aplicaciones, hojas de cálculo, 
servicios en la nube y interfaces de programación de aplicaciones (APIs). El objetivo principal de la integración de datos es facilitar 
el procesamiento analítico eficiente, los flujos de trabajo operacionales y la toma de decisiones basada en datos.

Este proceso implica extraer, transformar y cargar datos en un repositorio central o base de datos para proporcionar 
una visión integral de los datos y permitir una toma de decisiones informada.

La integración de datos trasciende la mera agregación; requiere la reconciliación de diferencias estructurales, semánticas y sintácticas entre los conjuntos de datos fuente. A menudo se emplean soluciones middleware avanzadas o capas de datos virtuales para orquestar este proceso, garantizando una interoperabilidad sin problemas. El ecosistema unificado de datos resultante permite análisis de alta precisión, inteligencia operativa en tiempo real y conocimientos estratégicos para el negocio.

\subsection{Proceso de integración de datos}
\subsubsection{1. Inicio con Descubrimiento y Perfilado de Datos}
Esta fase implica un análisis profundo del panorama de datos, identificando las múltiples fuentes que alimentan el ecosistema informativo de la organización. Aquí, el enfoque está en comprender los matices de cada fuente de datos—su estructura, calidad y peculiaridades—sentando las bases para una integración sin fisuras.
\subsubsection{2. Extracción de Datos con Precisión}
Este paso crítico requiere extraer datos de sus repositorios nativos, lo que demanda precisión y sensibilidad para evitar interrumpir la funcionalidad de los sistemas fuente. El desafío radica en reunir eficientemente los datos de sus silos, asegurando que estén preparados para la fase de transformación subsiguiente.
\subsubsection{3. Transformación de Datos para Uniformidad}
El corazón del proceso de integración de datos reside en la transformación de datos. En esta etapa, los diversos elementos de datos son estandarizados, depurados y armonizados, asegurando que "hablen" un lenguaje común. Esta transformación es pivotal, convirtiendo una cacofonía de datos dispares en un conjunto de datos armonioso listo para la integración.
Un ejemplo ilustrativo podría ser un gigante del comercio electrónico integrando datos de interacción con clientes en múltiples canales. Aquí, datos de interacciones en redes sociales, visitas al sitio web e historiales de compras se someten a normalización y deduplicación, volviéndose analizables como un todo coherente.
\subsubsection{4. Carga de Datos en un Repositorio Unificado}
Esta fase implica transferir los datos refinados a un repositorio central, como un data warehouse, donde se almacenan, acceden y analizan. La carga debe ejecutarse manteniendo la integridad de los datos y optimizando para accesos futuros.
\subsubsection{5. Garantía de Integridad mediante Validación}
\subsubsection{6. Mantenimiento de Relevancia con Sincronización de Datos}
Validación y aseguramiento de calidad de datos. Este paso confirma que los datos integrados son precisos, consistentes y de alta calidad. Se aplican controles de validación rigurosos para detectar y rectificar cualquier anomalía, asegurando la fiabilidad de los datos para la toma de decisiones.
\subsubsection{7. Facilitación de Acceso y Consumo de Datos}
La culminación del proceso de integración de datos asegura que los datos integrados sean fácilmente accesibles para análisis e inteligencia de negocio. Esto implica disponibilizar los datos en formatos y a través de canales que soporten un consumo, análisis y reporte eficientes.

\subsection{Tipos de Integración de Datos}
Existen varios métodos de integración de datos, cada uno adecuado para diferentes casos de uso. Los principales tipos son los siguientes:

\subsubsection{Integración Manual de Datos}

La integración manual de datos implica la extracción, transformación y carga (ETL) de datos de fuentes dispares mediante intervención humana. Este enfoque, aunque conceptualmente simple, requiere una inversión significativa de recursos humanos. Si bien es intensivo en recursos, puede ser una solución efectiva para proyectos de integración a pequeña escala o especializados donde las herramientas automatizadas no son factibles o rentables.

El proceso central implica recolectar manualmente datos de diversas fuentes (por ejemplo, formularios en línea, encuestas a clientes, feeds de redes sociales). Típicamente, esto incluye técnicas manuales como copiar y pegar datos en software de hojas de cálculo. Los datos recolectados luego se transforman y limpian, usualmente utilizando software de hojas de cálculo o herramientas similares, para garantizar consistencia y precisión. Finalmente, los datos transformados se cargan en un repositorio central o base de datos, potencialmente usando scripts ETL simples o soluciones personalizadas.

Las organizaciones pueden emplear integración manual cuando la complejidad o sensibilidad de los datos requiere supervisión humana. Aplicaciones comunes incluyen segmentación de clientes, detección de fraudes y cumplimiento normativo. Por ejemplo, combinar manualmente datos exportados de diferentes bases de datos dentro de un entorno de hojas de cálculo representa un ejemplo típico, aunque potencialmente propenso a errores.

La definición fundamental de integración manual de datos es el proceso de combinar datos de múltiples fuentes sin el uso de herramientas o software automatizado. Esta dependencia de intervención manual en cada paso hace que el proceso sea lento y susceptible a errores.

\textbf{Ventajas:}

\begin{itemize}
    \item \textbf{Simplicidad y Accesibilidad:} No se requieren herramientas o software especializados. El software de hojas de cálculo, ampliamente disponible, provee una plataforma accesible para la integración manual de datos.
    \item \textbf{Flexibilidad y Personalización:} Permite un alto grado de personalización y flexibilidad, sin las limitaciones de herramientas automatizadas, permitiendo transformaciones a medida y manejo de matices únicos en los datos.
    \item \textbf{Adecuación para Proyectos Pequeños:} Efectivo para proyectos a pequeña escala con fuentes de datos limitadas y volúmenes bajos, donde la infraestructura de automatización puede no justificarse.
\end{itemize}

\textbf{Desventajas:}

\begin{itemize}
    \item \textbf{Consumo de Tiempo:} Proceso intensivo en mano de obra que requiere esfuerzo manual significativo en cada etapa del proceso ETL.
    \item \textbf{Propensión a Errores:} La intervención humana introduce riesgos de errores durante la extracción, transformación y carga de datos.
    \item \textbf{Ineficiencia a Escala:} Se vuelve cada vez más impráctico e ineficiente a medida que aumentan el volumen de datos, la complejidad de las fuentes y las transformaciones requeridas. La escalabilidad es severamente limitada.
    \item \textbf{Escalabilidad Limitada:} No es adecuado para proyectos de integración de datos a gran escala debido a limitaciones inherentes para manejar grandes conjuntos de datos y transformaciones complejas.
\end{itemize}

En resumen, la integración manual de datos presenta una opción viable para proyectos pequeños donde no se necesitan herramientas especializadas y los volúmenes de datos son manejables. Sin embargo, las limitaciones inherentes del proceso en cuanto a escalabilidad, eficiencia y propensión a errores requieren una consideración cuidadosa antes de emplearlo para esfuerzos de integración de datos complejos o a gran escala. Decidir entre automatizar la integración de datos o continuar con métodos manuales requiere sopesar cuidadosamente estas compensaciones.
\subsubsection{Integración de Datos mediante Middleware}

Las soluciones de middleware son intermediarios esenciales en escenarios de integración complejos, facilitando la comunicación y transformación de datos entre sistemas dispares. Actúan como una capa de software que puentea la brecha entre diversas aplicaciones, permitiendo el intercambio y traducción de datos sin intervención manual. Estas soluciones comúnmente proveen sincronización en tiempo real, procesamiento automatizado y entrega garantizada de datos. Las organizaciones las utilizan para impulsar la Integración de Aplicaciones Empresariales (EAI) e interacciones sistema-a-sistema.

Considere un escenario donde un sistema almacena datos de clientes en formato XML, mientras otro espera JSON. El middleware permite la traducción entre estos formatos, automatizando el proceso de transformación en lugar de requerir codificación manual. Herramientas como ETL (Extraer, Transformar, Cargar) simplifican aún más la integración al automatizar estos procesos.

La integración mediante middleware utiliza software especializado para integrar datos de fuentes diversas. El middleware extrae datos, los transforma a un formato común y los carga en un repositorio central o base de datos. Gestiona el flujo de datos entre sistemas, asegurando procesamiento y almacenamiento eficientes.

Diferentes tipos de middleware atienden necesidades específicas:

\begin{itemize}
    \item \textbf{Middleware orientado a mensajes:} Enfocado en comunicación asíncrona usando colas de mensajes.
    \item \textbf{Middleware para bases de datos:} Provee una interfaz unificada para acceder a diferentes bases.
    \item \textbf{Middleware de integración de aplicaciones:} Conecta y orquesta aplicaciones mediante APIs y servicios.
\end{itemize}

Cada tipo ofrece capacidades únicas para distintos proyectos.

\textbf{Ventajas:}

\begin{itemize}
    \item \textbf{Eficiencia mejorada:} Centraliza la gestión de flujos, optimizando el proceso.
    \item \textbf{Escalabilidad:} Plataforma flexible para integrar numerosas fuentes.
    \item \textbf{Simplificación:} Interfaz común para acceso y manipulación de datos.
    \item \textbf{Calidad de datos:} Incluye manejo de errores y limpieza.
\end{itemize}

\textbf{Desventajas:}

\begin{itemize}
    \item \textbf{Complejidad:} Requiere integración de software adicional.
    \item \textbf{Costo:} Necesita habilidades especializadas para implementación y mantenimiento.
    \item \textbf{Dependencia:} Crea dependencia de la plataforma.
    \item \textbf{Retos de integración:} Requiere coordinación cuidadosa con infraestructuras existentes.
\end{itemize}

En conclusión, el middleware puede mejorar significativamente la eficiencia y escalabilidad de la integración, aunque su adopción debe evaluarse cuidadosamente contra sus potenciales desventajas.

\subsubsection{Almacenamiento de Datos (Data Warehousing)}

El data warehousing implica la extracción, transformación y carga (ETL) de datos desde fuentes heterogéneas hacia un repositorio centralizado (data warehouse). Este entorno estructurado está optimizado para consultas sistemáticas, reportes y análisis avanzado.

Un data warehouse provee una visión histórica e integrada de los datos organizacionales, facilitando análisis de tendencias y toma de decisiones informada. A diferencia de bases transaccionales, están diseñados para rendimiento analítico mediante esquemas desnormalizados, índices optimizados y almacenamiento columnar que acelera consultas sobre grandes volúmenes.

Son cruciales para organizaciones que necesitan:

\begin{itemize}
    \item \textbf{Análisis de clientes:} Comportamiento, segmentación y campañas personalizadas.
    \item \textbf{Reportes financieros:} Estados consolidados, KPIs y cumplimiento regulatorio.
    \item \textbf{Modelado predictivo:} Pronósticos, evaluación de riesgos y optimización.
\end{itemize}

Por ejemplo, con datos de ventas fragmentados en múltiples bases operacionales, un data warehouse los consolida permitiendo consultas históricas complejas. Esta agregación habilita análisis que serían ineficientes en bases tradicionales. Además, integrar datos de ventas, marketing y CRM permite reportes unificados con visión holística del negocio.

\subsubsection{Integración Basada en Aplicaciones}

La integración de datos basada en aplicaciones es un paradigma de diseño donde las funcionalidades de integración se incorporan directamente dentro de una aplicación, permitiendo conectividad y sincronización de datos entre diferentes sistemas en tiempo real. Este enfoque provee capacidades intrínsecas de integración, eliminando la necesidad de herramientas o capas separadas.

A diferencia de métodos tradicionales que dependen de mecanismos externos, esta integración otorga a las aplicaciones control directo sobre la conectividad y armonización de datos. Al incorporar la lógica de integración directamente en la aplicación, las organizaciones logran mayor agilidad, eficiencia e interoperabilidad. Esta integración más estrecha permite flujos de trabajo personalizados y control detallado sobre las transformaciones de datos.

Por ejemplo, un sistema CRM que se integra directamente con herramientas de email marketing ilustra este enfoque. La sincronización en tiempo real mejora la precisión de los datos y optimiza los esfuerzos de marketing.

Además, este método facilita la incorporación de diversos patrones de integración (procesamiento por lotes o streaming en tiempo real) dentro de la arquitectura de la aplicación. Esto permite obtener insights más profundos, fomentando la innovación y maximizando el valor de los sistemas organizacionales. La integración en la nube es un ejemplo claro, unificando datos de múltiples fuentes cloud para mejorar la toma de decisiones.

\textbf{Ventajas:}

\begin{itemize}
    \item \textbf{Simplicidad:} Los usuarios interactúan directamente con la aplicación, sin necesidad de entender herramientas complejas de integración.
    \item \textbf{Rendimiento:} La integración embebida puede ser más eficiente que capas separadas, especialmente con grandes volúmenes de datos.
    \item \textbf{Personalización:} Permite adaptar la lógica de integración a necesidades específicas de la aplicación.
\end{itemize}

\textbf{Desventajas:}

\begin{itemize}
    \item \textbf{Complejidad:} Implementar integración dentro de aplicaciones puede ser complejo, especialmente con fuentes diversas.
    \item \textbf{Interoperabilidad:} Puede limitar el intercambio de datos entre sistemas diversos comparado con capas de integración independientes.
    \item \textbf{Mantenimiento:} Actualizar la lógica de integración embebida puede ser costoso conforme evoluciona la aplicación.
\end{itemize}

En resumen, este enfoque ofrece beneficios de simplicidad y rendimiento, pero presenta desafíos en complejidad y mantenimiento.

\subsubsection{Federación de Datos (Virtualización)}
La federación de datos, o integración virtual, permite acceder y consultar datos en tiempo real a través de fuentes heterogéneas sin migración física. En lugar de consolidar datos, crea una capa virtual que presenta una vista lógica unificada de datos distribuidos en diferentes sistemas.

Esta capa virtual abstrae las fuentes subyacentes. Por ejemplo, datos de clientes en un CRM, ventas en un ERP e inventario en otra base pueden consultarse mediante una interfaz unificada. Aplicaciones típicas incluyen análisis avanzado de clientes, motores de recomendación y detección de fraudes. Como ilustración, permite consultar múltiples bases como si fueran una única base de datos unificada.

\textbf{Características clave:}
\begin{itemize}
    \item Acceso en tiempo real sin replicación física
    \item Vista lógica unificada de múltiples fuentes
    \item Elimina necesidad de procesos ETL tradicionales
    \item Ideal para escenarios que requieren datos actualizados
\end{itemize}

\textbf{Beneficios principales:}
\begin{itemize}
    \item Reduce la redundancia de datos
    \item Minimiza la latencia en el acceso
    \item Mantiene datos en sus sistemas fuente
    \item Proporciona una visión integral
\end{itemize}

\subsection{Enfoques de Integración de Datos}
\subsubsection{Integración por Lotes (Batch Integration)}
La integración por lotes (Batch Integration) es un elemento fundamental del proceso de integración de datos, diseñado para escenarios donde los datos en tiempo real no son primordiales. Este enfoque, integral al proceso de integración de datos, implica la agregación de datos en intervalos predeterminados, optimizando así la eficiencia y minimizando la carga en los sistemas de origen. A pesar de su simplicidad, el método de integración por lotes (Batch Integration) dentro del proceso de integración de datos puede introducir retrasos en la disponibilidad de los datos, lo que plantea desafíos para la toma de decisiones sensibles al tiempo.

\subsubsection{Integración en Tiempo Real (Real-time Integration)}
El proceso de integración de datos se ve significativamente mejorado por la integración en tiempo real (Real-time Integration), que garantiza la disponibilidad inmediata de los datos, un requisito crítico para las operaciones que exigen datos actualizados al minuto. Esta faceta del proceso de integración de datos es esencial para permitir la toma de decisiones dinámicas y la capacidad de respuesta operativa. Sin embargo, se debe considerar la complejidad y la intensidad de recursos de la integración en tiempo real (Real-time Integration) dentro del proceso de integración de datos.

\subsubsection{Integración Híbrida (Hybrid Integration)}
La integración híbrida (Hybrid Integration) representa una combinación estratégica dentro del proceso de integración de datos, combinando las fortalezas de los métodos por lotes (Batch) y en tiempo real (Real-time) para ofrecer una solución versátil adaptada a diversas necesidades comerciales. Este enfoque enriquece el proceso de integración de datos al proporcionar flexibilidad, permitiendo a las organizaciones equilibrar la inmediatez y la eficiencia en función de condiciones específicas de integración de datos.

\subsubsection{Integración Basada en la Nube (Cloud-based Integration)}
La computación en la nube ha introducido la integración basada en la nube (Cloud-based Integration) en el proceso de integración de datos, ofreciendo escalabilidad, rentabilidad y accesibilidad remota. Este enfoque se ha vuelto cada vez más relevante en la integración de datos, especialmente para las organizaciones con equipos geográficamente dispersos que requieren acceso a datos integrados. La seguridad de los datos y la dependencia de Internet son desafíos inherentes dentro de este enfoque de integración de datos.

\subsection{Técnicas de Integración de Datos}

La integración de datos eficaz es crucial para consolidar la información de fuentes dispares. Se encuentran disponibles varias técnicas, cada una con sus propias fortalezas y debilidades.

\subsubsection{ETL (Extract, Transform, Load)}

ETL (Extract, Transform, Load) es un proceso de integración de datos bien establecido que consta de tres fases principales:

\begin{itemize}
    \item \textbf{Extract (Extracción):} Los datos se recuperan de varios sistemas de origen.
    \item \textbf{Transform (Transformación):} Los datos extraídos se someten a limpieza, formato y transformación para garantizar la coherencia y la idoneidad para el sistema de destino. Esto puede incluir la estandarización, la deduplicación y el enriquecimiento.
    \item \textbf{Load (Carga):} Los datos transformados se cargan en el sistema de destino designado, como un almacén de datos (data warehouse).
\end{itemize}

Las herramientas ETL (Extract, Transform, Load) facilitan el movimiento y la transformación de datos entre sistemas. Algunos ejemplos son Rivery Data Integration, Talend Data Integration, Informatica PowerCenter y Oracle Data Integrator. Un ejemplo práctico implica extraer información de clientes de una plataforma de comercio electrónico, estandarizar los formatos de las direcciones y cargarla en un almacén de datos (data warehouse) centralizado. Este proceso garantiza la precisión y la coherencia de los datos para la presentación de informes y el análisis.

\subsubsection{ELT (Extract, Load, Transform)}

ELT (Extract, Load, Transform) representa un enfoque más moderno de la integración de datos. A diferencia de ETL (Extract, Transform, Load), ELT invierte el orden de los pasos de transformación y carga:

\begin{itemize}
    \item \textbf{Extract (Extracción):} Los datos se recuperan de varios sistemas de origen.
    \item \textbf{Load (Carga):} Los datos brutos se cargan directamente en el sistema de destino, a menudo un lago de datos (data lake) o un almacén de datos (data warehouse) que ofrece importantes recursos computacionales.
    \item \textbf{Transform (Transformación):} La transformación de datos se realiza dentro del sistema de destino utilizando sus capacidades de procesamiento. Esto a menudo aprovecha tecnologías como SQL u otros frameworks de procesamiento de datos.
\end{itemize}

Un ejemplo de ELT (Extract, Load, Transform) implica cargar datos de registro (log data) sin procesar en un lago de datos (data lake) y luego usar consultas SQL para transformar y analizar los datos. Este enfoque aprovecha la potencia de procesamiento del lago de datos (data lake) para una manipulación eficiente de los datos.

\subsubsection{Comparación de Estrategias de Integración}

La selección de una estrategia de integración adecuada depende de requisitos específicos, incluidos el volumen de datos, la tolerancia a la latencia y la complejidad de los datos. La Tabla \ref{tab:integration_strategies} resume varias estrategias de integración de datos y sus características.

\begin{table}[h!]
\begin{adjustwidth}{-3em}{-3em}
\centering
\caption{Comparación de Estrategias de Integración de Datos}
\label{tab:integration_strategies}
\begin{tabular}{|p{2.5cm}|p{2.5cm}|p{4cm}|p{4cm}|p{4cm}|}
\hline
\textbf{Estrategia de Integración} & \textbf{Adecuada para} & \textbf{Cómo Funciona} & \textbf{Ventajas} & \textbf{Desventajas} \\
\hline
Batch ETL & Integración no en tiempo real & Extrae, Transforma, Carga en lotes & Rentable, adecuado para grandes volúmenes de datos & No en tiempo real, latencia de datos potencial \\
\hline
Real-time ETL & Integración casi en tiempo real & Extracción, transformación y carga continuas & Datos actualizados al minuto, crítico para la toma de decisiones en tiempo real & Más complejo y potencialmente caro \\
\hline
Change Data Capture (CDC) & Sincronización de datos en tiempo real & Captura y replica los cambios del sistema de origen & Actualizaciones en tiempo real, minimiza la latencia de los datos & Configuración compleja, uso intensivo de recursos \\
\hline
Data Federation / Virtualization & Fuentes de datos heterogéneas & Proporciona una vista unificada sin integrar físicamente los datos & Minimiza la duplicación de datos, simplifica el acceso & Desafíos de rendimiento para consultas complejas, depende del rendimiento del sistema subyacente \\
\hline
Data Replication & Sincronización de datos distribuidos & Copia datos entre sistemas, mantiene la sincronización & Garantiza la coherencia de los datos en todas las ubicaciones & Uso intensivo de recursos, posibles conflictos de datos \\
\hline
Integración Basada en API (API-Based Integration) & Servicios de terceros / aplicaciones en la nube & Conecta sistemas a través de APIs para el intercambio de datos & Eficiente para servicios en la nube y socios externos & Control limitado sobre las APIs de terceros, puede ser necesario un desarrollo personalizado, depende de la disponibilidad y la estabilidad de la API \\
\hline
\end{tabular}
\end{adjustwidth}
\end{table}


\noindent \textbf{Batch ETL} es adecuado para escenarios donde la latencia de los datos no es crítica. Procesa los datos en lotes, lo que lo hace rentable para grandes volúmenes.

\noindent \textbf{Real-time ETL} proporciona integración de datos casi en tiempo real, crucial para aplicaciones que requieren información actualizada al minuto. Sin embargo, es más complejo y potencialmente costoso de implementar.

\noindent \textbf{Change Data Capture (CDC)} se centra en la sincronización de datos en tiempo real capturando y replicando los cambios en los sistemas de origen. Minimiza la latencia de los datos, pero requiere una configuración compleja y un uso intensivo de recursos.

\noindent \textbf{Data Federation/Virtualization} ofrece una vista unificada de los datos de fuentes heterogéneas sin integración física. Si bien minimiza la duplicación de datos y simplifica el acceso, el rendimiento puede ser un desafío para las consultas complejas. Su rendimiento depende en gran medida de las capacidades de los sistemas subyacentes.

\noindent \textbf{Data Replication} garantiza la coherencia de los datos en diferentes ubicaciones copiando datos entre sistemas. Si bien proporciona coherencia de datos, requiere muchos recursos y puede provocar conflictos de datos si no se gestiona con cuidado.

\noindent \textbf{Integración Basada en API (API-Based Integration)} utiliza APIs para el intercambio de datos, particularmente útil para la integración con servicios de terceros y aplicaciones en la nube. Si bien es eficiente para tales servicios, el control sobre las APIs de terceros es limitado y puede ser necesario un desarrollo personalizado. La dependencia de la disponibilidad y la estabilidad de la API introduce posibles vulnerabilidades.

\subsection{Herramientas y Tecnologías para la Integración de Datos}
Podemos simplificar el proceso de integración de datos utilizando diversas herramientas y tecnologías, las cuales son:

\subsubsection{Herramientas ETL}
Las herramientas ETL automatizan los procesos de extract, transform, and load (extraer, transformar y cargar), haciendo que la integración de datos sea más eficiente.

Por ejemplo, Talend, Apache NiFi e Informatica son herramientas ETL populares utilizadas para optimizar la integración de datos.

\subsubsection{Soluciones de Data Warehousing}
Las soluciones de Data Warehousing proporcionan un repositorio central para los datos integrados, lo que permite una consulta y análisis organizados.

Amazon Redshift, Google BigQuery y Snowflake son soluciones de Data Warehousing ampliamente utilizadas.

\subsubsection{Herramientas de Virtualización de Datos}
Las herramientas de virtualización crean una capa de datos virtual, permitiendo el acceso en tiempo real a los datos integrados.

Por ejemplo, Denodo, IBM Data Virtualization y Red Hat JBoss Data Virtualization son ejemplos de herramientas de virtualización de datos.

\subsubsection{Enterprise Information Integration (EII)}
Las herramientas EII proporcionan una vista unificada de los datos de fuentes dispares.  Permiten a los usuarios acceder y consultar datos sin tener que mover o copiar los datos.  Las soluciones EII populares incluyen WebSphere Information Integrator de IBM, TIBCO ActiveMatrix BusinessWorks y Microsoft SQL Server Integration Services.
\subsection{Mejores Prácticas para la Integración de Datos}
Para asegurar una integración de datos exitosa, siga estas mejores prácticas:

\subsubsection{Definir Objetivos Claros}
Defina claramente sus objetivos de integración de datos, tales como mejorar la precisión de los datos, mejorar la toma de decisiones u optimizar las operaciones.

Establecer el objetivo de integrar los datos de ventas y clientes para obtener mejores conocimientos del cliente es un ejemplo de un objetivo claro.

\subsubsection{Elegir las Herramientas Adecuadas}
Seleccione las herramientas que se ajusten a sus necesidades de integración, considerando factores como el volumen de datos, la complejidad y los requisitos en tiempo real.

Por ejemplo, usar una herramienta ETL para el procesamiento por lotes de grandes conjuntos de datos y una herramienta de virtualización de datos para el acceso a datos en tiempo real puede optimizar la integración de datos.

\subsubsection{Asegurar la Calidad de los Datos}
Implemente controles de calidad de los datos para garantizar la precisión y la consistencia de los datos integrados.

Por ejemplo, utilizar reglas de validación de datos para comprobar si hay duplicados y valores faltantes asegura una alta calidad de los datos.

\subsubsection{Mantener la Seguridad de los Datos}
Asegúrese de que los procesos de integración de datos actúen de acuerdo con las regulaciones de seguridad y privacidad de los datos, protegiendo la información confidencial.

Por ejemplo, cifrar los datos durante la transferencia y asegurar el cumplimiento con GDPR son críticos para mantener la seguridad de los datos.

\subsubsection{Monitorear y Optimizar}
Monitoree regularmente los procesos de integración de datos y optimícelos para obtener rendimiento y eficiencia.

Por ejemplo, utilizar herramientas de monitoreo del rendimiento para identificar cuellos de botella y mejorar la velocidad de procesamiento de datos puede mejorar la eficiencia.

\subsection{Desafíos en la Integración de Datos}
La integración de datos puede presentar varios desafíos, incluyendo:

\begin{itemize}
    \item Data Silos (Silos de Datos): Los datos almacenados en sistemas aislados pueden ser difíciles de integrar, lo que lleva a vistas de datos incompletas o inconsistentes.\\
    Ejemplo: Diferentes departamentos que utilizan bases de datos separadas sin una estrategia unificada de integración de datos pueden crear silos de datos. 
    \item Data Quality Issues (Problemas de Calidad de los Datos): La mala calidad de los datos puede llevar a análisis y toma de decisiones inexactas, socavando el valor de los datos integrados.\\
    Ejemplo: Formatos de datos inconsistentes y registros duplicados pueden causar errores en los informes.
    \item Complex Data Transformation (Transformación Compleja de Datos): Los procesos complejos de transformación de datos pueden llevar mucho tiempo y requerir habilidades especializadas.\\
    Ejemplo: Convertir datos de varios formatos y estructuras a un formato común para la integración puede ser un desafío.
    \item Scalability (Escalabilidad): Integrar grandes volúmenes de datos de múltiples fuentes puede ser un desafío, requiriendo soluciones escalables.\\
    Ejemplo: Manejar la integración de datos transaccionales de alta frecuencia de sistemas de comercio electrónico y financieros exige soluciones de integración de datos escalables.
\end{itemize}



\chapter{Conferencia 8}
\normalfont\LARGE \textbf{DevOps: Introducción}
\normalfont\small\\

% https://aws.amazon.com/devops/what-is-devops/

\section{Definición de DevOps y Fundamentos}

DevOps is a transformative approach to software development and delivery that merges cultural philosophies, practices, and tools to enhance an organization's ability to deliver applications and services at high velocity. By combining the terms 'Development' (Dev) and 'Operations' (Ops), DevOps represents a cultural shift that fosters collaboration between traditionally siloed teams, enabling faster evolution and improvement of products compared to traditional software development and infrastructure management processes. This accelerated pace allows organizations to better serve their customers and maintain a competitive edge in the market.

DevOps is defined as 'a set of practices intended to reduce the time between committing a change to a system and the change being placed into normal production, while ensuring high quality'. Practically, it is characterized by shared ownership, workflow automation, and rapid feedback. 
DevOps, particularly through continuous delivery, adheres to the 'Bring the pain forward' principle, addressing challenges early in the process through automation and early issue detection.

\subsection{Core Principles}

At its foundation, DevOps emphasizes:
\begin{itemize}
    \item \textbf{Collaboration:} Breaking down barriers between development and operations teams to enable seamless communication and shared ownership of the software lifecycle.
    \item \textbf{Automation:} Leveraging tools to automate every phase of the software development lifecycle (SDLC), including planning, coding, testing, deployment, and monitoring, to improve efficiency, consistency, and error reduction.
    \item \textbf{Continuous Delivery:} Implementing practices like continuous integration and continuous deployment (CI/CD) to enable rapid, reliable, and repeatable releases.
    \item \textbf{Feedback Loops:} Incorporating rapid feedback mechanisms to foster iterative improvement and quicker decision-making.
\end{itemize}

\subsection{DevOps Life Cycle}
\begin{enumerate}
    \item Plan - Requirements determination and analysis of business objectives. Project roadmaps are designed to optimize business impact and produce the intended result. Create a project roadmap to maximize the business value and deliver the desired product.
    \item Code - Source code is developed in version-controlled environments (e.g., Git, GitHub, GitLab). 
    \item Build - After programmers have completed their tasks, they submit the code to the common code source. 
    \item Test - To assure software integrity, the product is first delivered to the test platform to execute various sorts of screening such as user acceptability testing, safety testing, integration checking, speed testing, and so on.  
    \item Release - At this point, the build is prepared to be deployed in the operational environment. The DevOps department prepares updates or sends several versions to production when the build satisfies all checks based on the organizational demands.
    \item Deploy - At this point, Infrastructure-as-Code assists in creating the operational infrastructure and subsequently publishes the build using various DevOps lifecycle tools.
    \item Operate - This version is now convenient for users to utilize. The management department take care of server configuration and deployment at this point. 
    \item Monitor - The DevOps workflow is observed at this level depending on data gathered from consumer behavior, application efficiency, and other sources. The ability to observe the complete surroundings aids teams in identifying bottlenecks affecting the production and operations teams' performance. 
\end{enumerate}
DevOps follows positive techniques that consist of code, building, testing, releasing, deploying, operating, displaying, and planning. DevOps lifecycle follows a range of phases such as non-stop development, non-stop integration, non-stop testing, non-stop monitoring, and non-stop feedback. 


7 Cs of DevOps 

\begin{itemize}
    \item \textbf{Continuous Development (CDv)}:
    The continuous development phase initiates with requirements elicitation and stakeholder alignment, followed by maintenance of a product backlog derived from customer feedback. This backlog is decomposed into iterative releases and milestones to enable incremental software development. Upon requirements finalization, development commences through modular, version-controlled code commits. This paradigm shift from monolithic development to iterative coding allows for rapid adaptation to requirement changes and performance optimizations.

    The CDv process emphasizes atomic code contributions that undergo immediate validation through subsequent pipeline stages. This approach enhances code quality through frequent integration points, enables early defect detection, and facilitates developer focus on discrete functional units. The technical implementation typically employs distributed version control systems (e.g., Git) with branch strategies like GitFlow to maintain code integrity during parallel development streams.

    \item \textbf{Continuous Integration (CI)}:
    CI constitutes the software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. This practice incorporates static code analysis, unit testing, and build verification through automated pipelines. Each commit triggers a new build instance in ephemeral environments, ensuring isolation and reproducibility.

    The key goals of continuous integration are to find and address bugs quicker, improve software quality, and reduce the time it takes to validate and release new software updates.

    Modern implementations leverage containerized build environments (e.g., Docker) to guarantee consistency across development workstations and CI platforms.

    \item \textbf{Continuous Testing (CT)}:
    CT embeds quality verification throughout the delivery pipeline through automated test suites executed in production-like environments. The test pyramid methodology guides implementation, prioritizing unit tests (70-80\%), integration tests (10-20\%), and end-to-end tests (5-10\%). Container orchestration platforms (e.g., Kubernetes) facilitate parallel test execution across multiple environment configurations.

    Test automation frameworks (Selenium, Cypress, etc.) integrate with CI systems to provide immediate feedback on regression impacts. Advanced implementations incorporate mutation testing and chaos engineering principles to validate system resilience. Failed tests automatically trigger defect tickets and pipeline rollback mechanisms.

    \item \textbf{Continuous Deployment/Delivery (CD)}:
    Code changes are automatically built, tested, and prepared for a release to production. It expands upon continuous integration by deploying all code changes to a testing environment and/or a production environment after the build stage. When continuous delivery is implemented properly, developers will always have a deployment-ready build artifact that has passed through a standardized test process.
    
    Continuous Deployment is the process of deploying by automating the release of code changes into the production environment. The code changes run through a series of automated tests, and once they pass, are pushed immediately to the software's users.
    
    Continuous Delivery maintains manual approval gates pre-production while automating all preceding stages. Pushes new builds to an automated QA testing environment to look for bugs and other issues, and once it passes, is approved for deployment 
    \item \textbf{Continuous Monitoring (CM)}:
    CM implements observability through metrics collection (Prometheus), distributed tracing (Jaeger), and log aggregation (ELK Stack). Telemetry data feeds into anomaly detection algorithms that trigger auto-remediation workflows. Key metrics include:
    \begin{itemize}
        \item Infrastructure: CPU/Memory utilization, network I/O
        \item Application: Latency, error rates, throughput
        \item Business: Conversion rates, feature adoption
    \end{itemize}

    Visualization tools (Grafana) synthesize monitoring data into actionable dashboards. Alerting rules follow the SLO/SLI framework to maintain service quality objectives.

    \item \textbf{Continuous Feedback (CF)}:
    CF establishes bi-directional communication channels between users and development teams through:
    \begin{itemize}
        \item Structured: In-app telemetry, NPS surveys
        \item Unstructured: Sentiment analysis of support tickets, social media
    \end{itemize}

    Feedback loops integrate with product backlogs through automated ticket generation (Jira, Azure DevOps). Multivariate testing (A/B, canary) provides quantitative performance data to guide iterative improvements.

    \item \textbf{Continuous Operations (CO)}:
    CO ensures application availability through:
    \begin{itemize}
        \item Zero-downtime deployments (rolling updates, cluster rotation)
        \item Self-healing systems (pod restart policies, horizontal autoscaling)
        \item Disaster recovery (geo-replication, backup/restore workflows)
    \end{itemize}

    Container orchestration platforms (Kubernetes) implement declarative state management to maintain desired service levels. Chaos engineering tools (Gremlin) proactively validate system resilience under failure conditions.
\end{itemize}

\section{DevOps Practices. Part1}
\subsection{CI/CD}

The CI/CD pipeline is essentially a workflow that provides a pathway through which DevOps teams automate the software delivery process. 
In the absence of an automated pipeline, teams would have to configure their workflow to be performed manually, which is time-consuming and prone to error. The CI/CD pipeline removes manual errors, standardizes developers' feedback loops and increases the speed of product iterations.

Stages:
\begin{itemize}
    \item Source stage: Developers check out and locally work on application source codes stored in a central repository, such as GitHub. They then create a new branch for the feature or bug they want to fix, running tests on it locally in their development environment, before committing it back to the source repository.
    \item Build stage: New code committed to the source repository triggers the build stage, in which the branch codes are gathered and compiled to build a runnable instance of the release. Developers can build and test their releases multiple times a day to detect errors in the code and receive alerts if a build fails, or about other issues so the team can implement a fix as soon as possible. Once the code is error-free, it moves onto the next stage.
    \item Test stage: The build runs through several tests to validate the code and ensure it is acting as it should. Most commonly, these include unit tests, where the application is broken down into small units of code and tested individually, and user acceptance testing (UAT), in which people use the app to determine if the code requires more changes before it is deployed to production. Load, security and other continuous testing may be conducted at this stage.
    \item Deployment stage: When the application is ready for production, there are many different deployment strategies from which to choose. The best choice depends on the type of application, how much overhauling it has undergone, the target environment and other factors. The following are modern deployment approaches for cloud applications. 

    - Rolling deployment: This method delivers the updated application incrementally until all targets have the updated version. Rolling deployments pose less downtime risk and are easy to roll back but require services to support both the new and old versions of the app.\\
    - Blue-green deployment: In this method, developers run two versions of the app in parallel on separate infrastructures. The last stable version of the app is run in the production environment (blue) and the new version is run in a staging environment (green). The green version of the app is tested for functionality and performance, and as it passes, traffic is shifted from the blue environment to the green one, which becomes the new production environment. Blue-green deployments are fast and easy to implement but can be costly if the replicated production environment is particularly complex.\\
    - Canary deployment: In this deployment model, developers release an application to a small subset of users who use it and provide feedback. Once the updated app is confirmed to be working correctly, it's rolled out to the remaining users. The strategy allows developers to test two versions of an app with real users side-by-side and allows updates and rollbacks without downtime.\\

At every stage of the pipeline, the development team receives alerts to errors so they can immediately address the issue. The code changes go through the pipeline again, so only error-free code is deployed to production.
\end{itemize}

Best practices:
\begin{itemize}
    \item Use a consistent environment: A reliable CI/CD pipeline will always produce the same output for a particular input. But you can't have a reliable pipeline if each run modifies the pipeline environment, so start each workflow from the same isolated environment.
    \item Apply pipeline analytics: Pipeline analytics visualizes telemetry from your CI/DC processes, expanding visibility and increasing measurement capabilities, so you know how well your delivery pipeline is functioning. 
    \item Commit early and often: Committing changes early and frequently enable teams to get the most out of CI/CD. Specifically, because atomic commits are small changes made rapidly, are easier to test, easier to validate, easier to roll back, and provide the best basis for faster iteration. Frequent commits also ensure changes won't be lost in extreme cases of machine loss, or developer negligence.
    \item Build once: Successful CI/CD pipelines typically include the build process as the first step in the CI/CD cycle, and the step is executed only once with the resulting output used through the entire pipeline. Reusing a single build prevents newly introduced  inconsistencies as it moves through the pipeline stages. 
    For example, if the deployment process involves deploying to a test environment, staging environment, and production environment automation often will rebuild assets each time. This means the binary or binaries deployed to production were not actually the same ones deployed to staging and may not be identical. When this happens all tests have to be run again to validate the changes or we cannot have confidence that they will work when put into production. 
    \item Implement version control: A version control system is essential for storing source code, tracking code changes and reverting to earlier deployments when needed. Be sure to apply and use version control for scripts, documentation, libraries and configurations for your application. 
    \item Develop incrementally: Working in small iterations — breaking down the feature into smaller sub-features and using unique feature flags for each one — will make it easier to isolate problems and reduce the risk of integration issues when the feature is pushed to production.
    \item Monitor your pipelines: Correlate your pipeline data with other metrics in order to achieve optimal overall performance of your applications.
\end{itemize}

\subsection{Infrastructure as Code (IaC)}
 
Infrastructure as Code is a practice in which infrastructure is provisioned and managed using code and software development techniques, such as version control and continuous integration. Enables developers and system administrators to interact with infrastructure programmatically, and at scale, instead of needing to manually set up and configure resources.
The IT infrastructure managed by this process comprises both physical equipment, such as bare-metal servers, as well as virtual machines, and associated configuration resources. 
Thus, engineers can interface with infrastructure using code-based tools and treat infrastructure in a manner similar to how they treat application code. Because they are defined by code, infrastructure and servers can quickly be deployed using standardized patterns, updated with the latest patches and versions, or duplicated in repeatable ways.

Featuresof IaC

\begin{itemize}
    \item Automation: IAC automates the provisioning and configuration of infrastructure, reducing manual errors and saving time.
    \item Repeatability: IAC scripts can be used repeatedly, making it easy to recreate the same infrastructure in multiple environments.
    \item Version Control: IAC code is stored in version control systems like Git, which makes it easy to track changes, revert to previous versions, and collaborate with others.
    \item Scalability: IAC makes it easy to scale infrastructure up or down, adding or removing resources as needed.
    \item Transparency: IAC makes the infrastructure transparent and understandable, as the code defines the infrastructure components and their relationships.
    \item Improved Security: IAC helps ensure that infrastructure is configured consistently and securely, reducing the risk of security vulnerabilities.
\end{itemize}

Use Cases of IaC

\begin{itemize}
    \item Provisioning Virtual Machines (VMs): Using IAC, you can write code to provision VMs in a cloud computing environment, and specify the number of VMs, the operating system, and the required software.
    \item Deploying a Network: You can use IAC to deploy a network, specify the network topology, create subnets, and configure security groups.
    \item Setting up a Database: You can write code to set up a database, specify the database engine, configure users, and define the schema.
    \item Deploying a Web Application: You can use IAC to deploy a web application, specify the web server, configure the application server, and set up load balancing.
    \item Managing DNS Records: You can use IAC to manage Domain Name System (DNS) records, automate the creation and deletion of records, and ensure consistency across multiple environments. 
\end{itemize}

Tools:
\begin{itemize}
    \item Terraform: A widely-used open-source tool from HashiCorp that automates the management of cloud and on-premises resources.
    \item Ansible: A simple, agentless automation tool for managing configuration, provisioning, and deployments.
	\item Kubernetes: A platform for automating containerized applications' deployment, scaling, and management.
\end{itemize}

\subsection{Monitoring Basics}
Monitoring is the process of collecting data about the health and performance of an application or infrastructure. 

Logging is the process of recording events that occur within an application or infrastructure. These events can include error messages, warnings, and other system messages.

Organizations monitor metrics and logs to see how application and infrastructure performance impacts the experience of their product's end user. 
By capturing, categorizing, and then analyzing data and logs generated by applications and infrastructure, organizations understand how changes or updates impact users, shedding insights into the root causes of problems or unexpected changes. 
Active monitoring becomes increasingly important as services must be available 24/7 and as application and infrastructure update frequency increases. Creating alerts or performing real-time analysis of this data also helps organizations more proactively monitor their services.

DevOps monitoring and logging provide real-time visibility into system health, enabling teams to identify and address issues promptly. 
By continuously monitoring key metrics such as response times, CPU and memory utilization, and error rates, teams gain a comprehensive understanding of system behavior and can proactively detect and resolve potential bottlenecks or failures. 
Monitoring and logging also help in capacity planning and resource optimization by identifying performance trends and forecasting future requirements.

Monitoring and logging enable effective troubleshooting. When issues arise, detailed logs and metrics aid in pinpointing the root cause, reducing mean time to resolution (MTTR). 
Logs serve as a valuable source of information, capturing system events, errors, and user activities. They can be used to trace transactions, reconstruct events leading to failures, and provide valuable context during post-mortem analysis.

Monitoring and logging contribute to data-driven decision-making and continuous improvement. 
By analyzing historical data and trends, teams can identify areas for optimization, enhance system performance, and make informed decisions regarding capacity scaling, feature prioritization, and infrastructure upgrades.

Popular Tools:
\begin{itemize}
    \item Prometheus is a widely adopted open-source monitoring solution known for its scalability, flexibility, and strong integration capabilities.
    \item Grafana is a popular open-source visualization platform that integrates seamlessly with various data sources, including Prometheus. It allows users to create rich, interactive dashboards and graphs, providing real-time insights into system performance and metrics.
    \item ELK Stack (Elasticsearch, Logstash, Kibana): The ELK Stack, now commonly referred to as the Elastic Stack, is a powerful combination of open-source tools for log management and analysis. It consists of Elasticsearch, a distributed search and analytics engine; Logstash, a versatile data processing pipeline; and Kibana, a web-based visualization platform. The Elastic Stack enables centralized collection, storage, and analysis of logs from various sources. Logstash allows for log ingestion, parsing, and transformation before sending the data to Elasticsearch for indexing and storage.
\end{itemize}

% DevOps Model Defined: DevOps is the combination of cultural philosophies, practices, and tools that increases an organization's ability to deliver applications and services at high velocity: evolving and improving products at a faster pace than organizations using traditional software development and infrastructure management processes. This speed enables organizations to better serve their customers and compete more effectively in the market.

% DevOps is a combination of two words, "Development" and "Operations". It represents a cultural approach that emphasizes collaboration between Development(Dev) and Operations(Ops) teams to increase the efficiency, speed, and security of the entire software development and delivery compared to traditional processes. The reason why DevOps is booming is that most of the companies are moving to the cloud, and they need to hire DevOps Engineers to manage systems efficiently.

% DevOps combines development (Dev) and operations (Ops) to increase the efficiency, speed, and security of software development and delivery compared to traditional processes. A more nimble software development lifecycle results in a competitive advantage for businesses and their customers.

% DevOps is a software development approach emphasizing collaboration, automation, and continuous delivery to provide high-quality products to customers quickly and efficiently. DevOps breaks down silos between development and operations teams to enable seamless communication, faster time-to-market, and improved customer satisfaction.

% It allows a team to handle the complete application lifecycle, from development to testing, operations, and deployment. It shows cooperation between Development and Operations groups to deploy code to production quickly in an automated and repeatable manner.

% Every phase of the software development lifecycle, including planning, coding, testing, deployment, and monitoring, is heavily automated in DevOps. This improves productivity, ensures consistency, and lowers error rates in the development process. A culture of continuous improvement is also promoted by DevOps, where feedback loops are incorporated into the procedure to facilitate quicker iteration and better decision-making. Organizations can increase their agility, lower costs, and speed up innovation by adopting DevOps. 

% DevOps aims to improve the release and quality of software products.

% The framework and workflow consist of change management (environment, code, and data version control), continuous integration/continuous deployment (CI/CD), and configuration as code.

% DevOps is the integration and automation of the software development and information technology operations. DevOps encompasses necessary tasks of software development and can lead to shortening development time and improving the development life cycle. According to Neal Ford, DevOps, particularly through continuous delivery, employs the "Bring the pain forward" principle, tackling tough tasks early, fostering automation and swift issue detection. Software programmers and architects should use fitness functions to keep their software in check.

% Although debated, DevOps is characterized by key principles: shared ownership, workflow automation, and rapid feedback. From an academic perspective, Len Bass, Ingo Weber, and Liming Zhu—three computer science researchers from the CSIRO and the Software Engineering Institute—suggested defining DevOps as "a set of practices intended to reduce the time between committing a change to a system and the change being placed into normal production, while ensuring high quality". However, the term is used in multiple contexts. At its most successful, DevOps is a combination of specific practices, culture change, and tools.

% DevOps can be best explained as people working together to conceive, build and deliver secure software at top speed. DevOps practices enable software development (dev) and operations (ops) teams to accelerate delivery through automation, collaboration, fast feedback, and iterative improvement. Stemming from an Agile approach to software development, a DevOps process expands on the cross-functional approach of building and shipping applications in a faster and more iterative manner.

% In adopting a DevOps development process, you are making a decision to improve the flow and value delivery of your application by encouraging a more collaborative environment at all stages of the development cycle.DevOps represents a change in mindset for IT culture. In building on top of Agile, lean practices, and systems theory, DevOps focuses on incremental development and rapid delivery of software. Success relies on the ability to create a culture of accountability, improved collaboration, empathy, and joint responsibility for business outcomes.


\chapter{Conferencia 9}
\normalfont\LARGE \textbf{DevOps: Advanced DevOps Practices for Scalable Systems}
\normalfont\small\\

Microservices:
The microservices architecture is a design approach to build a single application as a set of small services. Each service runs in its own process and communicates with other services through a well-defined interface using a lightweight mechanism, typically an HTTP-based application programming interface (API). Microservices are built around business capabilities; each service is scoped to a single purpose. You can use different frameworks or programming languages to write microservices and deploy them independently, as a single service, or as a group of services.

Learn more about AWS Lambda

Configuration Management

Developers and system administrators use code to automate operating system and host configuration, operational tasks, and more. The use of code makes configuration changes repeatable and standardized. It frees developers and systems administrators from manually configuring operating systems, system applications, or server software.

Learn how you can configure and manage Amazon EC2 and on-premises systems with Amazon EC2 Systems Manager

Learn to use configuration management with AWS OpsWorks
Policy as Code

With infrastructure and its configuration codified with the cloud, organizations can monitor and enforce compliance dynamically and at scale. Infrastructure that is described by code can thus be tracked, validated, and reconfigured in an automated way. This makes it easier for organizations to govern changes over resources and ensure that security measures are properly enforced in a distributed manner (e.g. information security or compliance with PCI-DSS or HIPAA). This allows teams within an organization to move at higher velocity since non-compliant resources can be automatically flagged for further investigation or even automatically brought back into compliance.

Learn how you can use AWS Config and Config Rules to monitor and enforce compliance for your infrastructure

\chapter{Conferencia 10}
\normalfont\LARGE \textbf{DataOps: DevOps for Data-Centric Workflows}
\normalfont\small\\
DataOps is still a work in progress. Practitioners have done a
good job of adapting DevOps principles to the data domain and mapping
out an initial vision through the DataOps Manifesto and other resources.
Data engineers would do well to make DataOps practices a high priority in
all of their work. The up-front effort will see a significant long-term payoff
through faster delivery of products, better rel

A few DataOps considerations are as follows:
Automation
There's the automation impacting the source system, such as code
updates and new features. Then there's the DataOps automation that
you've set up for your data workflows. Does an issue in the source
system's automation impact your data workflow automation? If so,
consider decoupling these systems so they can perform automation
independently.Observability
How will you know when there's an issue with a source system, such as
an outage or a data-quality issue? Set up monitoring for source system
uptime (or use the monitoring created by the team that owns the source
system). Set up checks to ensure that data from the source system
conforms with expectations for downstream usage. For example, is the
data of good quality? Is the schema conformant? Are customer records
consistent? Is data hashed as stipulated by the internal policy?
Incident response
What's your plan if something bad happens? For example, how will
your data pipeline behave if a source system goes offline? What's your
plan to backfill the “lost” data once the source system is back online?

DataOps
The steps you take in data management—data quality, governance, and
security—are monitored in DataOps. Essentially, DataOps operationalizes
data management. The following are some things to monitor:
Data health and data downtime
Latency of systems serving data—dashboards, databases, etc.
Data quality
Data and system security and access
Data and model versions being served
Uptime to achieve an SLO
A variety of new tools have sprung up to address various monitoring
aspects. For example, many popular data observability tools aim to
minimize data downtime and maximize data quality. Observability toolsmay cross over from data to ML, supporting monitoring of models and
model performance. More conventional DevOps monitoring is also critical
to DataOps—e.g., you need to monitor whether connections are stable
among storage, transformation, and serving.
As in every stage of the data engineering lifecycle, version-control code and
operationalize deployment. This applies to analytical code, data logic code,
ML scripts, and orchestration jobs. Use multiple stages of deployment (dev,
test, prod) for reports and models.

\chapter{Bibliografía}
- El libro "Fundamentals of Data Engineering" va de : the data engineering lifecycle: data generation, storage, ingestion, transformation, and serving. Since the dawn of data, we've seen the rise and fall of innumerable specific technologies and vendor products, but the data engineering lifecycle stages have remained essentially unchanged. 

\end{document}
